{"config":{"lang":["de","en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"ADR/","title":"Architecture Decision Records (ADR)","text":"<p>This directory contains records of important architectural decisions made during the development of this project.</p> <ul> <li>ADR-001: Center of Mass vs Geometric Center</li> </ul>"},{"location":"ADR/ADR-001/","title":"ADR-001: Center of Mass vs Geometric Center","text":""},{"location":"ADR/ADR-001/#context","title":"Context","text":"<p>Objects detected from segmentation masks have two natural reference points: - Geometric center (bounding box midpoint) - Center of mass (centroid of mask pixels)</p>"},{"location":"ADR/ADR-001/#decision","title":"Decision","text":"<p>Expose both as <code>pose_center()</code> and <code>pose_com()</code> respectively in the <code>Object</code> class.</p>"},{"location":"ADR/ADR-001/#rationale","title":"Rationale","text":"<ul> <li>Center of Mass (CoM) is generally better for irregular shapes. For many robotic manipulation tasks, the gripper should target the center of mass to ensure stable picks.</li> <li>Geometric Center is simpler and works well for regular, box-shaped objects. It is also a common fallback when a full segmentation mask is not available.</li> <li>Providing both allows users to choose the most appropriate reference point for their specific use case (e.g., visual alignment vs. physical manipulation).</li> </ul>"},{"location":"ADR/ADR-001/#consequences","title":"Consequences","text":"<ul> <li>Slight increase in API complexity due to having two similar methods.</li> <li>Users must understand the difference between the two to use the library effectively.</li> <li>Future consideration: If usage patterns show that one is overwhelmingly preferred, we might consider deprecating the other or making one the default while keeping the other accessible.</li> </ul>"},{"location":"","title":"Robot Workspace","text":"<p>Ein Python-Framework, das die L\u00fccke zwischen Kamerabildern und physikalischer Robotermanipulation schlie\u00dft. Es bietet die wesentlichen Datenstrukturen und Koordinatentransformationen, die ben\u00f6tigt werden, um erkannte Objekte von Visionssystemen in ausf\u00fchrbare Pick-and-Place-Ziele f\u00fcr Roboterarme umzuwandeln. Das Framework k\u00fcmmert sich um Workspace-Kalibrierung, Objektrepr\u00e4sentation mit physikalischen Eigenschaften und r\u00e4umliches Denken \u2013 so k\u00f6nnen mit Kameras ausgestattete Roboter verstehen, \"wo\" sich Objekte befinden und \"wie\" sie in realen Koordinaten gegriffen werden k\u00f6nnen.</p>"},{"location":"#uberblick","title":"\ud83c\udfaf \u00dcberblick","text":"<p>Das <code>robot_workspace</code>-Paket bietet ein vollst\u00e4ndiges Framework zur Verwaltung von Roboter-Workspaces, einschlie\u00dflich:</p> <ul> <li>\ud83c\udfaf Koordinatentransformationen: Nahtlose Transformation zwischen Kamera- und Welt-Koordinatensystemen</li> <li>\ud83d\udce6 Objektrepr\u00e4sentation: Reichhaltige Objektmodelle mit Position, Dimensionen, Segmentierungsmasken und Orientierung</li> <li>\ud83d\uddfa\ufe0f Workspace-Management: Definieren und Verwalten mehrerer Workspaces mit unterschiedlichen Konfigurationen</li> <li>\ud83d\udd0d R\u00e4umliche Abfragen: Finden von Objekten nach Position, Gr\u00f6\u00dfe, N\u00e4he oder benutzerdefinierten Kriterien</li> <li>\ud83d\udcbe Serialisierung: JSON-basierte Serialisierung f\u00fcr Datenpersistenz und Kommunikation</li> <li>\ud83e\udd16 Roboter-Unterst\u00fctzung: Native Unterst\u00fctzung f\u00fcr Niryo Ned2 und WidowX 250 6DOF Roboter (Echt und Simulation)</li> </ul>"},{"location":"#hauptmerkmale","title":"\u2728 Hauptmerkmale","text":""},{"location":"#vision-erkennung","title":"Vision &amp; Erkennung","text":"<ul> <li>Integration der Objekterkennung mit Bounding Boxes, Segmentierungsmasken und physikalischen Eigenschaften</li> <li>Berechnung des Massenschwerpunkts und optimaler Greiferorientierungen</li> <li>Unterst\u00fctzung f\u00fcr Multi-Objekt-Tracking und Management</li> </ul>"},{"location":"#koordinatensysteme","title":"Koordinatensysteme","text":"<ul> <li>Transformation zwischen relativen Bildkoordinaten (0-1) und Weltkoordinaten (Meter)</li> <li>Handhabung mehrerer Workspace-Konfigurationen mit unterschiedlichen Kameraposen</li> <li>Automatische Erkennung von Workspace-Grenzen</li> </ul>"},{"location":"#raumliches-denken","title":"R\u00e4umliches Denken","text":"<ul> <li>Abfrage von Objekten nach r\u00e4umlichen Beziehungen (links/rechts/oberhalb/unterhalb/nah bei)</li> <li>Finden des n\u00e4chstgelegenen Objekts zu angegebenen Koordinaten</li> <li>Filtern nach Gr\u00f6\u00dfe, Label oder benutzerdefinierten Kriterien</li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install -e .\n</code></pre> <p>F\u00fcr alle Features: <pre><code>pip install -e \".[all]\"\n</code></pre></p>"},{"location":"#schnellstart","title":"\ud83d\ude80 Schnellstart","text":"<pre><code>from robot_workspace import PoseObjectPNP, Object, Objects, NiryoWorkspaces\n\n# 1. Arbeiten mit Poses\npose = PoseObjectPNP(x=0.2, y=0.1, z=0.05, roll=0.0, pitch=1.57, yaw=0.0)\n\n# 2. Objektrepr\u00e4sentation\nobj = Object(\n    label=\"pencil\",\n    u_min=100, v_min=100, u_max=200, v_max=200,\n    mask_8u=None,\n    workspace=workspace\n)\n\n# 3. R\u00e4umliche Abfragen\nobjects = Objects([obj1, obj2, obj3])\nnearest, distance = objects.get_nearest_detected_object([0.25, 0.05])\n</code></pre>"},{"location":"configuration/","title":"Konfiguration","text":"<p>Informationen zur Konfiguration des Workspace-Systems.</p>"},{"location":"configuration/#workspace-konfiguration","title":"Workspace-Konfiguration","text":"<p>Workspaces werden in der Workspace-Implementierung definiert:</p> <pre><code>def _set_observation_pose(self):\n    if self._id == \"niryo_ws\":\n        self._observation_pose = PoseObjectPNP(\n            x=0.173, y=-0.002, z=0.277,\n            roll=-3.042, pitch=1.327, yaw=-3.027\n        )\n</code></pre>"},{"location":"configuration/#logging-konfiguration","title":"Logging-Konfiguration","text":"<p>Das Paket verwendet das Standard-Python-Logging-Modul. Sie k\u00f6nnen den Log-Level anpassen:</p> <pre><code>import logging\nlogging.getLogger(\"robot_workspace\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"getting-started/","title":"Erste Schritte","text":"<p>Diese Anleitung hilft Ihnen beim Einstieg in das <code>robot_workspace</code>-Paket.</p>"},{"location":"getting-started/#kernkonzepte","title":"Kernkonzepte","text":"<p>Das Repository bietet drei Hauptfunktionen:</p> <ol> <li>Pose-Repr\u00e4sentation - 6-DOF Posen (Position + Orientierung) f\u00fcr Objekte und Roboterziele.</li> <li>Objekt-Management - Erkannte Objekte mit physikalischen Eigenschaften, Positionen und r\u00e4umlichen Abfragen.</li> <li>Koordinatentransformation - Konvertierung von Kamerakoordinaten in Roboter-Weltkoordinaten.</li> </ol>"},{"location":"getting-started/#grundlegende-verwendung","title":"Grundlegende Verwendung","text":""},{"location":"getting-started/#1-arbeiten-mit-poses-position-orientierung","title":"1. Arbeiten mit Poses (Position + Orientierung)","text":"<pre><code>from robot_workspace import PoseObjectPNP\n\n# Pose erstellen\npose = PoseObjectPNP(x=0.2, y=0.1, z=0.05, roll=0.0, pitch=1.57, yaw=0.0)\nprint(f\"Position: [{pose.x}, {pose.y}, {pose.z}]\")\n\n# Offsets hinzuf\u00fcgen\npick_pose = pose.copy_with_offsets(z_offset=-0.02)  # Greifer 2cm absenken\n</code></pre>"},{"location":"getting-started/#2-reprasentation-erkannter-objekte","title":"2. Repr\u00e4sentation erkannter Objekte","text":"<pre><code>from robot_workspace import Object\n\nobj = Object(\n    label=\"pencil\",\n    u_min=100, v_min=100, u_max=200, v_max=200,\n    mask_8u=None,\n    workspace=workspace\n)\n\nprint(f\"Objekt '{obj.label()}' bei [{obj.x_com():.2f}, {obj.y_com():.2f}] Metern\")\n</code></pre>"},{"location":"getting-started/#3-raumliche-abfragen","title":"3. R\u00e4umliche Abfragen","text":"<pre><code>from robot_workspace import Objects, Location\n\nobjects = Objects([obj1, obj2, obj3])\n\n# Nach r\u00e4umlichen Beziehungen suchen\nleft_objects = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.2, 0.0]\n)\n\n# N\u00e4chstgelegenes Objekt finden\nnearest, distance = objects.get_nearest_detected_object([0.25, 0.05])\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Anleitung zur Installation des <code>robot_workspace</code>-Pakets.</p>"},{"location":"installation/#voraussetzungen","title":"Voraussetzungen","text":"<ul> <li>Python 3.9 oder h\u00f6her</li> <li>pip Paketmanager</li> </ul>"},{"location":"installation/#basis-installation","title":"Basis-Installation","text":"<pre><code># Repository klonen\ngit clone https://github.com/dgaida/robot_workspace.git\ncd robot_workspace\n\n# Im Entwicklungsmodus installieren\npip install -e .\n</code></pre>"},{"location":"installation/#mit-roboter-unterstutzung","title":"Mit Roboter-Unterst\u00fctzung","text":"<pre><code># Niryo Ned2 Unterst\u00fctzung\npip install -e \".[niryo]\"\n\n# Alle Features\npip install -e \".[all]\"\n</code></pre>"},{"location":"installation/#entwickler-installation","title":"Entwickler-Installation","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>Dies installiert zus\u00e4tzliche Werkzeuge: - <code>pytest</code> und <code>pytest-cov</code> f\u00fcr Tests - <code>black</code> f\u00fcr Code-Formatierung - <code>ruff</code> f\u00fcr Linting - <code>mypy</code> f\u00fcr Typpr\u00fcfung - <code>pre-commit</code> Hooks</p>"},{"location":"quality_metrics/","title":"Qualit\u00e4tsmetriken","text":"<p>Diese Seite bietet einen \u00dcberblick \u00fcber die Qualit\u00e4tsmetriken des Projekts, einschlie\u00dflich Dokumentationsabdeckung und Testergebnisse.</p>"},{"location":"quality_metrics/#api-dokumentationsabdeckung","title":"\ud83d\udcca API-Dokumentationsabdeckung","text":"<p>Wir verwenden <code>interrogate</code>, um unsere API-Dokumentationsabdeckung zu messen. Unser Ziel ist &gt;95%.</p> Metrik Status \u00d6ffentliche API-Abdeckung Docstring-Stil Google"},{"location":"quality_metrics/#testabdeckung","title":"\ud83e\uddea Testabdeckung","text":"Kategorie Abdeckung Gesamt Unit-Tests &gt;90% Integrationstests Verifiziert"},{"location":"quality_metrics/#code-qualitat","title":"\ud83d\udee0\ufe0f Code-Qualit\u00e4t","text":"<ul> <li>Linter: Ruff</li> <li>Formatierer: Black</li> <li>Typpr\u00fcfung: Mypy (Strict)</li> </ul>"},{"location":"troubleshooting/","title":"Fehlerbehebung","text":"<p>H\u00e4ufige Probleme und deren L\u00f6sungen.</p>"},{"location":"troubleshooting/#objekt-nicht-gefunden","title":"Objekt nicht gefunden","text":"<p>Wenn <code>get_detected_object</code> den Wert <code>None</code> zur\u00fcckgibt: - \u00dcberpr\u00fcfen Sie, ob die Koordinaten innerhalb der Workspace-Grenzen liegen. - Stellen Sie sicher, dass das Label korrekt geschrieben ist.</p>"},{"location":"troubleshooting/#koordinatentransformation-ungenau","title":"Koordinatentransformation ungenau","text":"<ul> <li>\u00dcberpr\u00fcfen Sie die Kamera-Kalibrierung.</li> <li>Stellen Sie sicher, dass die Workspace-Ecken korrekt definiert sind.</li> <li>Pr\u00fcfen Sie, ob die Bildaufl\u00f6sung mit der Konfiguration \u00fcbereinstimmt.</li> </ul>"},{"location":"api/","title":"API-Referenz","text":"<p>Dieser Abschnitt enth\u00e4lt die automatisch generierte Dokumentation f\u00fcr das <code>robot_workspace</code>-Paket.</p>"},{"location":"api/#kernmodule","title":"Kernmodule","text":""},{"location":"api/#robot_workspace.objects.object","title":"<code>robot_workspace.objects.object</code>","text":""},{"location":"api/#robot_workspace.objects.object-classes","title":"Classes","text":""},{"location":"api/#robot_workspace.objects.object.Object","title":"<code>Object</code>","text":"<p>               Bases: <code>ObjectAPI</code></p> <p>Class representing an object detected in a robot's workspace.</p> <p>Each object is characterized by its bounding box (in both pixel and world coordinates), segmentation mask (if available), size, and additional metadata. This class also provides methods to calculate dimensions, orientation, and other properties of the object.</p> <p>Attributes:</p> Name Type Description <code>_label</code> <code>str</code> <p>Label identifying the object (e.g., \"pencil\").</p> <code>_workspace</code> <code>Workspace</code> <p>Workspace in which the object resides.</p> <code>_verbose</code> <code>bool</code> <p>Whether verbose logging is enabled.</p> <code>_logger</code> <code>Logger</code> <p>Logger instance.</p> <code>_original_mask_8u</code> <code>ndarray</code> <p>Original segmentation mask.</p> <code>_u_rel_min</code> <code>float</code> <p>Minimum U relative coordinate.</p> <code>_v_rel_min</code> <code>float</code> <p>Minimum V relative coordinate.</p> <code>_u_rel_max</code> <code>float</code> <p>Maximum U relative coordinate.</p> <code>_v_rel_max</code> <code>float</code> <p>Maximum V relative coordinate.</p> <code>_width</code> <code>float</code> <p>Width in relative coordinates.</p> <code>_height</code> <code>float</code> <p>Height in relative coordinates.</p> <code>_gripper_rotation</code> <code>float</code> <p>Suggested orientation for the robot gripper.</p> <code>_size_m2</code> <code>float</code> <p>Area of the object in square meters.</p> <code>_pose_center</code> <code>PoseObjectPNP</code> <p>Pose of the geometric center.</p> <code>_u_rel_o</code> <code>float</code> <p>U relative coordinate of the center.</p> <code>_v_rel_o</code> <code>float</code> <p>V relative coordinate of the center.</p> <code>_pose_com</code> <code>PoseObjectPNP</code> <p>Pose of the center of mass.</p> <code>_u_rel_com</code> <code>float</code> <p>U relative coordinate of the COM.</p> <code>_v_rel_com</code> <code>float</code> <p>V relative coordinate of the COM.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>class Object(ObjectAPI):\n    \"\"\"\n    Class representing an object detected in a robot's workspace.\n\n    Each object is characterized by its bounding box (in both pixel and world coordinates),\n    segmentation mask (if available), size, and additional metadata. This class also provides\n    methods to calculate dimensions, orientation, and other properties of the object.\n\n    Attributes:\n        _label (str): Label identifying the object (e.g., \"pencil\").\n        _workspace (Workspace): Workspace in which the object resides.\n        _verbose (bool): Whether verbose logging is enabled.\n        _logger (logging.Logger): Logger instance.\n        _original_mask_8u (np.ndarray, optional): Original segmentation mask.\n        _u_rel_min (float): Minimum U relative coordinate.\n        _v_rel_min (float): Minimum V relative coordinate.\n        _u_rel_max (float): Maximum U relative coordinate.\n        _v_rel_max (float): Maximum V relative coordinate.\n        _width (float): Width in relative coordinates.\n        _height (float): Height in relative coordinates.\n        _gripper_rotation (float): Suggested orientation for the robot gripper.\n        _size_m2 (float): Area of the object in square meters.\n        _pose_center (PoseObjectPNP): Pose of the geometric center.\n        _u_rel_o (float): U relative coordinate of the center.\n        _v_rel_o (float): V relative coordinate of the center.\n        _pose_com (PoseObjectPNP): Pose of the center of mass.\n        _u_rel_com (float): U relative coordinate of the COM.\n        _v_rel_com (float): V relative coordinate of the COM.\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    @log_start_end_cls()\n    def __init__(\n        self,\n        label: str,\n        u_min: int,\n        v_min: int,\n        u_max: int,\n        v_max: int,\n        mask_8u: np.ndarray | None,\n        workspace: Workspace,\n        verbose: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initializes an Object instance.\n\n        Args:\n            label (str): Label of the object (e.g., \"chocolate bar\").\n            u_min (int): Upper-left corner u-coordinate of the bounding box (in pixels).\n            v_min (int): Upper-left corner v-coordinate of the bounding box (in pixels).\n            u_max (int): Lower-right corner u-coordinate of the bounding box (in pixels).\n            v_max (int): Lower-right corner v-coordinate of the bounding box (in pixels).\n            mask_8u (np.ndarray, optional): Segmentation mask of the object (8-bit, uint8).\n            workspace (Workspace): Workspace instance where the object is located.\n            verbose (bool): If True, enables verbose logging.\n\n        Raises:\n            ValueError: If the provided segmentation mask is not 8-bit unsigned,\n                or if the workspace has no image shape.\n        \"\"\"\n        super().__init__()\n\n        self._label = label\n        self._workspace = workspace\n        self._verbose = verbose\n        self._logger = logging.getLogger(\"robot_workspace\")\n\n        if self._workspace.img_shape() is None:\n            raise ValueError(\"Object has no image shape. This probably means that the Niryo Workspace was not detected.\")\n\n        # Store original mask for rotation\n        self._original_mask_8u = mask_8u.copy() if mask_8u is not None else None\n\n        # Initialize the object with the common initialization logic\n        self._init_object_properties(u_min, v_min, u_max, v_max, mask_8u)\n\n    def _init_object_properties(self, u_min: int, v_min: int, u_max: int, v_max: int, mask_8u: np.ndarray | None) -&gt; None:\n        \"\"\"\n        Common initialization logic for object properties.\n\n        Args:\n            u_min (int): Upper-left corner u-coordinate (pixels).\n            v_min (int): Upper-left corner v-coordinate (pixels).\n            u_max (int): Lower-right corner u-coordinate (pixels).\n            v_max (int): Lower-right corner v-coordinate (pixels).\n            mask_8u (np.ndarray, optional): Segmentation mask (8-bit, uint8).\n        \"\"\"\n        self._u_rel_min, self._v_rel_min = self._calc_rel_coordinates(u_min, v_min)\n        self._u_rel_max, self._v_rel_max = self._calc_rel_coordinates(u_max, v_max)\n\n        self._calc_width_height_m()\n\n        self._width = self._u_rel_max - self._u_rel_min\n        self._height = self._v_rel_max - self._v_rel_min\n\n        if mask_8u is not None:\n            self._calc_largest_contour(mask_8u)\n\n            gripper_rotation, center = self._calc_gripper_orientation_from_segmentation_mask()\n            width, height = self._rotated_bounding_box()\n\n            u_0 = center[0]\n            v_0 = center[1]\n\n            self._gripper_rotation = gripper_rotation\n\n            self._update_width_height(width, height)\n\n            cm = Object._calculate_center_of_mass(mask_8u)\n            if cm is not None:\n                cx, cy = cm\n            else:\n                cx, cy = u_0, v_0\n        else:\n            u_0 = int(u_min + (u_max - u_min) / 2)\n            v_0 = int(v_min + (v_max - v_min) / 2)\n            cx = u_0\n            cy = v_0\n            self._gripper_rotation = 0.0\n\n        self._calc_size()\n\n        self._pose_center, self._u_rel_o, self._v_rel_o = self._calc_pose_from_uv_coords(u_0, v_0)\n        self._pose_com, self._u_rel_com, self._v_rel_com = self._calc_pose_from_uv_coords(int(cx), int(cy))\n\n    def __str__(self) -&gt; str:\n        position = f\"x = {self._pose_com.x:.2f}, y = {self._pose_com.y:.2f}, z = {self._pose_com.z:.2f}\"\n        orientation = f\"roll = {self._pose_com.roll:.3f}, pitch = {self._pose_com.pitch:.3f}, yaw = {self._pose_com.yaw:.3f}\"\n        return self.label() + \"\\n\" + position + \"\\n\" + orientation\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n\n    # *** PUBLIC SET methods ***\n\n    def set_position(self, xy_coordinate: list[float]) -&gt; None:\n        \"\"\"\n        Legacy method kept for backwards compatibility.\n\n        Args:\n            xy_coordinate (list[float]): [x, y] world coordinates in meters.\n        \"\"\"\n        # Create a pose with the new x, y but keeping old z and orientation\n        new_pose = self._pose_com.copy_with_offsets(\n            x_offset=xy_coordinate[0] - self._pose_com.x, y_offset=xy_coordinate[1] - self._pose_com.y\n        )\n        self.set_pose_com(new_pose)\n\n    def set_pose_com(self, pose_com: PoseObjectPNP) -&gt; None:\n        \"\"\"\n        Updates the object's center of mass pose and recalculates all dependent properties.\n\n        Handles both position changes and orientation changes (rotation around z-axis).\n\n        Args:\n            pose_com (PoseObjectPNP): New pose for the object's center of mass.\n        \"\"\"\n        if self.verbose():\n            self._logger.debug(f\"set_pose_com: {self._pose_com} \u2192 {pose_com}\")\n\n        # 1. Calculate transformations\n        rotation_delta = pose_com.yaw - self._gripper_rotation\n        translation = self._calculate_translation(pose_com)\n\n        # 2. Transform bounding box\n        new_u_min, new_v_min, new_u_max, new_v_max = self._apply_pose_transform_to_bbox(rotation_delta, translation)\n\n        # 3. Transform mask if present\n        transformed_mask = self._apply_pose_transform_to_mask(rotation_delta, translation)\n\n        # 4. Reinitialize with new geometry\n        self._init_object_properties(new_u_min, new_v_min, new_u_max, new_v_max, transformed_mask)\n\n        # 5. Override calculated pose with exact provided pose\n        self._pose_com = pose_com\n\n    def _calculate_translation(self, target_pose: PoseObjectPNP) -&gt; tuple[int, int]:\n        \"\"\"\n        Calculate pixel translation needed to reach target pose.\n\n        Args:\n            target_pose (PoseObjectPNP): The target pose in world coordinates.\n\n        Returns:\n            tuple[int, int]: (translation_u, translation_v) in pixels.\n        \"\"\"\n        img_width, img_height, _ = self._workspace.img_shape()\n\n        # Calculate old center in pixel coordinates\n        old_center_u = int(self._u_rel_com * img_width)\n        old_center_v = int(self._v_rel_com * img_height)\n\n        # Calculate new center in pixel coordinates\n        new_center_u_rel, new_center_v_rel = self._world_to_rel_coordinates(target_pose)\n        new_center_u = int(new_center_u_rel * img_width)\n        new_center_v = int(new_center_v_rel * img_height)\n\n        translation_u = new_center_u - old_center_u\n        translation_v = new_center_v - old_center_v\n\n        return translation_u, translation_v\n\n    def _apply_pose_transform_to_bbox(self, rotation_delta: float, translation: tuple[int, int]) -&gt; tuple[int, int, int, int]:\n        \"\"\"\n        Apply rotation and translation to bounding box.\n\n        Args:\n            rotation_delta (float): Rotation change in radians.\n            translation (tuple[int, int]): (delta_u, delta_v) in pixels.\n\n        Returns:\n            tuple[int, int, int, int]: New (u_min, v_min, u_max, v_max).\n        \"\"\"\n        img_width, img_height, _ = self._workspace.img_shape()\n\n        # Calculate old center in pixel coordinates\n        old_center_u = int(self._u_rel_com * img_width)\n        old_center_v = int(self._v_rel_com * img_height)\n\n        # Calculate old bounding box corners in pixel coordinates\n        old_u_min = int(self._u_rel_min * img_width)\n        old_v_min = int(self._v_rel_min * img_height)\n        old_u_max = int(self._u_rel_max * img_width)\n        old_v_max = int(self._v_rel_max * img_height)\n\n        # Rotate bounding box corners around the old center\n        rotated_corners = self._rotate_bounding_box(\n            old_u_min, old_v_min, old_u_max, old_v_max, old_center_u, old_center_v, rotation_delta\n        )\n\n        # Find new axis-aligned bounding box from rotated corners\n        new_u_min = int(min(corner[0] for corner in rotated_corners))\n        new_v_min = int(min(corner[1] for corner in rotated_corners))\n        new_u_max = int(max(corner[0] for corner in rotated_corners))\n        new_v_max = int(max(corner[1] for corner in rotated_corners))\n\n        # Apply translation\n        translation_u, translation_v = translation\n        new_u_min += translation_u\n        new_v_min += translation_v\n        new_u_max += translation_u\n        new_v_max += translation_v\n\n        # Clamp to image boundaries\n        new_u_min = max(0, min(new_u_min, img_width - 1))\n        new_v_min = max(0, min(new_v_min, img_height - 1))\n        new_u_max = max(0, min(new_u_max, img_width - 1))\n        new_v_max = max(0, min(new_v_max, img_height - 1))\n\n        return new_u_min, new_v_min, new_u_max, new_v_max\n\n    def _apply_pose_transform_to_mask(self, rotation_delta: float, translation: tuple[int, int]) -&gt; np.ndarray | None:\n        \"\"\"\n        Apply rotation and translation to segmentation mask.\n\n        Args:\n            rotation_delta (float): Rotation change in radians.\n            translation (tuple[int, int]): (delta_u, delta_v) in pixels.\n\n        Returns:\n            np.ndarray | None: Transformed mask, or None if no mask exists.\n        \"\"\"\n        if self._original_mask_8u is None:\n            return None\n\n        img_width, img_height, _ = self._workspace.img_shape()\n        old_center_u = int(self._u_rel_com * img_width)\n        old_center_v = int(self._v_rel_com * img_height)\n\n        # Rotate mask\n        rotated_mask = self._rotate_mask(self._original_mask_8u, rotation_delta, old_center_u, old_center_v)\n\n        # Update the original mask with the rotated version for future rotations\n        self._original_mask_8u = rotated_mask.copy()\n\n        # Translate mask\n        translation_u, translation_v = translation\n        translated_mask = self._translate_mask(rotated_mask, translation_u, translation_v)\n\n        return translated_mask\n\n    # *** PUBLIC GET methods ***\n\n    def get_workspace_id(self) -&gt; str:\n        \"\"\"\n        Returns the ID of the workspace containing the object.\n\n        Returns:\n            str: Workspace ID.\n        \"\"\"\n        return self.workspace().id()\n\n    # *** PUBLIC methods ***\n\n    def as_string_for_llm(self) -&gt; str:\n        \"\"\"\n        Formats object details as a string for use with language models.\n\n        Returns:\n            str: String containing object information.\n        \"\"\"\n        return f\"\"\"- '{self.label()}' at world coordinates [{self.x_com():.2f}, {self.y_com():.2f}] with a width of {\n        self.width_m():.2f} meters, a height of {self.height_m():.2f} meters and a size of {\n        self.size_m2()*10000:.2f} square centimeters.\"\"\"\n\n    def as_string_for_llm_lbl(self) -&gt; str:\n        \"\"\"\n        Formats object details in a line-by-line style, optimized for language model usage.\n\n        Returns:\n            str: Detailed object information string.\n        \"\"\"\n        return f\"\"\"- '{self.label()}' at world coordinates [{self.x_com():.2f}, {self.y_com():.2f}] with\n    - width: {self.width_m():.2f} meters,\n    - height: {self.height_m():.2f} meters and\n    - size: {self.size_m2() * 10000:.2f} square centimeters.\"\"\"\n\n    def as_string_for_chat_window(self) -&gt; str:\n        \"\"\"\n        Formats object details for display in a chat interface.\n\n        Returns:\n            str: Chat-friendly object description.\n        \"\"\"\n        return f\"\"\"Detected a new object: {self.label()} at world coordinate ({self.x_com():.2f}, {\n        self.y_com():.2f}) with orientation {self.gripper_rotation():.1f} rad and size {\n        self.width_m():.2f} m x {self.height_m():.2f} m.\"\"\"\n\n    # *** NEUE METHODEN F\u00dcR JSON-SERIALISIERUNG ***\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Converts the Object instance to a dictionary that can be JSON serialized.\n\n        Returns:\n            dict[str, Any]: Dictionary representation of the object.\n        \"\"\"\n        return {\n            \"id\": self.generate_object_id(),\n            \"label\": self._label,\n            \"workspace_id\": self.get_workspace_id(),\n            \"timestamp\": time.time(),\n            \"position\": {\n                \"center_of_mass\": {\n                    \"x\": float(self.x_com()),\n                    \"y\": float(self.y_com()),\n                    \"z\": float(self._pose_com.z) if hasattr(self._pose_com, \"z\") else 0.0,\n                },\n                \"center\": {\n                    \"x\": float(self.x_center()),\n                    \"y\": float(self.y_center()),\n                    \"z\": float(self._pose_center.z) if hasattr(self._pose_center, \"z\") else 0.0,\n                },\n            },\n            \"image_coordinates\": {\n                \"center_rel\": {\"u\": float(self._u_rel_o), \"v\": float(self._v_rel_o)},\n                \"center_of_mass_rel\": {\"u\": float(self._u_rel_com), \"v\": float(self._v_rel_com)},\n                \"bounding_box_rel\": {\n                    \"u_min\": float(self._u_rel_min),\n                    \"v_min\": float(self._v_rel_min),\n                    \"u_max\": float(self._u_rel_max),\n                    \"v_max\": float(self._v_rel_max),\n                },\n            },\n            \"dimensions\": {\n                \"width_m\": float(self._width_m),\n                \"height_m\": float(self._height_m),\n                \"size_m2\": float(self._size_m2),\n            },\n            \"gripper_rotation\": float(self._gripper_rotation),\n            \"confidence\": getattr(self, \"_confidence\", 1.0),\n            \"class_id\": getattr(self, \"_class_id\", 0),\n        }\n\n    def to_json(self) -&gt; str:\n        \"\"\"\n        Converts the Object instance to a JSON string.\n\n        Returns:\n            str: JSON representation.\n        \"\"\"\n        return json.dumps(self.to_dict(), indent=2)\n\n    def generate_object_id(self) -&gt; str:\n        \"\"\"\n        Generates a unique ID for the object based on its properties.\n\n        Returns:\n            str: Unique object identifier.\n        \"\"\"\n        id_string = f\"{self._label}_{self.x_com():.3f}_{self.y_com():.3f}_{time.time()}\"\n        return hashlib.md5(id_string.encode(), usedforsecurity=False).hexdigest()[:8]\n\n    @staticmethod\n    def _deserialize_mask(mask_data: str, shape: tuple | list, dtype: str = \"uint8\") -&gt; np.ndarray:\n        \"\"\"\n        Deserialize base64 string back to numpy mask.\n\n        Args:\n            mask_data (str): Base64 encoded mask string.\n            shape (tuple | list): Original shape of the mask (height, width).\n            dtype (str): Data type of the mask (default: 'uint8').\n\n        Returns:\n            np.ndarray: Reconstructed mask array.\n\n        Raises:\n            ValueError: If mask_data is invalid or shape doesn't match.\n        \"\"\"\n        if isinstance(shape, list):\n            shape = tuple(shape)\n\n        try:\n            mask_bytes = base64.b64decode(mask_data.encode(\"utf-8\"))\n\n            # Validate size matches shape\n            dtype_obj = np.dtype(dtype)\n            expected_size = int(np.prod(shape)) * dtype_obj.itemsize\n\n            if len(mask_bytes) != expected_size:\n                raise ValueError(\n                    f\"Mask data size mismatch: expected {expected_size} bytes \"\n                    f\"for shape {shape} with dtype {dtype}, got {len(mask_bytes)} bytes\"\n                )\n\n            mask = np.frombuffer(mask_bytes, dtype=dtype_obj)\n            mask = mask.reshape(shape)\n            return mask\n        except Exception as e:\n            raise ValueError(f\"Failed to deserialize mask: {e}\") from e\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any], workspace: Workspace) -&gt; Object | None:\n        \"\"\"\n        Creates an Object instance from a dictionary.\n\n        Args:\n            data (dict[str, Any]): Dictionary containing object data.\n            workspace (Workspace): Workspace instance.\n\n        Returns:\n            Object | None: Reconstructed object instance or None if fails.\n        \"\"\"\n        logger = logging.getLogger(\"robot_workspace\")\n        try:\n            # Check if we have the new format (from to_dict) or old format (with bbox key)\n            if \"bbox\" in data:\n                # Old format - has bbox directly\n                bbox = data[\"bbox\"]\n                u_min = bbox[\"x_min\"]\n                v_min = bbox[\"y_min\"]\n                u_max = bbox[\"x_max\"]\n                v_max = bbox[\"y_max\"]\n            elif \"image_coordinates\" in data and \"bounding_box_rel\" in data[\"image_coordinates\"]:\n                # New format - convert from relative coordinates\n                bbox_rel = data[\"image_coordinates\"][\"bounding_box_rel\"]\n                img_shape = workspace.img_shape()\n                if img_shape is None:\n                    raise ValueError(\"Workspace image shape is None\")\n\n                u_min = int(bbox_rel[\"u_min\"] * img_shape[0])\n                v_min = int(bbox_rel[\"v_min\"] * img_shape[1])\n                u_max = int(bbox_rel[\"u_max\"] * img_shape[0])\n                v_max = int(bbox_rel[\"v_max\"] * img_shape[1])\n            else:\n                raise KeyError(\"Missing bounding box information in dictionary\")\n\n            # Handle mask if present\n            mask_8u = None\n            if data.get(\"has_mask\", False) and \"mask_data\" in data and \"mask_shape\" in data:\n                mask_8u = cls._deserialize_mask(data[\"mask_data\"], data[\"mask_shape\"], data.get(\"mask_dtype\", \"uint8\"))\n\n            # Create object\n            obj = Object(\n                label=data[\"label\"],\n                u_min=u_min,\n                v_min=v_min,\n                u_max=u_max,\n                v_max=v_max,\n                mask_8u=mask_8u,\n                workspace=workspace,\n            )\n\n            # Restore additional properties if needed\n            if \"confidence\" in data:\n                obj._confidence = data[\"confidence\"]\n            if \"class_id\" in data:\n                obj._class_id = data[\"class_id\"]\n\n            return obj\n\n        except Exception as e:\n            logger.error(f\"Error reconstructing object from dict: {e}\")\n            return None\n\n    @classmethod\n    def from_json(cls, json_str: str, workspace: Workspace) -&gt; Object | None:\n        \"\"\"\n        Creates an Object instance from a JSON string.\n\n        Args:\n            json_str (str): JSON string containing object data.\n            workspace (Workspace): Workspace instance.\n\n        Returns:\n            Object | None: Reconstructed object instance or None if fails.\n        \"\"\"\n        logger = logging.getLogger(\"robot_workspace\")\n        try:\n            data = json.loads(json_str)\n            return cls.from_dict(data, workspace)\n        except Exception as e:\n            logger.error(f\"Error parsing JSON: {e}\")\n            return None\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    @staticmethod\n    def calc_width_height(pose_ul: PoseObjectPNP, pose_lr: PoseObjectPNP) -&gt; tuple[float, float]:\n        \"\"\"\n        Calculates the width and the height between two PoseObjects in meters.\n\n        Args:\n            pose_ul (PoseObjectPNP): PoseObject in the upper left corner.\n            pose_lr (PoseObjectPNP): PoseObject in the lower right corner.\n\n        Returns:\n            tuple[float, float]: (width, height) in meters.\n        \"\"\"\n        width_m = pose_ul.y - pose_lr.y\n        height_m = pose_ul.x - pose_lr.x\n\n        return width_m, height_m\n\n    # *** PRIVATE methods ***\n\n    def _world_to_rel_coordinates(self, pose: PoseObjectPNP) -&gt; tuple[float, float]:\n        \"\"\"\n        Converts world coordinates to relative image coordinates.\n\n        Args:\n            pose (PoseObjectPNP): Pose in world coordinates.\n\n        Returns:\n            tuple[float, float]: (u_rel, v_rel) normalized [0, 1].\n        \"\"\"\n        # Get workspace corners in world coordinates\n        ul = self._workspace.xy_ul_wc()\n        lr = self._workspace.xy_lr_wc()\n\n        if ul is None or lr is None:\n            return 0.5, 0.5\n\n        x_range = lr.x - ul.x\n        u_rel = (pose.x - ul.x) / x_range if abs(x_range) &gt; 1e-06 else 0.5\n\n        y_range = ul.y - lr.y\n        v_rel = (ul.y - pose.y) / y_range if abs(y_range) &gt; 1e-06 else 0.5\n\n        u_rel = max(0.0, min(1.0, u_rel))\n        v_rel = max(0.0, min(1.0, v_rel))\n\n        return float(u_rel), float(v_rel)\n\n    def _rotate_bounding_box(\n        self, u_min: int, v_min: int, u_max: int, v_max: int, center_u: int, center_v: int, angle: float\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"\n        Rotates the four corners of a bounding box around a center point.\n\n        Args:\n            u_min, v_min (int): Top-left corner.\n            u_max, v_max (int): Bottom-right corner.\n            center_u, center_v (int): Center of rotation.\n            angle (float): Rotation angle in radians (counter-clockwise).\n\n        Returns:\n            list[tuple[int, int]]: Four rotated corner coordinates.\n        \"\"\"\n        corners = [\n            (u_min, v_min),\n            (u_max, v_min),\n            (u_max, v_max),\n            (u_min, v_max),\n        ]\n\n        rotated_corners = []\n        cos_angle = math.cos(angle)\n        sin_angle = math.sin(angle)\n\n        for u, v in corners:\n            u_translated = u - center_u\n            v_translated = v - center_v\n\n            u_rotated = u_translated * cos_angle - v_translated * sin_angle\n            v_rotated = u_translated * sin_angle + v_translated * cos_angle\n\n            u_final = int(u_rotated + center_u)\n            v_final = int(v_rotated + center_v)\n\n            rotated_corners.append((u_final, v_final))\n\n        return rotated_corners\n\n    def _rotate_mask(self, mask: np.ndarray, angle: float, center_u: int, center_v: int) -&gt; np.ndarray | None:\n        \"\"\"\n        Rotates a segmentation mask around a center point.\n\n        Args:\n            mask (np.ndarray): Original mask.\n            angle (float): Rotation angle in radians.\n            center_u, center_v (int): Center of rotation.\n\n        Returns:\n            np.ndarray | None: Rotated mask.\n        \"\"\"\n        if mask is None:\n            return None\n\n        angle_degrees = -math.degrees(angle)\n\n        height, width = mask.shape[:2]\n        rotation_matrix = cv2.getRotationMatrix2D((float(center_u), float(center_v)), angle_degrees, 1.0)\n\n        rotated_mask = cv2.warpAffine(mask, rotation_matrix, (width, height), flags=cv2.INTER_NEAREST)\n\n        return rotated_mask\n\n    def _translate_mask(self, mask: np.ndarray, delta_u: int, delta_v: int) -&gt; np.ndarray | None:\n        \"\"\"\n        Translates a segmentation mask by a given offset.\n\n        Args:\n            mask (np.ndarray): Original mask.\n            delta_u, delta_v (int): Translation offsets.\n\n        Returns:\n            np.ndarray | None: Translated mask.\n        \"\"\"\n        if mask is None:\n            return None\n\n        height, width = mask.shape[:2]\n        translation_matrix = np.float32([[1, 0, delta_u], [0, 1, delta_v]])\n\n        translated_mask = cv2.warpAffine(mask, translation_matrix, (width, height), flags=cv2.INTER_NEAREST)\n\n        return translated_mask\n\n    def _calc_size(self) -&gt; None:\n        \"\"\"Calculates the object's area in square meters.\"\"\"\n        area = self._calculate_largest_contour_area()\n\n        if area == 0:\n            self._size_m2 = self._width_m * self._height_m\n        else:\n            ratio_w, ratio_h = self._calc_size_of_pixel_in_m()\n            img_shape = self._workspace.img_shape()\n            if img_shape:\n                width, height, _nchannels = img_shape\n                area_img = width * height\n                self._size_m2 = float(area) / float(area_img) * ratio_w * ratio_h\n            else:\n                self._size_m2 = 0.0\n\n    def _calc_size_of_pixel_in_m(self) -&gt; tuple[float, float]:\n        \"\"\"\n        Computes the physical size of the workspace in meters.\n\n        Returns:\n            tuple[float, float]: (width_m, height_m).\n        \"\"\"\n        if self._width == 0 or self._height == 0:\n            return 0.0, 0.0\n        ratio_w = float(self._width_m / self._width)\n        ratio_h = float(self._height_m / self._height)\n\n        if self.verbose():\n            self._logger.debug(f\"Pixel size ratios - width: {ratio_w}, height: {ratio_h}\")\n\n        return ratio_w, ratio_h\n\n    def _update_width_height(self, width: int, height: int) -&gt; None:\n        \"\"\"\n        Updates the object's width and height in meters.\n\n        Args:\n            width (int): Width in pixels.\n            height (int): Height in pixels.\n        \"\"\"\n        ratio_w, ratio_h = self._calc_size_of_pixel_in_m()\n\n        self._width, self._height = self._calc_rel_coordinates(width, height)\n\n        self._width_m = self._width * ratio_w\n        self._height_m = self._height * ratio_h\n\n    def _calc_pose_from_uv_coords(self, u: int, v: int) -&gt; tuple[PoseObjectPNP, float, float]:\n        \"\"\"\n        Calculates the object's pose based on pixel coordinates.\n\n        Args:\n            u (int): Pixel u-coordinate.\n            v (int): Pixel v-coordinate.\n\n        Returns:\n            tuple[PoseObjectPNP, float, float]: (pose, u_rel, v_rel).\n        \"\"\"\n        u_rel, v_rel = self._calc_rel_coordinates(u, v)\n\n        pose = self._workspace.transform_camera2world_coords(self._workspace.id(), u_rel, v_rel, self._gripper_rotation)\n\n        return pose, u_rel, v_rel\n\n    @log_start_end_cls()\n    def _calc_width_height_m(self) -&gt; None:\n        \"\"\"Calculates physical width and height in meters.\"\"\"\n        if self.verbose():\n            print(self._label, self._u_rel_min)\n\n        pose_min = self._workspace.transform_camera2world_coords(self._workspace.id(), self._u_rel_min, self._v_rel_min)\n        pose_max = self._workspace.transform_camera2world_coords(self._workspace.id(), self._u_rel_max, self._v_rel_max)\n\n        if self.verbose():\n            print(pose_min, pose_max)\n\n        self._width_m, self._height_m = Object.calc_width_height(pose_min, pose_max)\n\n    def _calc_rel_coordinates(self, u: int, v: int) -&gt; tuple[float, float]:\n        \"\"\"\n        Converts pixel coordinates to relative coordinates (0-1).\n\n        Args:\n            u (int): Pixel u-coordinate.\n            v (int): Pixel v-coordinate.\n\n        Returns:\n            tuple[float, float]: (u_rel, v_rel).\n\n        Raises:\n            ValueError: If workspace image shape is invalid.\n        \"\"\"\n        img_workspace_shape = self._workspace.img_shape()\n        if img_workspace_shape is None:\n            return 0.5, 0.5\n\n        if img_workspace_shape[0] == 0 or img_workspace_shape[1] == 0:\n            raise ValueError(\n                f\"Invalid workspace image shape: {img_workspace_shape}. \"\n                \"Call workspace.set_img_shape() before creating objects.\"\n            )\n\n        u_rel = float(u / img_workspace_shape[0])\n        v_rel = float(v / img_workspace_shape[1])\n\n        return u_rel, v_rel\n\n    @log_start_end_cls()\n    def _calc_largest_contour(self, mask_8u: np.ndarray) -&gt; None:\n        \"\"\"\n        Determine the largest contour in a 2D segmentation mask.\n\n        Args:\n            mask_8u (np.ndarray): 8-bit unsigned integer array.\n\n        Raises:\n            ValueError: If mask_8u is not uint8.\n        \"\"\"\n        if mask_8u.dtype != np.uint8:\n            raise ValueError(\"Input mask must be an 8-bit unsigned integer array.\")\n\n        contours, _ = cv2.findContours(mask_8u, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if not contours:\n            self._largest_contour = None\n            return\n\n        self._largest_contour = max(contours, key=cv2.contourArea)\n\n    def _calculate_largest_contour_area(self) -&gt; int:\n        \"\"\"\n        Computes the area of the largest contour.\n\n        Returns:\n            int: Area in pixels.\n        \"\"\"\n        if self._largest_contour is None or len(self._largest_contour) == 0:\n            return 0\n\n        largest_area = cv2.contourArea(self._largest_contour)\n\n        return int(largest_area)\n\n    def _rotated_bounding_box(self) -&gt; tuple[int, int]:\n        \"\"\"\n        Computes the dimensions of the rotated bounding box.\n\n        Returns:\n            tuple[int, int]: (width, height) in pixels.\n        \"\"\"\n        _center, (width, height), _theta = self._get_params_of_min_area_rect()\n\n        if width == 0 and height == 0:\n            return width, height\n\n        if self._width &gt; self._height:\n            return max(width, height), min(width, height)\n        else:\n            return min(width, height), max(width, height)\n\n    def _get_params_of_min_area_rect(self) -&gt; tuple[tuple[int, int], tuple[int, int], float]:\n        \"\"\"\n        Calculate parameters of the minimum area rectangle.\n\n        Returns:\n            tuple: (center, dimensions, theta).\n        \"\"\"\n        if self._largest_contour is None or len(self._largest_contour) == 0:\n            return (0, 0), (0, 0), 0\n        else:\n            rect = cv2.minAreaRect(self._largest_contour)\n            center, (width, height), theta = rect\n            return (int(center[0]), int(center[1])), (int(width), int(height)), theta\n\n    def _calc_gripper_orientation_from_segmentation_mask(self) -&gt; tuple[float, tuple[int, int]]:\n        \"\"\"\n        Determines the optimal orientation for the robot gripper.\n\n        Returns:\n            tuple[float, tuple[int, int]]: (rotation_radians, center_pixels).\n        \"\"\"\n        gripper_rotation = 0.0\n\n        center, (width, height), theta = self._get_params_of_min_area_rect()\n\n        if not (width == 0 and height == 0):\n            theta_rad = math.radians(theta)\n            yaw_rel = theta_rad + math.pi / 2 if width &lt; height else theta_rad\n            yaw_rel += math.pi / 2\n            yaw_rel = yaw_rel % (2 * math.pi)\n            gripper_rotation = yaw_rel\n\n        return gripper_rotation, center\n\n    # *** PRIVATE STATIC/CLASS methods ***\n\n    @staticmethod\n    def _calculate_center_of_mass(mask_8u: np.ndarray) -&gt; tuple[float, float] | None:\n        \"\"\"\n        Calculates the center of mass of an object in a mask.\n\n        Args:\n            mask_8u (np.ndarray): 2D mask (uint8).\n\n        Returns:\n            tuple[float, float] | None: (cx, cy) in pixels, or None.\n        \"\"\"\n        binary_mask = (mask_8u &gt; 0).astype(np.uint8)\n        non_zero_indices = np.nonzero(binary_mask)\n\n        if len(non_zero_indices[0]) == 0:\n            return None\n\n        cx = np.mean(non_zero_indices[1])\n        cy = np.mean(non_zero_indices[0])\n\n        return float(cx), float(cy)\n\n    # *** PUBLIC properties ***\n\n    def label(self) -&gt; str:\n        \"\"\"Object label.\"\"\"\n        return self._label\n\n    def uv_rel_o(self) -&gt; tuple[float, float]:\n        \"\"\"Object center in relative coordinates.\"\"\"\n        return self._u_rel_o, self._v_rel_o\n\n    def u_rel_o(self) -&gt; float:\n        \"\"\"Object center u-coordinate.\"\"\"\n        return self._u_rel_o\n\n    def v_rel_o(self) -&gt; float:\n        \"\"\"Object center v-coordinate.\"\"\"\n        return self._v_rel_o\n\n    def pose_center(self) -&gt; PoseObjectPNP:\n        \"\"\"Geometric center pose.\"\"\"\n        if self._pose_center is None:\n            raise ValueError(\"Pose center not initialized\")\n        return self._pose_center\n\n    def x_center(self) -&gt; float:\n        \"\"\"Geometric center x (meters).\"\"\"\n        return self.pose_center().x\n\n    def y_center(self) -&gt; float:\n        \"\"\"Geometric center y (meters).\"\"\"\n        return self.pose_center().y\n\n    def xy_center(self) -&gt; tuple[float, float]:\n        \"\"\"Geometric center (x, y) (meters).\"\"\"\n        pc = self.pose_center()\n        return pc.x, pc.y\n\n    def pose_com(self) -&gt; PoseObjectPNP:\n        \"\"\"Center of mass pose.\"\"\"\n        if self._pose_com is None:\n            raise ValueError(\"Pose CoM not initialized\")\n        return self._pose_com\n\n    def x_com(self) -&gt; float:\n        \"\"\"Center of mass x (meters).\"\"\"\n        return self.pose_com().x\n\n    def y_com(self) -&gt; float:\n        \"\"\"Center of mass y (meters).\"\"\"\n        return self.pose_com().y\n\n    def xy_com(self) -&gt; tuple[float, float]:\n        \"\"\"Center of mass (x, y) (meters).\"\"\"\n        pcom = self.pose_com()\n        return pcom.x, pcom.y\n\n    def coordinate(self) -&gt; list[float]:\n        \"\"\"Center of mass (x, y) (meters).\"\"\"\n        pcom = self.pose_com()\n        return [pcom.x, pcom.y]\n\n    def shape_m(self) -&gt; tuple[float, float]:\n        \"\"\"Width and height in meters.\"\"\"\n        return self._width_m, self._height_m\n\n    def width_m(self) -&gt; float:\n        \"\"\"Width in meters.\"\"\"\n        return self._width_m\n\n    def height_m(self) -&gt; float:\n        \"\"\"Height in meters.\"\"\"\n        return self._height_m\n\n    def size_m2(self) -&gt; float:\n        \"\"\"Area in square meters.\"\"\"\n        return self._size_m2\n\n    def largest_contour(self) -&gt; np.ndarray | None:\n        \"\"\"Largest contour of mask.\"\"\"\n        return self._largest_contour\n\n    def gripper_rotation(self) -&gt; float:\n        \"\"\"Gripper rotation (radians).\"\"\"\n        return self._gripper_rotation\n\n    def workspace(self) -&gt; Workspace:\n        \"\"\"Associated workspace.\"\"\"\n        return self._workspace\n\n    def verbose(self) -&gt; bool:\n        \"\"\"Verbose mode status.\"\"\"\n        return self._verbose\n\n    # *** PRIVATE variables ***\n\n    _label: str = \"\"\n    _u_rel_o: float = 0.0\n    _v_rel_o: float = 0.0\n    _pose_center: PoseObjectPNP | None = None\n    _u_rel_com: float = 0.0\n    _v_rel_com: float = 0.0\n    _pose_com: PoseObjectPNP | None = None\n    _u_rel_min: float = 0.0\n    _v_rel_min: float = 0.0\n    _u_rel_max: float = 0.0\n    _v_rel_max: float = 0.0\n    _width: float = 0.0\n    _height: float = 0.0\n    _width_m: float = 0.0\n    _height_m: float = 0.0\n    _depth_m: float = 0.0\n    _size_m2: float = 0.0\n    _largest_contour: np.ndarray | None = None\n    _gripper_rotation: float = 0.0\n    _workspace: Workspace = None\n    _original_mask_8u: np.ndarray | None = None\n    _verbose: bool = False\n    _confidence: float = 1.0\n    _class_id: int = 0\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.objects.object.Object.__init__","title":"<code>__init__(label, u_min, v_min, u_max, v_max, mask_8u, workspace, verbose=False)</code>","text":"<p>Initializes an Object instance.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Label of the object (e.g., \"chocolate bar\").</p> required <code>u_min</code> <code>int</code> <p>Upper-left corner u-coordinate of the bounding box (in pixels).</p> required <code>v_min</code> <code>int</code> <p>Upper-left corner v-coordinate of the bounding box (in pixels).</p> required <code>u_max</code> <code>int</code> <p>Lower-right corner u-coordinate of the bounding box (in pixels).</p> required <code>v_max</code> <code>int</code> <p>Lower-right corner v-coordinate of the bounding box (in pixels).</p> required <code>mask_8u</code> <code>ndarray</code> <p>Segmentation mask of the object (8-bit, uint8).</p> required <code>workspace</code> <code>Workspace</code> <p>Workspace instance where the object is located.</p> required <code>verbose</code> <code>bool</code> <p>If True, enables verbose logging.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided segmentation mask is not 8-bit unsigned, or if the workspace has no image shape.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>@log_start_end_cls()\ndef __init__(\n    self,\n    label: str,\n    u_min: int,\n    v_min: int,\n    u_max: int,\n    v_max: int,\n    mask_8u: np.ndarray | None,\n    workspace: Workspace,\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes an Object instance.\n\n    Args:\n        label (str): Label of the object (e.g., \"chocolate bar\").\n        u_min (int): Upper-left corner u-coordinate of the bounding box (in pixels).\n        v_min (int): Upper-left corner v-coordinate of the bounding box (in pixels).\n        u_max (int): Lower-right corner u-coordinate of the bounding box (in pixels).\n        v_max (int): Lower-right corner v-coordinate of the bounding box (in pixels).\n        mask_8u (np.ndarray, optional): Segmentation mask of the object (8-bit, uint8).\n        workspace (Workspace): Workspace instance where the object is located.\n        verbose (bool): If True, enables verbose logging.\n\n    Raises:\n        ValueError: If the provided segmentation mask is not 8-bit unsigned,\n            or if the workspace has no image shape.\n    \"\"\"\n    super().__init__()\n\n    self._label = label\n    self._workspace = workspace\n    self._verbose = verbose\n    self._logger = logging.getLogger(\"robot_workspace\")\n\n    if self._workspace.img_shape() is None:\n        raise ValueError(\"Object has no image shape. This probably means that the Niryo Workspace was not detected.\")\n\n    # Store original mask for rotation\n    self._original_mask_8u = mask_8u.copy() if mask_8u is not None else None\n\n    # Initialize the object with the common initialization logic\n    self._init_object_properties(u_min, v_min, u_max, v_max, mask_8u)\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.as_string_for_chat_window","title":"<code>as_string_for_chat_window()</code>","text":"<p>Formats object details for display in a chat interface.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Chat-friendly object description.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def as_string_for_chat_window(self) -&gt; str:\n    \"\"\"\n    Formats object details for display in a chat interface.\n\n    Returns:\n        str: Chat-friendly object description.\n    \"\"\"\n    return f\"\"\"Detected a new object: {self.label()} at world coordinate ({self.x_com():.2f}, {\n    self.y_com():.2f}) with orientation {self.gripper_rotation():.1f} rad and size {\n    self.width_m():.2f} m x {self.height_m():.2f} m.\"\"\"\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.as_string_for_llm","title":"<code>as_string_for_llm()</code>","text":"<p>Formats object details as a string for use with language models.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String containing object information.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def as_string_for_llm(self) -&gt; str:\n    \"\"\"\n    Formats object details as a string for use with language models.\n\n    Returns:\n        str: String containing object information.\n    \"\"\"\n    return f\"\"\"- '{self.label()}' at world coordinates [{self.x_com():.2f}, {self.y_com():.2f}] with a width of {\n    self.width_m():.2f} meters, a height of {self.height_m():.2f} meters and a size of {\n    self.size_m2()*10000:.2f} square centimeters.\"\"\"\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.as_string_for_llm_lbl","title":"<code>as_string_for_llm_lbl()</code>","text":"<p>Formats object details in a line-by-line style, optimized for language model usage.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Detailed object information string.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def as_string_for_llm_lbl(self) -&gt; str:\n    \"\"\"\n    Formats object details in a line-by-line style, optimized for language model usage.\n\n    Returns:\n        str: Detailed object information string.\n    \"\"\"\n    return f\"\"\"- '{self.label()}' at world coordinates [{self.x_com():.2f}, {self.y_com():.2f}] with\n- width: {self.width_m():.2f} meters,\n- height: {self.height_m():.2f} meters and\n- size: {self.size_m2() * 10000:.2f} square centimeters.\"\"\"\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.calc_width_height","title":"<code>calc_width_height(pose_ul, pose_lr)</code>  <code>staticmethod</code>","text":"<p>Calculates the width and the height between two PoseObjects in meters.</p> <p>Parameters:</p> Name Type Description Default <code>pose_ul</code> <code>PoseObjectPNP</code> <p>PoseObject in the upper left corner.</p> required <code>pose_lr</code> <code>PoseObjectPNP</code> <p>PoseObject in the lower right corner.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: (width, height) in meters.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>@staticmethod\ndef calc_width_height(pose_ul: PoseObjectPNP, pose_lr: PoseObjectPNP) -&gt; tuple[float, float]:\n    \"\"\"\n    Calculates the width and the height between two PoseObjects in meters.\n\n    Args:\n        pose_ul (PoseObjectPNP): PoseObject in the upper left corner.\n        pose_lr (PoseObjectPNP): PoseObject in the lower right corner.\n\n    Returns:\n        tuple[float, float]: (width, height) in meters.\n    \"\"\"\n    width_m = pose_ul.y - pose_lr.y\n    height_m = pose_ul.x - pose_lr.x\n\n    return width_m, height_m\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.coordinate","title":"<code>coordinate()</code>","text":"<p>Center of mass (x, y) (meters).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def coordinate(self) -&gt; list[float]:\n    \"\"\"Center of mass (x, y) (meters).\"\"\"\n    pcom = self.pose_com()\n    return [pcom.x, pcom.y]\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.from_dict","title":"<code>from_dict(data, workspace)</code>  <code>classmethod</code>","text":"<p>Creates an Object instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary containing object data.</p> required <code>workspace</code> <code>Workspace</code> <p>Workspace instance.</p> required <p>Returns:</p> Type Description <code>Object | None</code> <p>Object | None: Reconstructed object instance or None if fails.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any], workspace: Workspace) -&gt; Object | None:\n    \"\"\"\n    Creates an Object instance from a dictionary.\n\n    Args:\n        data (dict[str, Any]): Dictionary containing object data.\n        workspace (Workspace): Workspace instance.\n\n    Returns:\n        Object | None: Reconstructed object instance or None if fails.\n    \"\"\"\n    logger = logging.getLogger(\"robot_workspace\")\n    try:\n        # Check if we have the new format (from to_dict) or old format (with bbox key)\n        if \"bbox\" in data:\n            # Old format - has bbox directly\n            bbox = data[\"bbox\"]\n            u_min = bbox[\"x_min\"]\n            v_min = bbox[\"y_min\"]\n            u_max = bbox[\"x_max\"]\n            v_max = bbox[\"y_max\"]\n        elif \"image_coordinates\" in data and \"bounding_box_rel\" in data[\"image_coordinates\"]:\n            # New format - convert from relative coordinates\n            bbox_rel = data[\"image_coordinates\"][\"bounding_box_rel\"]\n            img_shape = workspace.img_shape()\n            if img_shape is None:\n                raise ValueError(\"Workspace image shape is None\")\n\n            u_min = int(bbox_rel[\"u_min\"] * img_shape[0])\n            v_min = int(bbox_rel[\"v_min\"] * img_shape[1])\n            u_max = int(bbox_rel[\"u_max\"] * img_shape[0])\n            v_max = int(bbox_rel[\"v_max\"] * img_shape[1])\n        else:\n            raise KeyError(\"Missing bounding box information in dictionary\")\n\n        # Handle mask if present\n        mask_8u = None\n        if data.get(\"has_mask\", False) and \"mask_data\" in data and \"mask_shape\" in data:\n            mask_8u = cls._deserialize_mask(data[\"mask_data\"], data[\"mask_shape\"], data.get(\"mask_dtype\", \"uint8\"))\n\n        # Create object\n        obj = Object(\n            label=data[\"label\"],\n            u_min=u_min,\n            v_min=v_min,\n            u_max=u_max,\n            v_max=v_max,\n            mask_8u=mask_8u,\n            workspace=workspace,\n        )\n\n        # Restore additional properties if needed\n        if \"confidence\" in data:\n            obj._confidence = data[\"confidence\"]\n        if \"class_id\" in data:\n            obj._class_id = data[\"class_id\"]\n\n        return obj\n\n    except Exception as e:\n        logger.error(f\"Error reconstructing object from dict: {e}\")\n        return None\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.from_json","title":"<code>from_json(json_str, workspace)</code>  <code>classmethod</code>","text":"<p>Creates an Object instance from a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>JSON string containing object data.</p> required <code>workspace</code> <code>Workspace</code> <p>Workspace instance.</p> required <p>Returns:</p> Type Description <code>Object | None</code> <p>Object | None: Reconstructed object instance or None if fails.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str, workspace: Workspace) -&gt; Object | None:\n    \"\"\"\n    Creates an Object instance from a JSON string.\n\n    Args:\n        json_str (str): JSON string containing object data.\n        workspace (Workspace): Workspace instance.\n\n    Returns:\n        Object | None: Reconstructed object instance or None if fails.\n    \"\"\"\n    logger = logging.getLogger(\"robot_workspace\")\n    try:\n        data = json.loads(json_str)\n        return cls.from_dict(data, workspace)\n    except Exception as e:\n        logger.error(f\"Error parsing JSON: {e}\")\n        return None\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.generate_object_id","title":"<code>generate_object_id()</code>","text":"<p>Generates a unique ID for the object based on its properties.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique object identifier.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def generate_object_id(self) -&gt; str:\n    \"\"\"\n    Generates a unique ID for the object based on its properties.\n\n    Returns:\n        str: Unique object identifier.\n    \"\"\"\n    id_string = f\"{self._label}_{self.x_com():.3f}_{self.y_com():.3f}_{time.time()}\"\n    return hashlib.md5(id_string.encode(), usedforsecurity=False).hexdigest()[:8]\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.get_workspace_id","title":"<code>get_workspace_id()</code>","text":"<p>Returns the ID of the workspace containing the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Workspace ID.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def get_workspace_id(self) -&gt; str:\n    \"\"\"\n    Returns the ID of the workspace containing the object.\n\n    Returns:\n        str: Workspace ID.\n    \"\"\"\n    return self.workspace().id()\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.gripper_rotation","title":"<code>gripper_rotation()</code>","text":"<p>Gripper rotation (radians).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def gripper_rotation(self) -&gt; float:\n    \"\"\"Gripper rotation (radians).\"\"\"\n    return self._gripper_rotation\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.height_m","title":"<code>height_m()</code>","text":"<p>Height in meters.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def height_m(self) -&gt; float:\n    \"\"\"Height in meters.\"\"\"\n    return self._height_m\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.label","title":"<code>label()</code>","text":"<p>Object label.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def label(self) -&gt; str:\n    \"\"\"Object label.\"\"\"\n    return self._label\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.largest_contour","title":"<code>largest_contour()</code>","text":"<p>Largest contour of mask.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def largest_contour(self) -&gt; np.ndarray | None:\n    \"\"\"Largest contour of mask.\"\"\"\n    return self._largest_contour\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.pose_center","title":"<code>pose_center()</code>","text":"<p>Geometric center pose.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def pose_center(self) -&gt; PoseObjectPNP:\n    \"\"\"Geometric center pose.\"\"\"\n    if self._pose_center is None:\n        raise ValueError(\"Pose center not initialized\")\n    return self._pose_center\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.pose_com","title":"<code>pose_com()</code>","text":"<p>Center of mass pose.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def pose_com(self) -&gt; PoseObjectPNP:\n    \"\"\"Center of mass pose.\"\"\"\n    if self._pose_com is None:\n        raise ValueError(\"Pose CoM not initialized\")\n    return self._pose_com\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.set_pose_com","title":"<code>set_pose_com(pose_com)</code>","text":"<p>Updates the object's center of mass pose and recalculates all dependent properties.</p> <p>Handles both position changes and orientation changes (rotation around z-axis).</p> <p>Parameters:</p> Name Type Description Default <code>pose_com</code> <code>PoseObjectPNP</code> <p>New pose for the object's center of mass.</p> required Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def set_pose_com(self, pose_com: PoseObjectPNP) -&gt; None:\n    \"\"\"\n    Updates the object's center of mass pose and recalculates all dependent properties.\n\n    Handles both position changes and orientation changes (rotation around z-axis).\n\n    Args:\n        pose_com (PoseObjectPNP): New pose for the object's center of mass.\n    \"\"\"\n    if self.verbose():\n        self._logger.debug(f\"set_pose_com: {self._pose_com} \u2192 {pose_com}\")\n\n    # 1. Calculate transformations\n    rotation_delta = pose_com.yaw - self._gripper_rotation\n    translation = self._calculate_translation(pose_com)\n\n    # 2. Transform bounding box\n    new_u_min, new_v_min, new_u_max, new_v_max = self._apply_pose_transform_to_bbox(rotation_delta, translation)\n\n    # 3. Transform mask if present\n    transformed_mask = self._apply_pose_transform_to_mask(rotation_delta, translation)\n\n    # 4. Reinitialize with new geometry\n    self._init_object_properties(new_u_min, new_v_min, new_u_max, new_v_max, transformed_mask)\n\n    # 5. Override calculated pose with exact provided pose\n    self._pose_com = pose_com\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.set_position","title":"<code>set_position(xy_coordinate)</code>","text":"<p>Legacy method kept for backwards compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>xy_coordinate</code> <code>list[float]</code> <p>[x, y] world coordinates in meters.</p> required Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def set_position(self, xy_coordinate: list[float]) -&gt; None:\n    \"\"\"\n    Legacy method kept for backwards compatibility.\n\n    Args:\n        xy_coordinate (list[float]): [x, y] world coordinates in meters.\n    \"\"\"\n    # Create a pose with the new x, y but keeping old z and orientation\n    new_pose = self._pose_com.copy_with_offsets(\n        x_offset=xy_coordinate[0] - self._pose_com.x, y_offset=xy_coordinate[1] - self._pose_com.y\n    )\n    self.set_pose_com(new_pose)\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.shape_m","title":"<code>shape_m()</code>","text":"<p>Width and height in meters.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def shape_m(self) -&gt; tuple[float, float]:\n    \"\"\"Width and height in meters.\"\"\"\n    return self._width_m, self._height_m\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.size_m2","title":"<code>size_m2()</code>","text":"<p>Area in square meters.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def size_m2(self) -&gt; float:\n    \"\"\"Area in square meters.\"\"\"\n    return self._size_m2\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.to_dict","title":"<code>to_dict()</code>","text":"<p>Converts the Object instance to a dictionary that can be JSON serialized.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary representation of the object.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Converts the Object instance to a dictionary that can be JSON serialized.\n\n    Returns:\n        dict[str, Any]: Dictionary representation of the object.\n    \"\"\"\n    return {\n        \"id\": self.generate_object_id(),\n        \"label\": self._label,\n        \"workspace_id\": self.get_workspace_id(),\n        \"timestamp\": time.time(),\n        \"position\": {\n            \"center_of_mass\": {\n                \"x\": float(self.x_com()),\n                \"y\": float(self.y_com()),\n                \"z\": float(self._pose_com.z) if hasattr(self._pose_com, \"z\") else 0.0,\n            },\n            \"center\": {\n                \"x\": float(self.x_center()),\n                \"y\": float(self.y_center()),\n                \"z\": float(self._pose_center.z) if hasattr(self._pose_center, \"z\") else 0.0,\n            },\n        },\n        \"image_coordinates\": {\n            \"center_rel\": {\"u\": float(self._u_rel_o), \"v\": float(self._v_rel_o)},\n            \"center_of_mass_rel\": {\"u\": float(self._u_rel_com), \"v\": float(self._v_rel_com)},\n            \"bounding_box_rel\": {\n                \"u_min\": float(self._u_rel_min),\n                \"v_min\": float(self._v_rel_min),\n                \"u_max\": float(self._u_rel_max),\n                \"v_max\": float(self._v_rel_max),\n            },\n        },\n        \"dimensions\": {\n            \"width_m\": float(self._width_m),\n            \"height_m\": float(self._height_m),\n            \"size_m2\": float(self._size_m2),\n        },\n        \"gripper_rotation\": float(self._gripper_rotation),\n        \"confidence\": getattr(self, \"_confidence\", 1.0),\n        \"class_id\": getattr(self, \"_class_id\", 0),\n    }\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.to_json","title":"<code>to_json()</code>","text":"<p>Converts the Object instance to a JSON string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON representation.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"\n    Converts the Object instance to a JSON string.\n\n    Returns:\n        str: JSON representation.\n    \"\"\"\n    return json.dumps(self.to_dict(), indent=2)\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.u_rel_o","title":"<code>u_rel_o()</code>","text":"<p>Object center u-coordinate.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def u_rel_o(self) -&gt; float:\n    \"\"\"Object center u-coordinate.\"\"\"\n    return self._u_rel_o\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.uv_rel_o","title":"<code>uv_rel_o()</code>","text":"<p>Object center in relative coordinates.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def uv_rel_o(self) -&gt; tuple[float, float]:\n    \"\"\"Object center in relative coordinates.\"\"\"\n    return self._u_rel_o, self._v_rel_o\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.v_rel_o","title":"<code>v_rel_o()</code>","text":"<p>Object center v-coordinate.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def v_rel_o(self) -&gt; float:\n    \"\"\"Object center v-coordinate.\"\"\"\n    return self._v_rel_o\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.verbose","title":"<code>verbose()</code>","text":"<p>Verbose mode status.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def verbose(self) -&gt; bool:\n    \"\"\"Verbose mode status.\"\"\"\n    return self._verbose\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.width_m","title":"<code>width_m()</code>","text":"<p>Width in meters.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def width_m(self) -&gt; float:\n    \"\"\"Width in meters.\"\"\"\n    return self._width_m\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.workspace","title":"<code>workspace()</code>","text":"<p>Associated workspace.</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def workspace(self) -&gt; Workspace:\n    \"\"\"Associated workspace.\"\"\"\n    return self._workspace\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.x_center","title":"<code>x_center()</code>","text":"<p>Geometric center x (meters).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def x_center(self) -&gt; float:\n    \"\"\"Geometric center x (meters).\"\"\"\n    return self.pose_center().x\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.x_com","title":"<code>x_com()</code>","text":"<p>Center of mass x (meters).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def x_com(self) -&gt; float:\n    \"\"\"Center of mass x (meters).\"\"\"\n    return self.pose_com().x\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.xy_center","title":"<code>xy_center()</code>","text":"<p>Geometric center (x, y) (meters).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def xy_center(self) -&gt; tuple[float, float]:\n    \"\"\"Geometric center (x, y) (meters).\"\"\"\n    pc = self.pose_center()\n    return pc.x, pc.y\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.xy_com","title":"<code>xy_com()</code>","text":"<p>Center of mass (x, y) (meters).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def xy_com(self) -&gt; tuple[float, float]:\n    \"\"\"Center of mass (x, y) (meters).\"\"\"\n    pcom = self.pose_com()\n    return pcom.x, pcom.y\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.y_center","title":"<code>y_center()</code>","text":"<p>Geometric center y (meters).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def y_center(self) -&gt; float:\n    \"\"\"Geometric center y (meters).\"\"\"\n    return self.pose_center().y\n</code></pre>"},{"location":"api/#robot_workspace.objects.object.Object.y_com","title":"<code>y_com()</code>","text":"<p>Center of mass y (meters).</p> Source code in <code>robot_workspace/objects/object.py</code> <pre><code>def y_com(self) -&gt; float:\n    \"\"\"Center of mass y (meters).\"\"\"\n    return self.pose_com().y\n</code></pre>"},{"location":"api/#robot_workspace.objects.object-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.objects.objects","title":"<code>robot_workspace.objects.objects</code>","text":""},{"location":"api/#robot_workspace.objects.objects-classes","title":"Classes","text":""},{"location":"api/#robot_workspace.objects.objects.Objects","title":"<code>Objects</code>","text":"<p>               Bases: <code>list[Object]</code></p> <p>A class representing a list of Object instances.</p> <p>Objects are typically stored in a workspace and represent physical entities detected by vision. This class provides several spatial query methods to find objects based on coordinates, labels, or size.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>class Objects(list[Object]):\n    \"\"\"\n    A class representing a list of Object instances.\n\n    Objects are typically stored in a workspace and represent physical entities detected by vision.\n    This class provides several spatial query methods to find objects based on coordinates, labels, or size.\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(self, iterable: Iterable[Object] | None = None, verbose: bool = False) -&gt; None:\n        \"\"\"\n        Initializes the Objects instance.\n\n        Args:\n            iterable (Iterable[Object], optional): An iterable of Object instances. Defaults to an empty list.\n            verbose (bool): If True, enables verbose logging.\n        \"\"\"\n        if iterable is None:\n            iterable = []\n        super().__init__(iterable)\n\n        self._verbose = verbose\n\n    # *** PUBLIC SET methods ***\n\n    # *** PUBLIC GET methods ***\n\n    # *** PUBLIC methods ***\n\n    @log_start_end_cls()\n    def get_detected_object(\n        self, coordinate: list[float], label: str | None = None, serializable: bool = False\n    ) -&gt; Object | dict[str, Any] | None:\n        \"\"\"\n        Retrieves a detected object at or near a specified world coordinate, optionally filtering by label.\n\n        Checks for objects that are within a 2-centimeter radius of the specified coordinate.\n        If multiple objects meet the criteria, the first one found is returned.\n\n        Args:\n            coordinate (list[float]): A 2D coordinate in world units [x, y].\n            label (str, optional): An optional filter for the object's label.\n            serializable (bool): If True, returns a dictionary representation instead of an Object instance.\n\n        Returns:\n            Object | dict[str, Any] | None: The first object detected near the given coordinate, or None if not found.\n        \"\"\"\n        detected_objects = self.get_detected_objects(Location.CLOSE_TO, coordinate, label)\n\n        if detected_objects:\n            res: Object | dict[str, Any] = detected_objects[0].to_dict() if serializable else detected_objects[0]\n            return res\n        else:\n            return None\n\n    def get_detected_objects(\n        self,\n        location: Location | str = Location.NONE,\n        coordinate: list[float] | None = None,\n        label: str | None = None,\n    ) -&gt; Objects:\n        \"\"\"\n        Returns a list of objects filtered by spatial location, coordinate, and label.\n\n        Args:\n            location (Location | str): Spatial filter. Values can be \"left next to\", \"right next to\",\n                \"above\", \"below\", \"close to\", or Location enum equivalents.\n            coordinate (list[float], optional): (x, y) coordinate in meters used for spatial filtering.\n                Required if 'location' is not NONE.\n            label (str, optional): Filter by object label (substring match).\n\n        Returns:\n            Objects: A collection of filtered objects.\n\n        Raises:\n            ValueError: If coordinate is missing but required for the specified location filter.\n        \"\"\"\n        detected_objects = self\n\n        if label is not None:\n            detected_objects = Objects(obj for obj in self if label in obj.label())\n\n        location = Location.convert_str2location(location)\n\n        if location is Location.NONE:\n            return detected_objects\n\n        if coordinate is None:\n            raise ValueError(f\"Coordinate must be provided for location filter: {location}\")\n\n        if location == Location.LEFT_NEXT_TO:\n            return Objects(obj for obj in detected_objects if obj.y_com() &gt; coordinate[1])\n        elif location == Location.RIGHT_NEXT_TO:\n            return Objects(obj for obj in detected_objects if obj.y_com() &lt; coordinate[1])\n        elif location == Location.ABOVE:\n            return Objects(obj for obj in detected_objects if obj.x_com() &gt; coordinate[0])\n        elif location == Location.BELOW:\n            return Objects(obj for obj in detected_objects if obj.x_com() &lt; coordinate[0])\n        elif location == Location.CLOSE_TO:\n            return Objects(\n                obj\n                for obj in detected_objects\n                if np.sqrt((obj.x_com() - coordinate[0]) ** 2 + (obj.y_com() - coordinate[1]) ** 2) &lt;= 0.02\n            )\n        else:\n            print(\"Error in get_detected_objects: Unknown Location:\", location)\n            return Objects()\n\n    def get_detected_objects_serializable(\n        self,\n        location: Location | str = Location.NONE,\n        coordinate: list[float] | None = None,\n        label: str | None = None,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Similar to get_detected_objects but returns a list of dictionaries.\n\n        Args:\n            location (Location | str): Spatial filter.\n            coordinate (list[float], optional): Reference (x, y) coordinate.\n            label (str, optional): Filter by object label.\n\n        Returns:\n            list[dict[str, Any]]: List of dictionary representations of the filtered objects.\n        \"\"\"\n        detected_objects = self.get_detected_objects(location, coordinate, label)\n\n        objects = Objects.objects_to_dict_list(detected_objects)\n\n        return objects\n\n    def get_nearest_detected_object(self, coordinate: list[float], label: str | None = None) -&gt; tuple[Object | None, float]:\n        \"\"\"\n        Finds the object nearest to a specified coordinate.\n\n        Args:\n            coordinate (list[float]): Target (x, y) coordinate.\n            label (str, optional): If specified, only consider objects with this label.\n\n        Returns:\n            tuple[Object | None, float]: (nearest_object, distance_in_meters).\n        \"\"\"\n        nearest_object = None\n        min_distance = float(\"inf\")\n\n        for obj in self:\n            if label is None or obj.label() == label:\n                # Calculate Euclidean distance\n                distance = math.sqrt((obj.x_com() - coordinate[0]) ** 2 + (obj.y_com() - coordinate[1]) ** 2)\n                # print(distance, min_distance)\n                if distance &lt; min_distance:\n                    min_distance = distance\n                    nearest_object = obj\n\n        return nearest_object, min_distance\n\n    def get_detected_objects_as_comma_separated_string(self) -&gt; str:\n        \"\"\"\n        Returns the labels of all detected objects as a comma-separated string.\n\n        Returns:\n            str: Comma-separated labels.\n        \"\"\"\n        return f\"\"\"{', '.join(f\"'{item.label()}'\" for item in self)}\"\"\"\n\n    def get_largest_detected_object(self, serializable: bool = False) -&gt; tuple[Object, float] | tuple[dict[str, Any], float]:\n        \"\"\"\n        Identifies the largest object by area.\n\n        Args:\n            serializable (bool): If True, returns a dictionary instead of an Object.\n\n        Returns:\n            tuple: (largest_object, area_m2).\n        \"\"\"\n        largest_object = max(self, key=lambda obj: obj.size_m2())\n\n        size = largest_object.size_m2()\n\n        if serializable:\n            return largest_object.to_dict(), size\n        else:\n            return largest_object, size\n\n    def get_smallest_detected_object(self, serializable: bool = False) -&gt; tuple[Object, float] | tuple[dict[str, Any], float]:\n        \"\"\"\n        Identifies the smallest object by area.\n\n        Args:\n            serializable (bool): If True, returns a dictionary instead of an Object.\n\n        Returns:\n            tuple: (smallest_object, area_m2).\n        \"\"\"\n        smallest_object = min(self, key=lambda obj: obj.size_m2())\n\n        size = smallest_object.size_m2()\n\n        if serializable:\n            return smallest_object.to_dict(), size\n        else:\n            return smallest_object, size\n\n    def get_detected_objects_sorted(\n        self, ascending: bool = True, serializable: bool = False\n    ) -&gt; Objects | list[dict[str, Any]]:\n        \"\"\"\n        Returns objects sorted by their size.\n\n        Args:\n            ascending (bool): Sorting order. Defaults to True.\n            serializable (bool): If True, returns a list of dictionaries.\n\n        Returns:\n            Objects | list[dict[str, Any]]: Sorted collection of objects.\n        \"\"\"\n        sorted_objs = Objects(sorted(self, key=lambda obj: obj.size_m2(), reverse=not ascending))\n\n        if serializable:\n            return Objects.objects_to_dict_list(sorted_objs)\n        else:\n            return sorted_objs\n\n    # *** PUBLIC methods ***\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    # *** HELPER METHODE F\u00dcR REDIS PUBLISHER ***\n\n    @staticmethod\n    def objects_to_dict_list(objects: list[Object]) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Converts a list of Object instances to a list of dictionaries.\n\n        Args:\n            objects (list[Object]): List of Object instances.\n\n        Returns:\n            list[dict[str, Any]]: List of dictionary representations.\n        \"\"\"\n        return [obj.to_dict() for obj in objects]\n\n    @staticmethod\n    def dict_list_to_objects(dict_list: list[dict[str, Any]], workspace: Workspace) -&gt; Objects:\n        \"\"\"\n        Reconstructs an Objects collection from a list of dictionaries.\n\n        Args:\n            dict_list (list[dict[str, Any]]): List of object dictionaries.\n            workspace (Workspace): The workspace to associate with the objects.\n\n        Returns:\n            Objects: A collection of reconstructed objects.\n        \"\"\"\n        objects = Objects()\n        for obj_dict in dict_list:\n            obj = Object.from_dict(obj_dict, workspace)\n            if obj is not None:\n                objects.append(obj)\n        return objects\n\n    # *** PRIVATE methods ***\n\n    # *** PUBLIC properties ***\n\n    def verbose(self) -&gt; bool:\n        \"\"\"\n        Returns whether verbose logging is enabled.\n\n        Returns:\n            bool: True if verbose, else False.\n        \"\"\"\n        return self._verbose\n\n    # *** PRIVATE variables ***\n\n    _verbose = False\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.objects.objects.Objects.__init__","title":"<code>__init__(iterable=None, verbose=False)</code>","text":"<p>Initializes the Objects instance.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[Object]</code> <p>An iterable of Object instances. Defaults to an empty list.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, enables verbose logging.</p> <code>False</code> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def __init__(self, iterable: Iterable[Object] | None = None, verbose: bool = False) -&gt; None:\n    \"\"\"\n    Initializes the Objects instance.\n\n    Args:\n        iterable (Iterable[Object], optional): An iterable of Object instances. Defaults to an empty list.\n        verbose (bool): If True, enables verbose logging.\n    \"\"\"\n    if iterable is None:\n        iterable = []\n    super().__init__(iterable)\n\n    self._verbose = verbose\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.dict_list_to_objects","title":"<code>dict_list_to_objects(dict_list, workspace)</code>  <code>staticmethod</code>","text":"<p>Reconstructs an Objects collection from a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>dict_list</code> <code>list[dict[str, Any]]</code> <p>List of object dictionaries.</p> required <code>workspace</code> <code>Workspace</code> <p>The workspace to associate with the objects.</p> required <p>Returns:</p> Name Type Description <code>Objects</code> <code>Objects</code> <p>A collection of reconstructed objects.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>@staticmethod\ndef dict_list_to_objects(dict_list: list[dict[str, Any]], workspace: Workspace) -&gt; Objects:\n    \"\"\"\n    Reconstructs an Objects collection from a list of dictionaries.\n\n    Args:\n        dict_list (list[dict[str, Any]]): List of object dictionaries.\n        workspace (Workspace): The workspace to associate with the objects.\n\n    Returns:\n        Objects: A collection of reconstructed objects.\n    \"\"\"\n    objects = Objects()\n    for obj_dict in dict_list:\n        obj = Object.from_dict(obj_dict, workspace)\n        if obj is not None:\n            objects.append(obj)\n    return objects\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_detected_object","title":"<code>get_detected_object(coordinate, label=None, serializable=False)</code>","text":"<p>Retrieves a detected object at or near a specified world coordinate, optionally filtering by label.</p> <p>Checks for objects that are within a 2-centimeter radius of the specified coordinate. If multiple objects meet the criteria, the first one found is returned.</p> <p>Parameters:</p> Name Type Description Default <code>coordinate</code> <code>list[float]</code> <p>A 2D coordinate in world units [x, y].</p> required <code>label</code> <code>str</code> <p>An optional filter for the object's label.</p> <code>None</code> <code>serializable</code> <code>bool</code> <p>If True, returns a dictionary representation instead of an Object instance.</p> <code>False</code> <p>Returns:</p> Type Description <code>Object | dict[str, Any] | None</code> <p>Object | dict[str, Any] | None: The first object detected near the given coordinate, or None if not found.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>@log_start_end_cls()\ndef get_detected_object(\n    self, coordinate: list[float], label: str | None = None, serializable: bool = False\n) -&gt; Object | dict[str, Any] | None:\n    \"\"\"\n    Retrieves a detected object at or near a specified world coordinate, optionally filtering by label.\n\n    Checks for objects that are within a 2-centimeter radius of the specified coordinate.\n    If multiple objects meet the criteria, the first one found is returned.\n\n    Args:\n        coordinate (list[float]): A 2D coordinate in world units [x, y].\n        label (str, optional): An optional filter for the object's label.\n        serializable (bool): If True, returns a dictionary representation instead of an Object instance.\n\n    Returns:\n        Object | dict[str, Any] | None: The first object detected near the given coordinate, or None if not found.\n    \"\"\"\n    detected_objects = self.get_detected_objects(Location.CLOSE_TO, coordinate, label)\n\n    if detected_objects:\n        res: Object | dict[str, Any] = detected_objects[0].to_dict() if serializable else detected_objects[0]\n        return res\n    else:\n        return None\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_detected_objects","title":"<code>get_detected_objects(location=Location.NONE, coordinate=None, label=None)</code>","text":"<p>Returns a list of objects filtered by spatial location, coordinate, and label.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Location | str</code> <p>Spatial filter. Values can be \"left next to\", \"right next to\", \"above\", \"below\", \"close to\", or Location enum equivalents.</p> <code>NONE</code> <code>coordinate</code> <code>list[float]</code> <p>(x, y) coordinate in meters used for spatial filtering. Required if 'location' is not NONE.</p> <code>None</code> <code>label</code> <code>str</code> <p>Filter by object label (substring match).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Objects</code> <code>Objects</code> <p>A collection of filtered objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If coordinate is missing but required for the specified location filter.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def get_detected_objects(\n    self,\n    location: Location | str = Location.NONE,\n    coordinate: list[float] | None = None,\n    label: str | None = None,\n) -&gt; Objects:\n    \"\"\"\n    Returns a list of objects filtered by spatial location, coordinate, and label.\n\n    Args:\n        location (Location | str): Spatial filter. Values can be \"left next to\", \"right next to\",\n            \"above\", \"below\", \"close to\", or Location enum equivalents.\n        coordinate (list[float], optional): (x, y) coordinate in meters used for spatial filtering.\n            Required if 'location' is not NONE.\n        label (str, optional): Filter by object label (substring match).\n\n    Returns:\n        Objects: A collection of filtered objects.\n\n    Raises:\n        ValueError: If coordinate is missing but required for the specified location filter.\n    \"\"\"\n    detected_objects = self\n\n    if label is not None:\n        detected_objects = Objects(obj for obj in self if label in obj.label())\n\n    location = Location.convert_str2location(location)\n\n    if location is Location.NONE:\n        return detected_objects\n\n    if coordinate is None:\n        raise ValueError(f\"Coordinate must be provided for location filter: {location}\")\n\n    if location == Location.LEFT_NEXT_TO:\n        return Objects(obj for obj in detected_objects if obj.y_com() &gt; coordinate[1])\n    elif location == Location.RIGHT_NEXT_TO:\n        return Objects(obj for obj in detected_objects if obj.y_com() &lt; coordinate[1])\n    elif location == Location.ABOVE:\n        return Objects(obj for obj in detected_objects if obj.x_com() &gt; coordinate[0])\n    elif location == Location.BELOW:\n        return Objects(obj for obj in detected_objects if obj.x_com() &lt; coordinate[0])\n    elif location == Location.CLOSE_TO:\n        return Objects(\n            obj\n            for obj in detected_objects\n            if np.sqrt((obj.x_com() - coordinate[0]) ** 2 + (obj.y_com() - coordinate[1]) ** 2) &lt;= 0.02\n        )\n    else:\n        print(\"Error in get_detected_objects: Unknown Location:\", location)\n        return Objects()\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_detected_objects_as_comma_separated_string","title":"<code>get_detected_objects_as_comma_separated_string()</code>","text":"<p>Returns the labels of all detected objects as a comma-separated string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Comma-separated labels.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def get_detected_objects_as_comma_separated_string(self) -&gt; str:\n    \"\"\"\n    Returns the labels of all detected objects as a comma-separated string.\n\n    Returns:\n        str: Comma-separated labels.\n    \"\"\"\n    return f\"\"\"{', '.join(f\"'{item.label()}'\" for item in self)}\"\"\"\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_detected_objects_serializable","title":"<code>get_detected_objects_serializable(location=Location.NONE, coordinate=None, label=None)</code>","text":"<p>Similar to get_detected_objects but returns a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Location | str</code> <p>Spatial filter.</p> <code>NONE</code> <code>coordinate</code> <code>list[float]</code> <p>Reference (x, y) coordinate.</p> <code>None</code> <code>label</code> <code>str</code> <p>Filter by object label.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: List of dictionary representations of the filtered objects.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def get_detected_objects_serializable(\n    self,\n    location: Location | str = Location.NONE,\n    coordinate: list[float] | None = None,\n    label: str | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Similar to get_detected_objects but returns a list of dictionaries.\n\n    Args:\n        location (Location | str): Spatial filter.\n        coordinate (list[float], optional): Reference (x, y) coordinate.\n        label (str, optional): Filter by object label.\n\n    Returns:\n        list[dict[str, Any]]: List of dictionary representations of the filtered objects.\n    \"\"\"\n    detected_objects = self.get_detected_objects(location, coordinate, label)\n\n    objects = Objects.objects_to_dict_list(detected_objects)\n\n    return objects\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_detected_objects_sorted","title":"<code>get_detected_objects_sorted(ascending=True, serializable=False)</code>","text":"<p>Returns objects sorted by their size.</p> <p>Parameters:</p> Name Type Description Default <code>ascending</code> <code>bool</code> <p>Sorting order. Defaults to True.</p> <code>True</code> <code>serializable</code> <code>bool</code> <p>If True, returns a list of dictionaries.</p> <code>False</code> <p>Returns:</p> Type Description <code>Objects | list[dict[str, Any]]</code> <p>Objects | list[dict[str, Any]]: Sorted collection of objects.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def get_detected_objects_sorted(\n    self, ascending: bool = True, serializable: bool = False\n) -&gt; Objects | list[dict[str, Any]]:\n    \"\"\"\n    Returns objects sorted by their size.\n\n    Args:\n        ascending (bool): Sorting order. Defaults to True.\n        serializable (bool): If True, returns a list of dictionaries.\n\n    Returns:\n        Objects | list[dict[str, Any]]: Sorted collection of objects.\n    \"\"\"\n    sorted_objs = Objects(sorted(self, key=lambda obj: obj.size_m2(), reverse=not ascending))\n\n    if serializable:\n        return Objects.objects_to_dict_list(sorted_objs)\n    else:\n        return sorted_objs\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_largest_detected_object","title":"<code>get_largest_detected_object(serializable=False)</code>","text":"<p>Identifies the largest object by area.</p> <p>Parameters:</p> Name Type Description Default <code>serializable</code> <code>bool</code> <p>If True, returns a dictionary instead of an Object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Object, float] | tuple[dict[str, Any], float]</code> <p>(largest_object, area_m2).</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def get_largest_detected_object(self, serializable: bool = False) -&gt; tuple[Object, float] | tuple[dict[str, Any], float]:\n    \"\"\"\n    Identifies the largest object by area.\n\n    Args:\n        serializable (bool): If True, returns a dictionary instead of an Object.\n\n    Returns:\n        tuple: (largest_object, area_m2).\n    \"\"\"\n    largest_object = max(self, key=lambda obj: obj.size_m2())\n\n    size = largest_object.size_m2()\n\n    if serializable:\n        return largest_object.to_dict(), size\n    else:\n        return largest_object, size\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_nearest_detected_object","title":"<code>get_nearest_detected_object(coordinate, label=None)</code>","text":"<p>Finds the object nearest to a specified coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>coordinate</code> <code>list[float]</code> <p>Target (x, y) coordinate.</p> required <code>label</code> <code>str</code> <p>If specified, only consider objects with this label.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Object | None, float]</code> <p>tuple[Object | None, float]: (nearest_object, distance_in_meters).</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def get_nearest_detected_object(self, coordinate: list[float], label: str | None = None) -&gt; tuple[Object | None, float]:\n    \"\"\"\n    Finds the object nearest to a specified coordinate.\n\n    Args:\n        coordinate (list[float]): Target (x, y) coordinate.\n        label (str, optional): If specified, only consider objects with this label.\n\n    Returns:\n        tuple[Object | None, float]: (nearest_object, distance_in_meters).\n    \"\"\"\n    nearest_object = None\n    min_distance = float(\"inf\")\n\n    for obj in self:\n        if label is None or obj.label() == label:\n            # Calculate Euclidean distance\n            distance = math.sqrt((obj.x_com() - coordinate[0]) ** 2 + (obj.y_com() - coordinate[1]) ** 2)\n            # print(distance, min_distance)\n            if distance &lt; min_distance:\n                min_distance = distance\n                nearest_object = obj\n\n    return nearest_object, min_distance\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.get_smallest_detected_object","title":"<code>get_smallest_detected_object(serializable=False)</code>","text":"<p>Identifies the smallest object by area.</p> <p>Parameters:</p> Name Type Description Default <code>serializable</code> <code>bool</code> <p>If True, returns a dictionary instead of an Object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Object, float] | tuple[dict[str, Any], float]</code> <p>(smallest_object, area_m2).</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def get_smallest_detected_object(self, serializable: bool = False) -&gt; tuple[Object, float] | tuple[dict[str, Any], float]:\n    \"\"\"\n    Identifies the smallest object by area.\n\n    Args:\n        serializable (bool): If True, returns a dictionary instead of an Object.\n\n    Returns:\n        tuple: (smallest_object, area_m2).\n    \"\"\"\n    smallest_object = min(self, key=lambda obj: obj.size_m2())\n\n    size = smallest_object.size_m2()\n\n    if serializable:\n        return smallest_object.to_dict(), size\n    else:\n        return smallest_object, size\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.objects_to_dict_list","title":"<code>objects_to_dict_list(objects)</code>  <code>staticmethod</code>","text":"<p>Converts a list of Object instances to a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>list[Object]</code> <p>List of Object instances.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: List of dictionary representations.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>@staticmethod\ndef objects_to_dict_list(objects: list[Object]) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Converts a list of Object instances to a list of dictionaries.\n\n    Args:\n        objects (list[Object]): List of Object instances.\n\n    Returns:\n        list[dict[str, Any]]: List of dictionary representations.\n    \"\"\"\n    return [obj.to_dict() for obj in objects]\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects.Objects.verbose","title":"<code>verbose()</code>","text":"<p>Returns whether verbose logging is enabled.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if verbose, else False.</p> Source code in <code>robot_workspace/objects/objects.py</code> <pre><code>def verbose(self) -&gt; bool:\n    \"\"\"\n    Returns whether verbose logging is enabled.\n\n    Returns:\n        bool: True if verbose, else False.\n    \"\"\"\n    return self._verbose\n</code></pre>"},{"location":"api/#robot_workspace.objects.objects-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.objects.pose_object","title":"<code>robot_workspace.objects.pose_object</code>","text":""},{"location":"api/#robot_workspace.objects.pose_object-classes","title":"Classes","text":""},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP","title":"<code>PoseObjectPNP</code>","text":"<p>Pose object which stores x, y, z, roll, pitch &amp; yaw parameters.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>X coordinate in meters.</p> <code>y</code> <code>float</code> <p>Y coordinate in meters.</p> <code>z</code> <code>float</code> <p>Z coordinate in meters.</p> <code>roll</code> <code>float</code> <p>Roll orientation in radians.</p> <code>pitch</code> <code>float</code> <p>Pitch orientation in radians.</p> <code>yaw</code> <code>float</code> <p>Yaw orientation in radians.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>class PoseObjectPNP:\n    \"\"\"\n    Pose object which stores x, y, z, roll, pitch &amp; yaw parameters.\n\n    Attributes:\n        x (float): X coordinate in meters.\n        y (float): Y coordinate in meters.\n        z (float): Z coordinate in meters.\n        roll (float): Roll orientation in radians.\n        pitch (float): Pitch orientation in radians.\n        yaw (float): Yaw orientation in radians.\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(\n        self,\n        x: float = 0.0,\n        y: float = 0.0,\n        z: float = 0.0,\n        roll: float = 0.0,\n        pitch: float = 0.0,\n        yaw: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a PoseObjectPNP instance.\n\n        Args:\n            x (float): X coordinate in meters.\n            y (float): Y coordinate in meters.\n            z (float): Z coordinate in meters.\n            roll (float): Roll orientation in radians.\n            pitch (float): Pitch orientation in radians.\n            yaw (float): Yaw orientation in radians.\n        \"\"\"\n        # X (meter)\n        self.x = float(x)\n        # Y (meter)\n        self.y = float(y)\n        # Z (meter)\n        self.z = float(z)\n        # Roll (radian)\n        self.roll = float(roll)\n        # Pitch (radian)\n        self.pitch = float(pitch)\n        # Yaw (radian)\n        self.yaw = float(yaw)\n\n    def __str__(self) -&gt; str:\n        position = f\"x = {self.x:.4f}, y = {self.y:.4f}, z = {self.z:.4f}\"\n        orientation = f\"roll = {self.roll:.3f}, pitch = {self.pitch:.3f}, yaw = {self.yaw:.3f}\"\n        return position + \"\\n\" + orientation\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n\n    def __add__(self, other: PoseObjectPNP) -&gt; PoseObjectPNP:\n        x = self.x + other.x\n        y = self.y + other.y\n        z = self.z + other.z\n        roll = self.roll + other.roll\n        pitch = self.pitch + other.pitch\n        yaw = self.yaw + other.yaw\n        return PoseObjectPNP(x, y, z, roll, pitch, yaw)\n\n    def __sub__(self, other: PoseObjectPNP) -&gt; PoseObjectPNP:\n        x = self.x - other.x\n        y = self.y - other.y\n        z = self.z - other.z\n        roll = self.roll - other.roll\n        pitch = self.pitch - other.pitch\n        yaw = self.yaw - other.yaw\n        return PoseObjectPNP(x, y, z, roll, pitch, yaw)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, PoseObjectPNP):\n            return NotImplemented\n        return (\n            self.x == other.x\n            and self.y == other.y\n            and self.z == other.z\n            and self.roll == other.roll\n            and self.pitch == other.pitch\n            and self.yaw == other.yaw\n        )\n\n    @log_start_end_cls()\n    def approx_eq(self, other: PoseObjectPNP, eps_position: float = 0.1, eps_orientation: float = 0.1) -&gt; bool:\n        \"\"\"\n        Determines if two poses are approximately the same, accounting for angle periodicity.\n\n        Args:\n            other (PoseObjectPNP): Other pose object to compare with.\n            eps_position (float): Tolerance for position differences (in meters).\n            eps_orientation (float): Tolerance for orientation differences (in radians).\n\n        Returns:\n            bool: True if poses are approximately the same, False otherwise.\n        \"\"\"\n        # Calculate position differences\n        delta_position = math.sqrt((self.x - other.x) ** 2 + (self.y - other.y) ** 2 + (self.z - other.z) ** 2)\n\n        # Calculate orientation differences\n        delta_roll = abs(PoseObjectPNP._angular_difference(self.roll, other.roll))\n        delta_pitch = abs(PoseObjectPNP._angular_difference(self.pitch, other.pitch))\n        delta_yaw = abs(PoseObjectPNP._angular_difference(self.yaw, other.yaw))\n\n        # Check if both position and orientation differences are within tolerances\n        return (\n            delta_position &lt; eps_position\n            and delta_roll &lt; eps_orientation\n            and delta_pitch &lt; eps_orientation\n            and delta_yaw &lt; eps_orientation\n        )\n\n    def approx_eq_xyz(self, other: PoseObjectPNP, eps: float = 0.1) -&gt; bool:\n        \"\"\"\n        Compares the (x, y, z) coordinates of this pose object with another pose object.\n\n        Args:\n            other (PoseObjectPNP): The pose object to compare with.\n            eps (float): The allowable deviation for equality in each coordinate (default: 0.1).\n\n        Returns:\n            bool: True if the difference in x, y, and z coordinates between the two poses\n                is less than the specified tolerance (eps), otherwise False.\n        \"\"\"\n        return bool(abs(self.x - other.x) &lt; eps and abs(self.y - other.y) &lt; eps and abs(self.z - other.z) &lt; eps)\n\n    def copy_with_offsets(\n        self,\n        x_offset: float = 0.0,\n        y_offset: float = 0.0,\n        z_offset: float = 0.0,\n        roll_offset: float = 0.0,\n        pitch_offset: float = 0.0,\n        yaw_offset: float = 0.0,\n    ) -&gt; PoseObjectPNP:\n        \"\"\"\n        Creates a new pose object by applying offsets to its position and orientation.\n\n        Args:\n            x_offset (float): Offset to apply to the x-coordinate (default: 0.0).\n            y_offset (float): Offset to apply to the y-coordinate (default: 0.0).\n            z_offset (float): Offset to apply to the z-coordinate (default: 0.0).\n            roll_offset (float): Offset to apply to the roll orientation (default: 0.0).\n            pitch_offset (float): Offset to apply to the pitch orientation (default: 0.0).\n            yaw_offset (float): Offset to apply to the yaw orientation (default: 0.0).\n\n        Returns:\n            PoseObjectPNP: A new pose object with the offsets applied.\n        \"\"\"\n        return PoseObjectPNP(\n            self.x + x_offset,\n            self.y + y_offset,\n            self.z + z_offset,\n            self.roll + roll_offset,\n            self.pitch + pitch_offset,\n            self.yaw + yaw_offset,\n        )\n\n    def to_list(self) -&gt; list[float]:\n        \"\"\"\n        Return a list [x, y, z, roll, pitch, yaw] corresponding to the pose's parameters.\n\n        Returns:\n            list[float]: A list of the pose's parameters.\n        \"\"\"\n        list_pos = [self.x, self.y, self.z, self.roll, self.pitch, self.yaw]\n        return list(map(float, list_pos))\n\n    def to_transformation_matrix(self) -&gt; np.ndarray:\n        \"\"\"\n        Converts the pose into a 4x4 transformation matrix.\n\n        The transformation matrix represents the pose of an object in a\n        3D coordinate system. It combines translation and rotation into\n        a single homogeneous transformation.\n\n        Returns:\n            np.ndarray: A 4x4 transformation matrix of type float, where:\n                - The upper-left 3x3 submatrix represents the rotation.\n                - The last column represents the translation.\n                - The bottom row is [0, 0, 0, 1] for homogeneity.\n        \"\"\"\n        # Compute the rotation matrix from roll, pitch, yaw\n        rx = np.array([[1, 0, 0], [0, np.cos(self.roll), -np.sin(self.roll)], [0, np.sin(self.roll), np.cos(self.roll)]])\n\n        ry = np.array([[np.cos(self.pitch), 0, np.sin(self.pitch)], [0, 1, 0], [-np.sin(self.pitch), 0, np.cos(self.pitch)]])\n\n        rz = np.array([[np.cos(self.yaw), -np.sin(self.yaw), 0], [np.sin(self.yaw), np.cos(self.yaw), 0], [0, 0, 1]])\n\n        # Combined rotation matrix (Rz * Ry * Rx)\n        rotation_matrix = rz @ ry @ rx\n\n        # Translation vector\n        translation_vector = np.array([self.x, self.y, self.z])\n\n        # Construct the 4x4 transformation matrix\n        transformation_matrix = np.eye(4)  # Start with an identity matrix\n        transformation_matrix[:3, :3] = rotation_matrix\n        transformation_matrix[:3, 3] = translation_vector\n\n        return transformation_matrix\n\n    @property\n    def quaternion(self) -&gt; list[float]:\n        \"\"\"\n        Return the quaternion in a list [qx, qy, qz, qw].\n\n        Returns:\n            list[float]: Quaternion [qx, qy, qz, qw].\n        \"\"\"\n        return self.euler_to_quaternion(self.roll, self.pitch, self.yaw)\n\n    @property\n    def quaternion_pose(self) -&gt; list[float]:\n        \"\"\"\n        Return the position and the quaternion in a list [x, y, z, qx, qy, qz, qw].\n\n        Returns:\n            list[float]: Position [x, y, z] + quaternion [qx, qy, qz, qw].\n        \"\"\"\n        return [self.x, self.y, self.z, *list(self.euler_to_quaternion(self.roll, self.pitch, self.yaw))]\n\n    @staticmethod\n    def euler_to_quaternion(roll: float, pitch: float, yaw: float) -&gt; list[float]:\n        \"\"\"\n        Convert euler angles to quaternion.\n\n        Args:\n            roll (float): Roll in radians.\n            pitch (float): Pitch in radians.\n            yaw (float): Yaw in radians.\n\n        Returns:\n            list[float]: Quaternion in a list [qx, qy, qz, qw].\n        \"\"\"\n        qx = np.sin(roll / 2) * np.cos(pitch / 2) * np.cos(yaw / 2) - np.cos(roll / 2) * np.sin(pitch / 2) * np.sin(yaw / 2)\n        qy = np.cos(roll / 2) * np.sin(pitch / 2) * np.cos(yaw / 2) + np.sin(roll / 2) * np.cos(pitch / 2) * np.sin(yaw / 2)\n        qz = np.cos(roll / 2) * np.cos(pitch / 2) * np.sin(yaw / 2) - np.sin(roll / 2) * np.sin(pitch / 2) * np.cos(yaw / 2)\n        qw = np.cos(roll / 2) * np.cos(pitch / 2) * np.cos(yaw / 2) + np.sin(roll / 2) * np.sin(pitch / 2) * np.sin(yaw / 2)\n\n        return [float(qx), float(qy), float(qz), float(qw)]\n\n    @staticmethod\n    def quaternion_to_euler_angle(qx: float, qy: float, qz: float, qw: float) -&gt; tuple[float, float, float]:\n        \"\"\"\n        Convert quaternion to euler angles.\n\n        Args:\n            qx (float): Quaternion x.\n            qy (float): Quaternion y.\n            qz (float): Quaternion z.\n            qw (float): Quaternion w.\n\n        Returns:\n            tuple[float, float, float]: Euler angles (roll, pitch, yaw) in radians.\n        \"\"\"\n        ysqr = qy * qy\n\n        t0 = +2.0 * (qw * qx + qy * qz)\n        t1 = +1.0 - 2.0 * (qx * qx + ysqr)\n        roll = np.arctan2(t0, t1)\n\n        t2 = +2.0 * (qw * qy - qz * qx)\n\n        t2 = np.clip(t2, a_min=-1.0, a_max=1.0)\n        pitch = np.arcsin(t2)\n\n        t3 = +2.0 * (qw * qz + qx * qy)\n        t4 = +1.0 - 2.0 * (ysqr + qz * qz)\n        yaw = np.arctan2(t3, t4)\n\n        return float(roll), float(pitch), float(yaw)\n\n    # *** PUBLIC GET methods ***\n\n    # *** PUBLIC methods ***\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    @staticmethod\n    def convert_niryo_pose_object2pose_object(pose_object: Any) -&gt; PoseObjectPNP:\n        \"\"\"\n        Converts a PoseObject from Niryo class to a PoseObjectPNP object.\n\n        Args:\n            pose_object (Any): PoseObject from Niryo.\n\n        Returns:\n            PoseObjectPNP: The converted pose object.\n        \"\"\"\n        return PoseObjectPNP(\n            float(pose_object.x),\n            float(pose_object.y),\n            float(pose_object.z),\n            float(pose_object.roll),\n            float(pose_object.pitch),\n            float(pose_object.yaw),\n        )\n\n    @staticmethod\n    def convert_pose_object2niryo_pose_object(pose_object: PoseObjectPNP) -&gt; Any:\n        \"\"\"\n        Convert a PoseObjectPNP to a PoseObject from Niryo Robot.\n\n        Args:\n            pose_object (PoseObjectPNP): The pose object to convert.\n\n        Returns:\n            Any: Pose object as defined by Niryo.\n        \"\"\"\n        return PoseObject(pose_object.x, pose_object.y, pose_object.z, pose_object.roll, pose_object.pitch, pose_object.yaw)\n\n    # *** PRIVATE methods ***\n\n    # *** PRIVATE STATIC/CLASS methods ***\n\n    @staticmethod\n    def _normalize_angle(angle: float) -&gt; float:\n        \"\"\"\n        Normalize an angle to the range [-\u03c0, \u03c0].\n\n        Args:\n            angle (float): Angle in radians.\n\n        Returns:\n            float: Normalized angle.\n        \"\"\"\n        return (angle + math.pi) % (2 * math.pi) - math.pi\n\n    @staticmethod\n    def _angular_difference(angle1: float, angle2: float) -&gt; float:\n        \"\"\"\n        Calculate the smallest difference between two angles.\n\n        Args:\n            angle1 (float): Angle 1 in radians.\n            angle2 (float): Angle 2 in radians.\n\n        Returns:\n            float: Angular difference.\n        \"\"\"\n        return PoseObjectPNP._normalize_angle(angle1 - angle2)\n\n    # *** PUBLIC properties ***\n\n    def xy_coordinate(self) -&gt; list[float]:\n        \"\"\"\n        Returns the (x, y) coordinates of the pose.\n\n        Returns:\n            list[float]: [x, y] coordinates.\n        \"\"\"\n        return [self.x, self.y]\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP-attributes","title":"Attributes","text":""},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.quaternion","title":"<code>quaternion</code>  <code>property</code>","text":"<p>Return the quaternion in a list [qx, qy, qz, qw].</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Quaternion [qx, qy, qz, qw].</p>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.quaternion_pose","title":"<code>quaternion_pose</code>  <code>property</code>","text":"<p>Return the position and the quaternion in a list [x, y, z, qx, qy, qz, qw].</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Position [x, y, z] + quaternion [qx, qy, qz, qw].</p>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.__init__","title":"<code>__init__(x=0.0, y=0.0, z=0.0, roll=0.0, pitch=0.0, yaw=0.0)</code>","text":"<p>Initializes a PoseObjectPNP instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>X coordinate in meters.</p> <code>0.0</code> <code>y</code> <code>float</code> <p>Y coordinate in meters.</p> <code>0.0</code> <code>z</code> <code>float</code> <p>Z coordinate in meters.</p> <code>0.0</code> <code>roll</code> <code>float</code> <p>Roll orientation in radians.</p> <code>0.0</code> <code>pitch</code> <code>float</code> <p>Pitch orientation in radians.</p> <code>0.0</code> <code>yaw</code> <code>float</code> <p>Yaw orientation in radians.</p> <code>0.0</code> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>def __init__(\n    self,\n    x: float = 0.0,\n    y: float = 0.0,\n    z: float = 0.0,\n    roll: float = 0.0,\n    pitch: float = 0.0,\n    yaw: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Initializes a PoseObjectPNP instance.\n\n    Args:\n        x (float): X coordinate in meters.\n        y (float): Y coordinate in meters.\n        z (float): Z coordinate in meters.\n        roll (float): Roll orientation in radians.\n        pitch (float): Pitch orientation in radians.\n        yaw (float): Yaw orientation in radians.\n    \"\"\"\n    # X (meter)\n    self.x = float(x)\n    # Y (meter)\n    self.y = float(y)\n    # Z (meter)\n    self.z = float(z)\n    # Roll (radian)\n    self.roll = float(roll)\n    # Pitch (radian)\n    self.pitch = float(pitch)\n    # Yaw (radian)\n    self.yaw = float(yaw)\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.approx_eq","title":"<code>approx_eq(other, eps_position=0.1, eps_orientation=0.1)</code>","text":"<p>Determines if two poses are approximately the same, accounting for angle periodicity.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>PoseObjectPNP</code> <p>Other pose object to compare with.</p> required <code>eps_position</code> <code>float</code> <p>Tolerance for position differences (in meters).</p> <code>0.1</code> <code>eps_orientation</code> <code>float</code> <p>Tolerance for orientation differences (in radians).</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if poses are approximately the same, False otherwise.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>@log_start_end_cls()\ndef approx_eq(self, other: PoseObjectPNP, eps_position: float = 0.1, eps_orientation: float = 0.1) -&gt; bool:\n    \"\"\"\n    Determines if two poses are approximately the same, accounting for angle periodicity.\n\n    Args:\n        other (PoseObjectPNP): Other pose object to compare with.\n        eps_position (float): Tolerance for position differences (in meters).\n        eps_orientation (float): Tolerance for orientation differences (in radians).\n\n    Returns:\n        bool: True if poses are approximately the same, False otherwise.\n    \"\"\"\n    # Calculate position differences\n    delta_position = math.sqrt((self.x - other.x) ** 2 + (self.y - other.y) ** 2 + (self.z - other.z) ** 2)\n\n    # Calculate orientation differences\n    delta_roll = abs(PoseObjectPNP._angular_difference(self.roll, other.roll))\n    delta_pitch = abs(PoseObjectPNP._angular_difference(self.pitch, other.pitch))\n    delta_yaw = abs(PoseObjectPNP._angular_difference(self.yaw, other.yaw))\n\n    # Check if both position and orientation differences are within tolerances\n    return (\n        delta_position &lt; eps_position\n        and delta_roll &lt; eps_orientation\n        and delta_pitch &lt; eps_orientation\n        and delta_yaw &lt; eps_orientation\n    )\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.approx_eq_xyz","title":"<code>approx_eq_xyz(other, eps=0.1)</code>","text":"<p>Compares the (x, y, z) coordinates of this pose object with another pose object.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>PoseObjectPNP</code> <p>The pose object to compare with.</p> required <code>eps</code> <code>float</code> <p>The allowable deviation for equality in each coordinate (default: 0.1).</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the difference in x, y, and z coordinates between the two poses is less than the specified tolerance (eps), otherwise False.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>def approx_eq_xyz(self, other: PoseObjectPNP, eps: float = 0.1) -&gt; bool:\n    \"\"\"\n    Compares the (x, y, z) coordinates of this pose object with another pose object.\n\n    Args:\n        other (PoseObjectPNP): The pose object to compare with.\n        eps (float): The allowable deviation for equality in each coordinate (default: 0.1).\n\n    Returns:\n        bool: True if the difference in x, y, and z coordinates between the two poses\n            is less than the specified tolerance (eps), otherwise False.\n    \"\"\"\n    return bool(abs(self.x - other.x) &lt; eps and abs(self.y - other.y) &lt; eps and abs(self.z - other.z) &lt; eps)\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.convert_niryo_pose_object2pose_object","title":"<code>convert_niryo_pose_object2pose_object(pose_object)</code>  <code>staticmethod</code>","text":"<p>Converts a PoseObject from Niryo class to a PoseObjectPNP object.</p> <p>Parameters:</p> Name Type Description Default <code>pose_object</code> <code>Any</code> <p>PoseObject from Niryo.</p> required <p>Returns:</p> Name Type Description <code>PoseObjectPNP</code> <code>PoseObjectPNP</code> <p>The converted pose object.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>@staticmethod\ndef convert_niryo_pose_object2pose_object(pose_object: Any) -&gt; PoseObjectPNP:\n    \"\"\"\n    Converts a PoseObject from Niryo class to a PoseObjectPNP object.\n\n    Args:\n        pose_object (Any): PoseObject from Niryo.\n\n    Returns:\n        PoseObjectPNP: The converted pose object.\n    \"\"\"\n    return PoseObjectPNP(\n        float(pose_object.x),\n        float(pose_object.y),\n        float(pose_object.z),\n        float(pose_object.roll),\n        float(pose_object.pitch),\n        float(pose_object.yaw),\n    )\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.convert_pose_object2niryo_pose_object","title":"<code>convert_pose_object2niryo_pose_object(pose_object)</code>  <code>staticmethod</code>","text":"<p>Convert a PoseObjectPNP to a PoseObject from Niryo Robot.</p> <p>Parameters:</p> Name Type Description Default <code>pose_object</code> <code>PoseObjectPNP</code> <p>The pose object to convert.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Pose object as defined by Niryo.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>@staticmethod\ndef convert_pose_object2niryo_pose_object(pose_object: PoseObjectPNP) -&gt; Any:\n    \"\"\"\n    Convert a PoseObjectPNP to a PoseObject from Niryo Robot.\n\n    Args:\n        pose_object (PoseObjectPNP): The pose object to convert.\n\n    Returns:\n        Any: Pose object as defined by Niryo.\n    \"\"\"\n    return PoseObject(pose_object.x, pose_object.y, pose_object.z, pose_object.roll, pose_object.pitch, pose_object.yaw)\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.copy_with_offsets","title":"<code>copy_with_offsets(x_offset=0.0, y_offset=0.0, z_offset=0.0, roll_offset=0.0, pitch_offset=0.0, yaw_offset=0.0)</code>","text":"<p>Creates a new pose object by applying offsets to its position and orientation.</p> <p>Parameters:</p> Name Type Description Default <code>x_offset</code> <code>float</code> <p>Offset to apply to the x-coordinate (default: 0.0).</p> <code>0.0</code> <code>y_offset</code> <code>float</code> <p>Offset to apply to the y-coordinate (default: 0.0).</p> <code>0.0</code> <code>z_offset</code> <code>float</code> <p>Offset to apply to the z-coordinate (default: 0.0).</p> <code>0.0</code> <code>roll_offset</code> <code>float</code> <p>Offset to apply to the roll orientation (default: 0.0).</p> <code>0.0</code> <code>pitch_offset</code> <code>float</code> <p>Offset to apply to the pitch orientation (default: 0.0).</p> <code>0.0</code> <code>yaw_offset</code> <code>float</code> <p>Offset to apply to the yaw orientation (default: 0.0).</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>PoseObjectPNP</code> <code>PoseObjectPNP</code> <p>A new pose object with the offsets applied.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>def copy_with_offsets(\n    self,\n    x_offset: float = 0.0,\n    y_offset: float = 0.0,\n    z_offset: float = 0.0,\n    roll_offset: float = 0.0,\n    pitch_offset: float = 0.0,\n    yaw_offset: float = 0.0,\n) -&gt; PoseObjectPNP:\n    \"\"\"\n    Creates a new pose object by applying offsets to its position and orientation.\n\n    Args:\n        x_offset (float): Offset to apply to the x-coordinate (default: 0.0).\n        y_offset (float): Offset to apply to the y-coordinate (default: 0.0).\n        z_offset (float): Offset to apply to the z-coordinate (default: 0.0).\n        roll_offset (float): Offset to apply to the roll orientation (default: 0.0).\n        pitch_offset (float): Offset to apply to the pitch orientation (default: 0.0).\n        yaw_offset (float): Offset to apply to the yaw orientation (default: 0.0).\n\n    Returns:\n        PoseObjectPNP: A new pose object with the offsets applied.\n    \"\"\"\n    return PoseObjectPNP(\n        self.x + x_offset,\n        self.y + y_offset,\n        self.z + z_offset,\n        self.roll + roll_offset,\n        self.pitch + pitch_offset,\n        self.yaw + yaw_offset,\n    )\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.euler_to_quaternion","title":"<code>euler_to_quaternion(roll, pitch, yaw)</code>  <code>staticmethod</code>","text":"<p>Convert euler angles to quaternion.</p> <p>Parameters:</p> Name Type Description Default <code>roll</code> <code>float</code> <p>Roll in radians.</p> required <code>pitch</code> <code>float</code> <p>Pitch in radians.</p> required <code>yaw</code> <code>float</code> <p>Yaw in radians.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Quaternion in a list [qx, qy, qz, qw].</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>@staticmethod\ndef euler_to_quaternion(roll: float, pitch: float, yaw: float) -&gt; list[float]:\n    \"\"\"\n    Convert euler angles to quaternion.\n\n    Args:\n        roll (float): Roll in radians.\n        pitch (float): Pitch in radians.\n        yaw (float): Yaw in radians.\n\n    Returns:\n        list[float]: Quaternion in a list [qx, qy, qz, qw].\n    \"\"\"\n    qx = np.sin(roll / 2) * np.cos(pitch / 2) * np.cos(yaw / 2) - np.cos(roll / 2) * np.sin(pitch / 2) * np.sin(yaw / 2)\n    qy = np.cos(roll / 2) * np.sin(pitch / 2) * np.cos(yaw / 2) + np.sin(roll / 2) * np.cos(pitch / 2) * np.sin(yaw / 2)\n    qz = np.cos(roll / 2) * np.cos(pitch / 2) * np.sin(yaw / 2) - np.sin(roll / 2) * np.sin(pitch / 2) * np.cos(yaw / 2)\n    qw = np.cos(roll / 2) * np.cos(pitch / 2) * np.cos(yaw / 2) + np.sin(roll / 2) * np.sin(pitch / 2) * np.sin(yaw / 2)\n\n    return [float(qx), float(qy), float(qz), float(qw)]\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.quaternion_to_euler_angle","title":"<code>quaternion_to_euler_angle(qx, qy, qz, qw)</code>  <code>staticmethod</code>","text":"<p>Convert quaternion to euler angles.</p> <p>Parameters:</p> Name Type Description Default <code>qx</code> <code>float</code> <p>Quaternion x.</p> required <code>qy</code> <code>float</code> <p>Quaternion y.</p> required <code>qz</code> <code>float</code> <p>Quaternion z.</p> required <code>qw</code> <code>float</code> <p>Quaternion w.</p> required <p>Returns:</p> Type Description <code>tuple[float, float, float]</code> <p>tuple[float, float, float]: Euler angles (roll, pitch, yaw) in radians.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>@staticmethod\ndef quaternion_to_euler_angle(qx: float, qy: float, qz: float, qw: float) -&gt; tuple[float, float, float]:\n    \"\"\"\n    Convert quaternion to euler angles.\n\n    Args:\n        qx (float): Quaternion x.\n        qy (float): Quaternion y.\n        qz (float): Quaternion z.\n        qw (float): Quaternion w.\n\n    Returns:\n        tuple[float, float, float]: Euler angles (roll, pitch, yaw) in radians.\n    \"\"\"\n    ysqr = qy * qy\n\n    t0 = +2.0 * (qw * qx + qy * qz)\n    t1 = +1.0 - 2.0 * (qx * qx + ysqr)\n    roll = np.arctan2(t0, t1)\n\n    t2 = +2.0 * (qw * qy - qz * qx)\n\n    t2 = np.clip(t2, a_min=-1.0, a_max=1.0)\n    pitch = np.arcsin(t2)\n\n    t3 = +2.0 * (qw * qz + qx * qy)\n    t4 = +1.0 - 2.0 * (ysqr + qz * qz)\n    yaw = np.arctan2(t3, t4)\n\n    return float(roll), float(pitch), float(yaw)\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.to_list","title":"<code>to_list()</code>","text":"<p>Return a list [x, y, z, roll, pitch, yaw] corresponding to the pose's parameters.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: A list of the pose's parameters.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>def to_list(self) -&gt; list[float]:\n    \"\"\"\n    Return a list [x, y, z, roll, pitch, yaw] corresponding to the pose's parameters.\n\n    Returns:\n        list[float]: A list of the pose's parameters.\n    \"\"\"\n    list_pos = [self.x, self.y, self.z, self.roll, self.pitch, self.yaw]\n    return list(map(float, list_pos))\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.to_transformation_matrix","title":"<code>to_transformation_matrix()</code>","text":"<p>Converts the pose into a 4x4 transformation matrix.</p> <p>The transformation matrix represents the pose of an object in a 3D coordinate system. It combines translation and rotation into a single homogeneous transformation.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 4x4 transformation matrix of type float, where: - The upper-left 3x3 submatrix represents the rotation. - The last column represents the translation. - The bottom row is [0, 0, 0, 1] for homogeneity.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>def to_transformation_matrix(self) -&gt; np.ndarray:\n    \"\"\"\n    Converts the pose into a 4x4 transformation matrix.\n\n    The transformation matrix represents the pose of an object in a\n    3D coordinate system. It combines translation and rotation into\n    a single homogeneous transformation.\n\n    Returns:\n        np.ndarray: A 4x4 transformation matrix of type float, where:\n            - The upper-left 3x3 submatrix represents the rotation.\n            - The last column represents the translation.\n            - The bottom row is [0, 0, 0, 1] for homogeneity.\n    \"\"\"\n    # Compute the rotation matrix from roll, pitch, yaw\n    rx = np.array([[1, 0, 0], [0, np.cos(self.roll), -np.sin(self.roll)], [0, np.sin(self.roll), np.cos(self.roll)]])\n\n    ry = np.array([[np.cos(self.pitch), 0, np.sin(self.pitch)], [0, 1, 0], [-np.sin(self.pitch), 0, np.cos(self.pitch)]])\n\n    rz = np.array([[np.cos(self.yaw), -np.sin(self.yaw), 0], [np.sin(self.yaw), np.cos(self.yaw), 0], [0, 0, 1]])\n\n    # Combined rotation matrix (Rz * Ry * Rx)\n    rotation_matrix = rz @ ry @ rx\n\n    # Translation vector\n    translation_vector = np.array([self.x, self.y, self.z])\n\n    # Construct the 4x4 transformation matrix\n    transformation_matrix = np.eye(4)  # Start with an identity matrix\n    transformation_matrix[:3, :3] = rotation_matrix\n    transformation_matrix[:3, 3] = translation_vector\n\n    return transformation_matrix\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object.PoseObjectPNP.xy_coordinate","title":"<code>xy_coordinate()</code>","text":"<p>Returns the (x, y) coordinates of the pose.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: [x, y] coordinates.</p> Source code in <code>robot_workspace/objects/pose_object.py</code> <pre><code>def xy_coordinate(self) -&gt; list[float]:\n    \"\"\"\n    Returns the (x, y) coordinates of the pose.\n\n    Returns:\n        list[float]: [x, y] coordinates.\n    \"\"\"\n    return [self.x, self.y]\n</code></pre>"},{"location":"api/#robot_workspace.objects.pose_object-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.workspaces.workspace","title":"<code>robot_workspace.workspaces.workspace</code>","text":""},{"location":"api/#robot_workspace.workspaces.workspace-classes","title":"Classes","text":""},{"location":"api/#robot_workspace.workspaces.workspace.Workspace","title":"<code>Workspace</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class representing a robotic workspace.</p> <p>A workspace defines a region where a robot can pick and place objects. It handles coordinate transformations between camera images and world coordinates.</p> <p>Attributes:</p> Name Type Description <code>_id</code> <code>str</code> <p>Unique identifier for the workspace.</p> <code>_verbose</code> <code>bool</code> <p>If True, enables verbose logging.</p> <code>_logger</code> <code>Logger</code> <p>Logger instance.</p> <code>_observation_pose</code> <code>PoseObjectPNP</code> <p>Optimal pose for the camera to observe the workspace.</p> <code>_xy_ul_wc</code> <code>PoseObjectPNP</code> <p>Upper-left corner in world coordinates.</p> <code>_xy_ll_wc</code> <code>PoseObjectPNP</code> <p>Lower-left corner in world coordinates.</p> <code>_xy_ur_wc</code> <code>PoseObjectPNP</code> <p>Upper-right corner in world coordinates.</p> <code>_xy_lr_wc</code> <code>PoseObjectPNP</code> <p>Lower-right corner in world coordinates.</p> <code>_xy_center_wc</code> <code>PoseObjectPNP</code> <p>Center of the workspace in world coordinates.</p> <code>_width_m</code> <code>float</code> <p>Width of the workspace in meters.</p> <code>_height_m</code> <code>float</code> <p>Height of the workspace in meters.</p> <code>_img_shape</code> <code>tuple[int, int, int]</code> <p>Shape of the camera image (height, width, channels).</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>class Workspace(ABC):\n    \"\"\"\n    Abstract base class representing a robotic workspace.\n\n    A workspace defines a region where a robot can pick and place objects.\n    It handles coordinate transformations between camera images and world coordinates.\n\n    Attributes:\n        _id (str): Unique identifier for the workspace.\n        _verbose (bool): If True, enables verbose logging.\n        _logger (logging.Logger): Logger instance.\n        _observation_pose (PoseObjectPNP): Optimal pose for the camera to observe the workspace.\n        _xy_ul_wc (PoseObjectPNP): Upper-left corner in world coordinates.\n        _xy_ll_wc (PoseObjectPNP): Lower-left corner in world coordinates.\n        _xy_ur_wc (PoseObjectPNP): Upper-right corner in world coordinates.\n        _xy_lr_wc (PoseObjectPNP): Lower-right corner in world coordinates.\n        _xy_center_wc (PoseObjectPNP): Center of the workspace in world coordinates.\n        _width_m (float): Width of the workspace in meters.\n        _height_m (float): Height of the workspace in meters.\n        _img_shape (tuple[int, int, int]): Shape of the camera image (height, width, channels).\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    @log_start_end_cls()\n    def __init__(self, workspace_id: str, verbose: bool = False) -&gt; None:\n        \"\"\"\n        Initializes the workspace.\n\n        Args:\n            workspace_id (str): Unique ID of the workspace.\n            verbose (bool): Whether to enable verbose logging.\n        \"\"\"\n        self._id = workspace_id\n        self._verbose = verbose\n        self._logger = logging.getLogger(\"robot_workspace\")\n\n        self._set_observation_pose()\n\n        self._set_4corners_of_workspace()\n\n        self._calc_width_height()\n\n        self._calc_center_of_workspace()\n\n    def __str__(self) -&gt; str:\n        size = f\"width = {self._width_m:.2f}, height = {self._height_m:.2f}\"\n        return \"Workspace \" + self.id() + \"\\n\" + size + \"\\n\" + str(self._xy_center_wc)\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n\n    # *** PUBLIC GET methods ***\n\n    @log_start_end_cls()\n    def is_visible(self, camera_pose: PoseObjectPNP) -&gt; bool:\n        \"\"\"\n        Checks whether the workspace is visible from the given camera pose.\n\n        Args:\n            camera_pose (PoseObjectPNP): The current pose of the camera.\n\n        Returns:\n            bool: True if the workspace is considered visible, False otherwise.\n        \"\"\"\n        if self.verbose() and self._logger:\n            self._logger.debug(f\"is_visible check: camera={camera_pose}, obs={self._observation_pose}\")\n\n        if self._observation_pose is None:\n            return False\n\n        return camera_pose.approx_eq_xyz(self._observation_pose)\n\n    # *** PUBLIC methods ***\n\n    def set_img_shape(self, img_shape: tuple[int, int, int]) -&gt; None:\n        \"\"\"\n        Sets the image shape for the workspace.\n\n        Args:\n            img_shape (tuple[int, int, int]): (height, width, channels).\n        \"\"\"\n        self._img_shape = img_shape\n\n    @abstractmethod\n    def transform_camera2world_coords(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float = 0.0) -&gt; PoseObjectPNP:\n        \"\"\"\n        Transforms relative image coordinates to world coordinates.\n\n        Args:\n            workspace_id (str): ID of the workspace.\n            u_rel (float): Normalized horizontal coordinate [0, 1].\n            v_rel (float): Normalized vertical coordinate [0, 1].\n            yaw (float): Orientation of the object at the given point.\n\n        Returns:\n            PoseObjectPNP: Corresponding pose in world coordinates.\n        \"\"\"\n        pass\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    # *** PRIVATE methods ***\n\n    @abstractmethod\n    def _set_4corners_of_workspace(self) -&gt; None:\n        \"\"\"Sets the four corners of the workspace in world coordinates.\"\"\"\n        pass\n\n    @abstractmethod\n    def _set_observation_pose(self) -&gt; None:\n        \"\"\"Sets the optimal observation pose for the workspace.\"\"\"\n        pass\n\n    @log_start_end_cls()\n    def _calc_width_height(self) -&gt; None:\n        \"\"\"Calculates the physical width and height of the workspace.\"\"\"\n        self._width_m, self._height_m = Object.calc_width_height(self._xy_ul_wc, self._xy_lr_wc)\n\n    @log_start_end_cls()\n    def _calc_center_of_workspace(self) -&gt; None:\n        \"\"\"Calculates the center point of the workspace.\"\"\"\n        if self._xy_ll_wc is None or self._xy_ul_wc is None or self._xy_lr_wc is None:\n            raise ValueError(\"Workspace corners must be set before calculating center.\")\n\n        dx = self._xy_ll_wc.x - self._xy_ul_wc.x\n        dy = self._xy_ll_wc.y - self._xy_lr_wc.y\n\n        self._xy_center_wc = self._xy_lr_wc.copy_with_offsets(-dx / 2.0, dy / 2.0)\n\n        if self.verbose() and self._logger:\n            self._logger.debug(f\"_calc_center_of_workspace: {self._xy_center_wc}, {self._xy_ll_wc.x}\")\n\n    # *** PRIVATE STATIC/CLASS methods ***\n\n    # *** PUBLIC properties ***\n\n    def id(self) -&gt; str:\n        \"\"\"Returns the workspace ID.\"\"\"\n        return self._id\n\n    # def environment(self) -&gt; \"Environment\":\n    #     return self._environment\n\n    def xy_ul_wc(self) -&gt; PoseObjectPNP:\n        \"\"\"Returns the upper-left corner in world coordinates.\"\"\"\n        return self._xy_ul_wc\n\n    def xy_ll_wc(self) -&gt; PoseObjectPNP:\n        \"\"\"Returns the lower-left corner in world coordinates.\"\"\"\n        return self._xy_ll_wc\n\n    def xy_ur_wc(self) -&gt; PoseObjectPNP:\n        \"\"\"Returns the upper-right corner in world coordinates.\"\"\"\n        return self._xy_ur_wc\n\n    def xy_lr_wc(self) -&gt; PoseObjectPNP:\n        \"\"\"Returns the lower-right corner in world coordinates.\"\"\"\n        return self._xy_lr_wc\n\n    def xy_center_wc(self) -&gt; PoseObjectPNP:\n        \"\"\"Returns the center of the workspace in world coordinates.\"\"\"\n        return self._xy_center_wc\n\n    def width_m(self) -&gt; float:\n        \"\"\"Returns the width of the workspace in meters.\"\"\"\n        return self._width_m\n\n    def height_m(self) -&gt; float:\n        \"\"\"Returns the height of the workspace in meters.\"\"\"\n        return self._height_m\n\n    def img_shape(self) -&gt; tuple[int, int, int]:\n        \"\"\"Returns the image shape of the workspace.\"\"\"\n        return self._img_shape\n\n    def observation_pose(self) -&gt; PoseObjectPNP:\n        \"\"\"Returns the optimal observation pose.\"\"\"\n        return self._observation_pose\n\n    def verbose(self) -&gt; bool:\n        \"\"\"Returns whether verbose logging is enabled.\"\"\"\n        return self._verbose\n\n    # *** PRIVATE variables ***\n\n    _id: str = \"\"\n    _xy_ul_wc: PoseObjectPNP = None\n    _xy_ll_wc: PoseObjectPNP = None\n    _xy_ur_wc: PoseObjectPNP = None\n    _xy_lr_wc: PoseObjectPNP = None\n    _xy_center_wc: PoseObjectPNP = None\n    _width_m: float = 0.0\n    _height_m: float = 0.0\n    _img_shape: tuple[int, int, int] = None\n    _observation_pose: PoseObjectPNP = None\n    _verbose: bool = False\n    _logger: logging.Logger = None\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.__init__","title":"<code>__init__(workspace_id, verbose=False)</code>","text":"<p>Initializes the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>Unique ID of the workspace.</p> required <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> <code>False</code> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>@log_start_end_cls()\ndef __init__(self, workspace_id: str, verbose: bool = False) -&gt; None:\n    \"\"\"\n    Initializes the workspace.\n\n    Args:\n        workspace_id (str): Unique ID of the workspace.\n        verbose (bool): Whether to enable verbose logging.\n    \"\"\"\n    self._id = workspace_id\n    self._verbose = verbose\n    self._logger = logging.getLogger(\"robot_workspace\")\n\n    self._set_observation_pose()\n\n    self._set_4corners_of_workspace()\n\n    self._calc_width_height()\n\n    self._calc_center_of_workspace()\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.height_m","title":"<code>height_m()</code>","text":"<p>Returns the height of the workspace in meters.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def height_m(self) -&gt; float:\n    \"\"\"Returns the height of the workspace in meters.\"\"\"\n    return self._height_m\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.id","title":"<code>id()</code>","text":"<p>Returns the workspace ID.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def id(self) -&gt; str:\n    \"\"\"Returns the workspace ID.\"\"\"\n    return self._id\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.img_shape","title":"<code>img_shape()</code>","text":"<p>Returns the image shape of the workspace.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def img_shape(self) -&gt; tuple[int, int, int]:\n    \"\"\"Returns the image shape of the workspace.\"\"\"\n    return self._img_shape\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.is_visible","title":"<code>is_visible(camera_pose)</code>","text":"<p>Checks whether the workspace is visible from the given camera pose.</p> <p>Parameters:</p> Name Type Description Default <code>camera_pose</code> <code>PoseObjectPNP</code> <p>The current pose of the camera.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the workspace is considered visible, False otherwise.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>@log_start_end_cls()\ndef is_visible(self, camera_pose: PoseObjectPNP) -&gt; bool:\n    \"\"\"\n    Checks whether the workspace is visible from the given camera pose.\n\n    Args:\n        camera_pose (PoseObjectPNP): The current pose of the camera.\n\n    Returns:\n        bool: True if the workspace is considered visible, False otherwise.\n    \"\"\"\n    if self.verbose() and self._logger:\n        self._logger.debug(f\"is_visible check: camera={camera_pose}, obs={self._observation_pose}\")\n\n    if self._observation_pose is None:\n        return False\n\n    return camera_pose.approx_eq_xyz(self._observation_pose)\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.observation_pose","title":"<code>observation_pose()</code>","text":"<p>Returns the optimal observation pose.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def observation_pose(self) -&gt; PoseObjectPNP:\n    \"\"\"Returns the optimal observation pose.\"\"\"\n    return self._observation_pose\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.set_img_shape","title":"<code>set_img_shape(img_shape)</code>","text":"<p>Sets the image shape for the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>img_shape</code> <code>tuple[int, int, int]</code> <p>(height, width, channels).</p> required Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def set_img_shape(self, img_shape: tuple[int, int, int]) -&gt; None:\n    \"\"\"\n    Sets the image shape for the workspace.\n\n    Args:\n        img_shape (tuple[int, int, int]): (height, width, channels).\n    \"\"\"\n    self._img_shape = img_shape\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.transform_camera2world_coords","title":"<code>transform_camera2world_coords(workspace_id, u_rel, v_rel, yaw=0.0)</code>  <code>abstractmethod</code>","text":"<p>Transforms relative image coordinates to world coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace.</p> required <code>u_rel</code> <code>float</code> <p>Normalized horizontal coordinate [0, 1].</p> required <code>v_rel</code> <code>float</code> <p>Normalized vertical coordinate [0, 1].</p> required <code>yaw</code> <code>float</code> <p>Orientation of the object at the given point.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>PoseObjectPNP</code> <code>PoseObjectPNP</code> <p>Corresponding pose in world coordinates.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>@abstractmethod\ndef transform_camera2world_coords(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float = 0.0) -&gt; PoseObjectPNP:\n    \"\"\"\n    Transforms relative image coordinates to world coordinates.\n\n    Args:\n        workspace_id (str): ID of the workspace.\n        u_rel (float): Normalized horizontal coordinate [0, 1].\n        v_rel (float): Normalized vertical coordinate [0, 1].\n        yaw (float): Orientation of the object at the given point.\n\n    Returns:\n        PoseObjectPNP: Corresponding pose in world coordinates.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.verbose","title":"<code>verbose()</code>","text":"<p>Returns whether verbose logging is enabled.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def verbose(self) -&gt; bool:\n    \"\"\"Returns whether verbose logging is enabled.\"\"\"\n    return self._verbose\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.width_m","title":"<code>width_m()</code>","text":"<p>Returns the width of the workspace in meters.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def width_m(self) -&gt; float:\n    \"\"\"Returns the width of the workspace in meters.\"\"\"\n    return self._width_m\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.xy_center_wc","title":"<code>xy_center_wc()</code>","text":"<p>Returns the center of the workspace in world coordinates.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def xy_center_wc(self) -&gt; PoseObjectPNP:\n    \"\"\"Returns the center of the workspace in world coordinates.\"\"\"\n    return self._xy_center_wc\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.xy_ll_wc","title":"<code>xy_ll_wc()</code>","text":"<p>Returns the lower-left corner in world coordinates.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def xy_ll_wc(self) -&gt; PoseObjectPNP:\n    \"\"\"Returns the lower-left corner in world coordinates.\"\"\"\n    return self._xy_ll_wc\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.xy_lr_wc","title":"<code>xy_lr_wc()</code>","text":"<p>Returns the lower-right corner in world coordinates.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def xy_lr_wc(self) -&gt; PoseObjectPNP:\n    \"\"\"Returns the lower-right corner in world coordinates.\"\"\"\n    return self._xy_lr_wc\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.xy_ul_wc","title":"<code>xy_ul_wc()</code>","text":"<p>Returns the upper-left corner in world coordinates.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def xy_ul_wc(self) -&gt; PoseObjectPNP:\n    \"\"\"Returns the upper-left corner in world coordinates.\"\"\"\n    return self._xy_ul_wc\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace.Workspace.xy_ur_wc","title":"<code>xy_ur_wc()</code>","text":"<p>Returns the upper-right corner in world coordinates.</p> Source code in <code>robot_workspace/workspaces/workspace.py</code> <pre><code>def xy_ur_wc(self) -&gt; PoseObjectPNP:\n    \"\"\"Returns the upper-right corner in world coordinates.\"\"\"\n    return self._xy_ur_wc\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.workspace-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.workspaces.niryo_workspace","title":"<code>robot_workspace.workspaces.niryo_workspace</code>","text":""},{"location":"api/#robot_workspace.workspaces.niryo_workspace-classes","title":"Classes","text":""},{"location":"api/#robot_workspace.workspaces.niryo_workspace.NiryoWorkspace","title":"<code>NiryoWorkspace</code>","text":"<p>               Bases: <code>Workspace</code></p> <p>Implementation of Workspace for the Niryo Ned2 robot.</p> <p>This class provides specific coordinate transformations and poses for the Niryo Ned2 robotic arm and its mounted camera.</p> Source code in <code>robot_workspace/workspaces/niryo_workspace.py</code> <pre><code>class NiryoWorkspace(Workspace):\n    \"\"\"\n    Implementation of Workspace for the Niryo Ned2 robot.\n\n    This class provides specific coordinate transformations and poses\n    for the Niryo Ned2 robotic arm and its mounted camera.\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(\n        self,\n        workspace_id: str,\n        environment: EnvironmentProtocol,\n        verbose: bool = False,\n        config: WorkspaceConfig | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the NiryoWorkspace.\n\n        Args:\n            workspace_id (str): Unique ID of the workspace.\n            environment (EnvironmentProtocol): Object providing robot environment access.\n            verbose (bool): Whether to enable verbose output.\n            config (WorkspaceConfig, optional): Optional workspace configuration.\n        \"\"\"\n        self._environment = environment\n        self._config = config\n        self._logger = logging.getLogger(\"robot_workspace\")\n\n        super().__init__(workspace_id, verbose)\n\n    # *** PUBLIC GET methods ***\n\n    # *** PUBLIC methods ***\n\n    @classmethod\n    def from_config(cls, config: WorkspaceConfig, environment: EnvironmentProtocol, verbose: bool = False) -&gt; NiryoWorkspace:\n        \"\"\"\n        Creates a NiryoWorkspace instance from a configuration object.\n\n        Args:\n            config (WorkspaceConfig): Configuration instance.\n            environment (EnvironmentProtocol): Environment object.\n            verbose (bool): Whether to enable verbose output.\n\n        Returns:\n            NiryoWorkspace: The initialized workspace instance.\n        \"\"\"\n        workspace = cls(config.id, environment, verbose, config)\n\n        # Override observation pose from config if available\n        if config.observation_pose:\n            workspace._observation_pose = PoseObjectPNP(\n                x=config.observation_pose.x,\n                y=config.observation_pose.y,\n                z=config.observation_pose.z,\n                roll=config.observation_pose.roll,\n                pitch=config.observation_pose.pitch,\n                yaw=config.observation_pose.yaw,\n            )\n\n        # Override image shape from config if available\n        if config.image_shape:\n            workspace.set_img_shape(config.image_shape)\n\n        return workspace\n\n    def transform_camera2world_coords(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float = 0.0) -&gt; PoseObjectPNP:\n        \"\"\"\n        Transforms relative image coordinates to Niryo world coordinates.\n\n        Args:\n            workspace_id (str): ID of the workspace.\n            u_rel (float): Normalized horizontal coordinate [0, 1].\n            v_rel (float): Normalized vertical coordinate [0, 1].\n            yaw (float): Orientation of the object.\n\n        Returns:\n            PoseObjectPNP: Corresponding pose in world coordinates.\n        \"\"\"\n        if self.verbose():\n            self._logger.debug(\n                f\"transform_camera2world_coords input - workspace_id: {workspace_id}, u_rel: {u_rel}, v_rel: {v_rel}, yaw: {yaw}\"\n            )\n\n        obj_coords = self._environment.get_robot_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n\n        if self.verbose():\n            self._logger.debug(f\"transform_camera2world_coords output: {obj_coords}\")\n\n        return obj_coords\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    # *** PRIVATE methods ***\n\n    @log_start_end_cls()\n    def _set_4corners_of_workspace(self) -&gt; None:\n        \"\"\"Sets the four corners of the workspace in world coordinates.\"\"\"\n        self._xy_ul_wc = self.transform_camera2world_coords(self._id, 0.0, 0.0)\n        self._xy_ll_wc = self.transform_camera2world_coords(self._id, 0.0, 1.0)\n        self._xy_ur_wc = self.transform_camera2world_coords(self._id, 1.0, 0.0)\n        self._xy_lr_wc = self.transform_camera2world_coords(self._id, 1.0, 1.0)\n\n        if self.verbose():\n            self._logger.debug(f\"Workspace corners - UL: {self._xy_ul_wc}, LL: {self._xy_ll_wc}\")\n            self._logger.debug(f\"Workspace corners - UR: {self._xy_ur_wc}, LR: {self._xy_lr_wc}\")\n\n    @log_start_end_cls()\n    def _set_observation_pose(self) -&gt; None:\n        \"\"\"Sets the observation pose using configuration data.\"\"\"\n        if not self._config:\n            raise ValueError(\n                f\"No configuration provided for workspace '{self._id}'. \" \"Initialize with config_path or from_config().\"\n            )\n\n        if not self._config.observation_pose:\n            raise ValueError(f\"No observation pose defined in config for workspace '{self._id}'\")\n\n        self._observation_pose = PoseObjectPNP(\n            x=self._config.observation_pose.x,\n            y=self._config.observation_pose.y,\n            z=self._config.observation_pose.z,\n            roll=self._config.observation_pose.roll,\n            pitch=self._config.observation_pose.pitch,\n            yaw=self._config.observation_pose.yaw,\n        )\n\n    # *** PRIVATE STATIC/CLASS methods ***\n\n    # *** PUBLIC properties ***\n\n    def environment(self) -&gt; EnvironmentProtocol:\n        \"\"\"Returns the environment associated with this workspace.\"\"\"\n        return self._environment\n\n    # *** PRIVATE variables ***\n\n    _environment: EnvironmentProtocol = None\n    _logger: logging.Logger = None\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.niryo_workspace.NiryoWorkspace-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.workspaces.niryo_workspace.NiryoWorkspace.__init__","title":"<code>__init__(workspace_id, environment, verbose=False, config=None)</code>","text":"<p>Initializes the NiryoWorkspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>Unique ID of the workspace.</p> required <code>environment</code> <code>EnvironmentProtocol</code> <p>Object providing robot environment access.</p> required <code>verbose</code> <code>bool</code> <p>Whether to enable verbose output.</p> <code>False</code> <code>config</code> <code>WorkspaceConfig</code> <p>Optional workspace configuration.</p> <code>None</code> Source code in <code>robot_workspace/workspaces/niryo_workspace.py</code> <pre><code>def __init__(\n    self,\n    workspace_id: str,\n    environment: EnvironmentProtocol,\n    verbose: bool = False,\n    config: WorkspaceConfig | None = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the NiryoWorkspace.\n\n    Args:\n        workspace_id (str): Unique ID of the workspace.\n        environment (EnvironmentProtocol): Object providing robot environment access.\n        verbose (bool): Whether to enable verbose output.\n        config (WorkspaceConfig, optional): Optional workspace configuration.\n    \"\"\"\n    self._environment = environment\n    self._config = config\n    self._logger = logging.getLogger(\"robot_workspace\")\n\n    super().__init__(workspace_id, verbose)\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.niryo_workspace.NiryoWorkspace.environment","title":"<code>environment()</code>","text":"<p>Returns the environment associated with this workspace.</p> Source code in <code>robot_workspace/workspaces/niryo_workspace.py</code> <pre><code>def environment(self) -&gt; EnvironmentProtocol:\n    \"\"\"Returns the environment associated with this workspace.\"\"\"\n    return self._environment\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.niryo_workspace.NiryoWorkspace.from_config","title":"<code>from_config(config, environment, verbose=False)</code>  <code>classmethod</code>","text":"<p>Creates a NiryoWorkspace instance from a configuration object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WorkspaceConfig</code> <p>Configuration instance.</p> required <code>environment</code> <code>EnvironmentProtocol</code> <p>Environment object.</p> required <code>verbose</code> <code>bool</code> <p>Whether to enable verbose output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>NiryoWorkspace</code> <code>NiryoWorkspace</code> <p>The initialized workspace instance.</p> Source code in <code>robot_workspace/workspaces/niryo_workspace.py</code> <pre><code>@classmethod\ndef from_config(cls, config: WorkspaceConfig, environment: EnvironmentProtocol, verbose: bool = False) -&gt; NiryoWorkspace:\n    \"\"\"\n    Creates a NiryoWorkspace instance from a configuration object.\n\n    Args:\n        config (WorkspaceConfig): Configuration instance.\n        environment (EnvironmentProtocol): Environment object.\n        verbose (bool): Whether to enable verbose output.\n\n    Returns:\n        NiryoWorkspace: The initialized workspace instance.\n    \"\"\"\n    workspace = cls(config.id, environment, verbose, config)\n\n    # Override observation pose from config if available\n    if config.observation_pose:\n        workspace._observation_pose = PoseObjectPNP(\n            x=config.observation_pose.x,\n            y=config.observation_pose.y,\n            z=config.observation_pose.z,\n            roll=config.observation_pose.roll,\n            pitch=config.observation_pose.pitch,\n            yaw=config.observation_pose.yaw,\n        )\n\n    # Override image shape from config if available\n    if config.image_shape:\n        workspace.set_img_shape(config.image_shape)\n\n    return workspace\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.niryo_workspace.NiryoWorkspace.transform_camera2world_coords","title":"<code>transform_camera2world_coords(workspace_id, u_rel, v_rel, yaw=0.0)</code>","text":"<p>Transforms relative image coordinates to Niryo world coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace.</p> required <code>u_rel</code> <code>float</code> <p>Normalized horizontal coordinate [0, 1].</p> required <code>v_rel</code> <code>float</code> <p>Normalized vertical coordinate [0, 1].</p> required <code>yaw</code> <code>float</code> <p>Orientation of the object.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>PoseObjectPNP</code> <code>PoseObjectPNP</code> <p>Corresponding pose in world coordinates.</p> Source code in <code>robot_workspace/workspaces/niryo_workspace.py</code> <pre><code>def transform_camera2world_coords(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float = 0.0) -&gt; PoseObjectPNP:\n    \"\"\"\n    Transforms relative image coordinates to Niryo world coordinates.\n\n    Args:\n        workspace_id (str): ID of the workspace.\n        u_rel (float): Normalized horizontal coordinate [0, 1].\n        v_rel (float): Normalized vertical coordinate [0, 1].\n        yaw (float): Orientation of the object.\n\n    Returns:\n        PoseObjectPNP: Corresponding pose in world coordinates.\n    \"\"\"\n    if self.verbose():\n        self._logger.debug(\n            f\"transform_camera2world_coords input - workspace_id: {workspace_id}, u_rel: {u_rel}, v_rel: {v_rel}, yaw: {yaw}\"\n        )\n\n    obj_coords = self._environment.get_robot_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n\n    if self.verbose():\n        self._logger.debug(f\"transform_camera2world_coords output: {obj_coords}\")\n\n    return obj_coords\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.niryo_workspace-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.workspaces.widowx_workspace","title":"<code>robot_workspace.workspaces.widowx_workspace</code>","text":""},{"location":"api/#robot_workspace.workspaces.widowx_workspace-classes","title":"Classes","text":""},{"location":"api/#robot_workspace.workspaces.widowx_workspace.WidowXWorkspace","title":"<code>WidowXWorkspace</code>","text":"<p>               Bases: <code>Workspace</code></p> <p>Implementation of Workspace for the WidowX 250 6DOF robot.</p> <p>The WidowX robot typically uses a third-person camera view rather than a gripper-mounted camera.</p> Source code in <code>robot_workspace/workspaces/widowx_workspace.py</code> <pre><code>class WidowXWorkspace(Workspace):\n    \"\"\"\n    Implementation of Workspace for the WidowX 250 6DOF robot.\n\n    The WidowX robot typically uses a third-person camera view\n    rather than a gripper-mounted camera.\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(\n        self,\n        workspace_id: str,\n        environment: EnvironmentProtocol,\n        verbose: bool = False,\n        config: WorkspaceConfig | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the WidowXWorkspace.\n\n        Args:\n            workspace_id (str): Unique ID of the workspace.\n            environment (EnvironmentProtocol): Object implementing EnvironmentProtocol.\n            verbose (bool): Whether to enable verbose output.\n            config (WorkspaceConfig, optional): Optional workspace configuration.\n        \"\"\"\n        self._environment = environment\n        self._config = config\n        self._logger = logging.getLogger(\"robot_workspace\")\n\n        super().__init__(workspace_id, verbose)\n\n    # *** PUBLIC GET methods ***\n\n    # *** PUBLIC methods ***\n\n    @classmethod\n    def from_config(cls, config: WorkspaceConfig, environment: EnvironmentProtocol, verbose: bool = False) -&gt; WidowXWorkspace:\n        \"\"\"\n        Creates a WidowXWorkspace from a configuration object.\n\n        Args:\n            config (WorkspaceConfig): Workspace configuration.\n            environment (EnvironmentProtocol): Environment object.\n            verbose (bool): Whether to enable verbose output.\n\n        Returns:\n            WidowXWorkspace: The initialized workspace instance.\n        \"\"\"\n        workspace = cls(config.id, environment, verbose, config)\n\n        # Override observation pose from config if available\n        if config.observation_pose:\n            workspace._observation_pose = PoseObjectPNP(\n                x=config.observation_pose.x,\n                y=config.observation_pose.y,\n                z=config.observation_pose.z,\n                roll=config.observation_pose.roll,\n                pitch=config.observation_pose.pitch,\n                yaw=config.observation_pose.yaw,\n            )\n\n        # Override image shape from config if available\n        if config.image_shape:\n            workspace.set_img_shape(config.image_shape)\n\n        return workspace\n\n    def transform_camera2world_coords(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float = 0.0) -&gt; PoseObjectPNP:\n        \"\"\"\n        Transforms relative image coordinates to WidowX world coordinates.\n\n        Args:\n            workspace_id (str): ID of the workspace.\n            u_rel (float): Normalized horizontal coordinate [0, 1].\n            v_rel (float): Normalized vertical coordinate [0, 1].\n            yaw (float): Orientation of the object.\n\n        Returns:\n            PoseObjectPNP: Corresponding pose in world coordinates.\n        \"\"\"\n        if self.verbose() and self._logger:\n            self._logger.debug(\n                f\"transform_camera2world_coords input - workspace_id: {workspace_id}, u_rel: {u_rel}, v_rel: {v_rel}, yaw: {yaw}\"\n            )\n\n        # Delegate to environment's transformation if available\n        if hasattr(self, \"_environment\") and self._environment is not None:\n            obj_coords = self._environment.get_robot_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n            if self.verbose() and self._logger:\n                self._logger.debug(f\"transform_camera2world_coords output: {obj_coords}\")\n            return obj_coords\n\n        # Fallback: Linear interpolation using workspace corners or defaults\n        if self._xy_ul_wc is not None and self._xy_lr_wc is not None:\n            x_min = self._xy_lr_wc.x\n            x_max = self._xy_ul_wc.x\n            y_min = self._xy_lr_wc.y\n            y_max = self._xy_ul_wc.y\n\n            x = x_max - u_rel * (x_max - x_min)\n            y = y_max - v_rel * (y_max - y_min)\n        else:\n            x = 0.5 - u_rel * 0.4\n            y = 0.2 - v_rel * 0.4\n\n        obj_coords = PoseObjectPNP(x, y, 0.05, 0.0, 1.57, yaw)\n\n        if self.verbose() and self._logger:\n            self._logger.debug(f\"transform_camera2world_coords output (fallback): {obj_coords}\")\n\n        return obj_coords\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    # *** PRIVATE methods ***\n\n    @log_start_end_cls()\n    def _set_4corners_of_workspace(self) -&gt; None:\n        \"\"\"Sets the four corners of the workspace using the transform method.\"\"\"\n        self._xy_ul_wc = self.transform_camera2world_coords(self._id, 0.0, 0.0)\n        self._xy_ll_wc = self.transform_camera2world_coords(self._id, 1.0, 0.0)\n        self._xy_ur_wc = self.transform_camera2world_coords(self._id, 0.0, 1.0)\n        self._xy_lr_wc = self.transform_camera2world_coords(self._id, 1.0, 1.0)\n\n        if self.verbose():\n            self._logger.debug(f\"Workspace corners - UL: {self._xy_ul_wc}, LL: {self._xy_ll_wc}\")\n            self._logger.debug(f\"Workspace corners - UR: {self._xy_ur_wc}, LR: {self._xy_lr_wc}\")\n\n    @log_start_end_cls()\n    def _set_observation_pose(self) -&gt; None:\n        \"\"\"Sets the observation pose using configuration data.\"\"\"\n        if not self._config:\n            raise ValueError(\n                f\"No configuration provided for workspace '{self._id}'. \" \"Initialize with config_path or from_config().\"\n            )\n\n        if not self._config.observation_pose:\n            raise ValueError(f\"No observation pose defined in config for workspace '{self._id}'\")\n\n        self._observation_pose = PoseObjectPNP(\n            x=self._config.observation_pose.x,\n            y=self._config.observation_pose.y,\n            z=self._config.observation_pose.z,\n            roll=self._config.observation_pose.roll,\n            pitch=self._config.observation_pose.pitch,\n            yaw=self._config.observation_pose.yaw,\n        )\n\n    # *** PRIVATE STATIC/CLASS methods ***\n\n    # *** PUBLIC properties ***\n\n    def environment(self) -&gt; EnvironmentProtocol:\n        \"\"\"Returns the environment associated with this workspace.\"\"\"\n        return self._environment\n\n    # *** PRIVATE variables ***\n\n    _environment: EnvironmentProtocol = None\n    _logger: logging.Logger = None\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.widowx_workspace.WidowXWorkspace-functions","title":"Functions","text":""},{"location":"api/#robot_workspace.workspaces.widowx_workspace.WidowXWorkspace.__init__","title":"<code>__init__(workspace_id, environment, verbose=False, config=None)</code>","text":"<p>Initializes the WidowXWorkspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>Unique ID of the workspace.</p> required <code>environment</code> <code>EnvironmentProtocol</code> <p>Object implementing EnvironmentProtocol.</p> required <code>verbose</code> <code>bool</code> <p>Whether to enable verbose output.</p> <code>False</code> <code>config</code> <code>WorkspaceConfig</code> <p>Optional workspace configuration.</p> <code>None</code> Source code in <code>robot_workspace/workspaces/widowx_workspace.py</code> <pre><code>def __init__(\n    self,\n    workspace_id: str,\n    environment: EnvironmentProtocol,\n    verbose: bool = False,\n    config: WorkspaceConfig | None = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the WidowXWorkspace.\n\n    Args:\n        workspace_id (str): Unique ID of the workspace.\n        environment (EnvironmentProtocol): Object implementing EnvironmentProtocol.\n        verbose (bool): Whether to enable verbose output.\n        config (WorkspaceConfig, optional): Optional workspace configuration.\n    \"\"\"\n    self._environment = environment\n    self._config = config\n    self._logger = logging.getLogger(\"robot_workspace\")\n\n    super().__init__(workspace_id, verbose)\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.widowx_workspace.WidowXWorkspace.environment","title":"<code>environment()</code>","text":"<p>Returns the environment associated with this workspace.</p> Source code in <code>robot_workspace/workspaces/widowx_workspace.py</code> <pre><code>def environment(self) -&gt; EnvironmentProtocol:\n    \"\"\"Returns the environment associated with this workspace.\"\"\"\n    return self._environment\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.widowx_workspace.WidowXWorkspace.from_config","title":"<code>from_config(config, environment, verbose=False)</code>  <code>classmethod</code>","text":"<p>Creates a WidowXWorkspace from a configuration object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WorkspaceConfig</code> <p>Workspace configuration.</p> required <code>environment</code> <code>EnvironmentProtocol</code> <p>Environment object.</p> required <code>verbose</code> <code>bool</code> <p>Whether to enable verbose output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>WidowXWorkspace</code> <code>WidowXWorkspace</code> <p>The initialized workspace instance.</p> Source code in <code>robot_workspace/workspaces/widowx_workspace.py</code> <pre><code>@classmethod\ndef from_config(cls, config: WorkspaceConfig, environment: EnvironmentProtocol, verbose: bool = False) -&gt; WidowXWorkspace:\n    \"\"\"\n    Creates a WidowXWorkspace from a configuration object.\n\n    Args:\n        config (WorkspaceConfig): Workspace configuration.\n        environment (EnvironmentProtocol): Environment object.\n        verbose (bool): Whether to enable verbose output.\n\n    Returns:\n        WidowXWorkspace: The initialized workspace instance.\n    \"\"\"\n    workspace = cls(config.id, environment, verbose, config)\n\n    # Override observation pose from config if available\n    if config.observation_pose:\n        workspace._observation_pose = PoseObjectPNP(\n            x=config.observation_pose.x,\n            y=config.observation_pose.y,\n            z=config.observation_pose.z,\n            roll=config.observation_pose.roll,\n            pitch=config.observation_pose.pitch,\n            yaw=config.observation_pose.yaw,\n        )\n\n    # Override image shape from config if available\n    if config.image_shape:\n        workspace.set_img_shape(config.image_shape)\n\n    return workspace\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.widowx_workspace.WidowXWorkspace.transform_camera2world_coords","title":"<code>transform_camera2world_coords(workspace_id, u_rel, v_rel, yaw=0.0)</code>","text":"<p>Transforms relative image coordinates to WidowX world coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace.</p> required <code>u_rel</code> <code>float</code> <p>Normalized horizontal coordinate [0, 1].</p> required <code>v_rel</code> <code>float</code> <p>Normalized vertical coordinate [0, 1].</p> required <code>yaw</code> <code>float</code> <p>Orientation of the object.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>PoseObjectPNP</code> <code>PoseObjectPNP</code> <p>Corresponding pose in world coordinates.</p> Source code in <code>robot_workspace/workspaces/widowx_workspace.py</code> <pre><code>def transform_camera2world_coords(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float = 0.0) -&gt; PoseObjectPNP:\n    \"\"\"\n    Transforms relative image coordinates to WidowX world coordinates.\n\n    Args:\n        workspace_id (str): ID of the workspace.\n        u_rel (float): Normalized horizontal coordinate [0, 1].\n        v_rel (float): Normalized vertical coordinate [0, 1].\n        yaw (float): Orientation of the object.\n\n    Returns:\n        PoseObjectPNP: Corresponding pose in world coordinates.\n    \"\"\"\n    if self.verbose() and self._logger:\n        self._logger.debug(\n            f\"transform_camera2world_coords input - workspace_id: {workspace_id}, u_rel: {u_rel}, v_rel: {v_rel}, yaw: {yaw}\"\n        )\n\n    # Delegate to environment's transformation if available\n    if hasattr(self, \"_environment\") and self._environment is not None:\n        obj_coords = self._environment.get_robot_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n        if self.verbose() and self._logger:\n            self._logger.debug(f\"transform_camera2world_coords output: {obj_coords}\")\n        return obj_coords\n\n    # Fallback: Linear interpolation using workspace corners or defaults\n    if self._xy_ul_wc is not None and self._xy_lr_wc is not None:\n        x_min = self._xy_lr_wc.x\n        x_max = self._xy_ul_wc.x\n        y_min = self._xy_lr_wc.y\n        y_max = self._xy_ul_wc.y\n\n        x = x_max - u_rel * (x_max - x_min)\n        y = y_max - v_rel * (y_max - y_min)\n    else:\n        x = 0.5 - u_rel * 0.4\n        y = 0.2 - v_rel * 0.4\n\n    obj_coords = PoseObjectPNP(x, y, 0.05, 0.0, 1.57, yaw)\n\n    if self.verbose() and self._logger:\n        self._logger.debug(f\"transform_camera2world_coords output (fallback): {obj_coords}\")\n\n    return obj_coords\n</code></pre>"},{"location":"api/#robot_workspace.workspaces.widowx_workspace-functions","title":"Functions","text":""},{"location":"architecture/","title":"Architektur-Dokumentation","text":"<p>Umfassende Dokumentation der Architektur, Entwurfsmuster und Implementierungsdetails des <code>robot_workspace</code>-Pakets.</p>"},{"location":"architecture/#systemarchitektur","title":"Systemarchitektur","text":"<p>Das <code>robot_workspace</code>-Paket trennt die Belange zwischen:</p> <ul> <li>Objekt-Repr\u00e4sentation: Erkannte Objekte mit physikalischen Eigenschaften.</li> <li>Workspace-Management: Definitionen von Roboter-Workspaces und Koordinatentransformationen.</li> <li>R\u00e4umliches Denken: Abfragen und Beziehungen zwischen Objekten.</li> </ul>"},{"location":"architecture/#komponenten-interaktion","title":"Komponenten-Interaktion","text":"graph TD     User[Benutzeranwendung] --&gt; Package[Robot Workspace Paket]     subgraph Package         Obj[Objekt-Schicht] &lt;--&gt; WS[Workspace-Schicht]     end     Package --&gt; Robot[Roboter-Kontrollschicht]     Robot --&gt; Hardware[Roboter-Hardware / Simulation]"},{"location":"architecture/#koordinatensysteme","title":"Koordinatensysteme","text":"<p>Das Paket verwendet drei verschiedene Koordinatensysteme:</p> <ol> <li>Bildkoordinaten (Pixel): (u, v) in Pixeln. Ursprung oben links.</li> <li>Relative Koordinaten (Normalisiert): (u_rel, v_rel) im Bereich [0, 1].</li> <li>Weltkoordinaten (Meter): (x, y, z) in Metern bezogen auf die Roboterbasis.</li> </ol>"},{"location":"architecture/#transformations-pipeline","title":"Transformations-Pipeline","text":"<pre><code>Bildkoordinaten (u, v)\n         \u2193\n    [Normalisierung]\n         \u2193\nRelative Koordinaten (u_rel, v_rel)\n         \u2193\n    [Workspace.transform_camera2world_coords()]\n         \u2193\nWeltkoordinaten (x, y, z, roll, pitch, yaw)\n</code></pre>"},{"location":"architecture/#entwurfsmuster","title":"Entwurfsmuster","text":"<ul> <li>Abstract Factory: F\u00fcr die Erstellung verschiedener Workspace-Typen (Niryo, WidowX).</li> <li>Collection Pattern: <code>Objects</code> als erweiterte Liste f\u00fcr r\u00e4umliche Abfragen.</li> <li>Serialization Pattern: Konsistente JSON-Serialisierung \u00fcber alle Objekte hinweg.</li> </ul>"},{"location":"development/docstrings/","title":"Docstring-Styleguide","text":"<p>Wir verwenden den Google Python Style Guide f\u00fcr alle Docstrings in diesem Projekt.</p>"},{"location":"development/docstrings/#format","title":"Format","text":"<pre><code>def funktions_name(param1: int, param2: str) -&gt; bool:\n    \"\"\"\n    Kurze Beschreibung der Funktion.\n\n    L\u00e4ngere Beschreibung, die die Logik, Seiteneffekte oder\n    alles andere erkl\u00e4rt, was f\u00fcr den Benutzer wichtig sein k\u00f6nnte.\n\n    Args:\n        param1 (int): Beschreibung des ersten Parameters.\n        param2 (str): Beschreibung des zweiten Parameters.\n\n    Returns:\n        bool: Beschreibung des R\u00fcckgabewerts.\n\n    Raises:\n        ValueError: Beschreibung, wann dieser Fehler ausgel\u00f6st wird.\n    \"\"\"\n</code></pre>"},{"location":"development/docstrings/#klassen","title":"Klassen","text":"<pre><code>class MeineKlasse:\n    \"\"\"\n    Zusammenfassung der Klasse.\n\n    Attributes:\n        attr1 (int): Beschreibung von attr1.\n        attr2 (str): Beschreibung von attr2.\n    \"\"\"\n</code></pre>"},{"location":"development/testing/","title":"Tests","text":"<p>Dokumentation der Teststrategie f\u00fcr <code>robot_workspace</code>.</p>"},{"location":"development/testing/#teststruktur","title":"Teststruktur","text":"<p>Tests sind in zwei Kategorien unterteilt:</p> <ul> <li>Unit-Tests: Testen einzelne Klassen und Methoden isoliert (unter <code>tests/unit/</code>).</li> <li>Integrationstests: Testen das Zusammenspiel mehrerer Komponenten (unter <code>tests/integration/</code>).</li> </ul>"},{"location":"development/testing/#tests-ausfuhren","title":"Tests ausf\u00fchren","text":"<pre><code># Alle Tests ausf\u00fchren\npytest\n\n# Nur Unit-Tests\npytest tests/unit\n\n# Mit Coverage-Bericht\npytest --cov=robot_workspace\n</code></pre>"},{"location":"development/testing/#api-dokumentationsprufung","title":"API-Dokumentationspr\u00fcfung","text":"<p>Wir verwenden <code>interrogate</code>, um sicherzustellen, dass alle \u00f6ffentlichen APIs dokumentiert sind.</p> <pre><code>interrogate robot_workspace\n</code></pre>"},{"location":"usage/examples/","title":"Beispiele","text":"<p>Hier finden Sie praktische Beispiele f\u00fcr die Verwendung des <code>robot_workspace</code>-Pakets.</p>"},{"location":"usage/examples/#kompletter-pick-and-place-workflow","title":"Kompletter Pick-and-Place Workflow","text":"<pre><code>from robot_workspace import Objects, Object, NiryoWorkspaces\n\n# 1. Workspaces initialisieren\nworkspaces = NiryoWorkspaces(env)\nws = workspaces.get_home_workspace()\n\n# 2. Erkannte Objekte hinzuf\u00fcgen\ndetected = [\n    {\"label\": \"W\u00fcrfel\", \"bbox\": [100, 100, 150, 150]},\n    {\"label\": \"Zylinder\", \"bbox\": [300, 200, 350, 250]}\n]\n\nobjs = Objects()\nfor item in detected:\n    objs.append(Object(\n        item[\"label\"],\n        item[\"bbox\"][0], item[\"bbox\"][1],\n        item[\"bbox\"][2], item[\"bbox\"][3],\n        None, ws\n    ))\n\n# 3. Zielobjekt finden\ntarget = objs.get_largest_detected_object()[0]\n\n# 4. Greifpose abrufen\npick_pose = target.pose_com()\n</code></pre>"},{"location":"usage/examples/#serialisierung","title":"Serialisierung","text":"<pre><code># In JSON konvertieren\njson_data = target.to_json()\n\n# Aus Dictionary wiederherstellen\nnew_obj = Object.from_dict(data_dict, ws)\n</code></pre>"},{"location":"en/","title":"Robot Workspace","text":"<p>A Python framework that bridges the gap between camera images and physical robot manipulation. It provides the essential data structures and coordinate transformations needed to convert detected objects from vision systems into actionable pick-and-place targets for robotic arms. The framework handles workspace calibration, object representation with physical properties, and spatial reasoning\u2014enabling vision-equipped robots to understand \"where\" objects are and \"how\" to grasp them in real-world coordinates.</p>"},{"location":"en/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>robot_workspace</code> package provides a complete framework for managing robotic workspaces, including:</p> <ul> <li>\ud83c\udfaf Coordinate Transformations: Seamlessly transform between camera and world coordinate frames</li> <li>\ud83d\udce6 Object Representation: Rich object models with position, dimensions, segmentation masks, and orientation</li> <li>\ud83d\uddfa\ufe0f Workspace Management: Define and manage multiple workspaces with different configurations</li> <li>\ud83d\udd0d Spatial Queries: Find objects by location, size, proximity, or custom criteria</li> <li>\ud83d\udcbe Serialization: JSON-based serialization for data persistence and communication</li> <li>\ud83e\udd16 Robot Support: Native support for Niryo Ned2 and WidowX 250 6DOF robots (real and simulation, extensible to other platforms)</li> </ul>"},{"location":"en/#key-features","title":"\u2728 Key Features","text":""},{"location":"en/#vision-detection","title":"Vision &amp; Detection","text":"<ul> <li>Integrate object detection with bounding boxes, segmentation masks, and physical properties</li> <li>Calculate center of mass and optimal gripper orientations</li> <li>Support for multi-object tracking and management</li> </ul>"},{"location":"en/#coordinate-systems","title":"Coordinate Systems","text":"<ul> <li>Transform between relative image coordinates (0-1) and world coordinates (meters)</li> <li>Handle multiple workspace configurations with different camera poses</li> <li>Automatic workspace boundary detection</li> </ul>"},{"location":"en/#spatial-reasoning","title":"Spatial Reasoning","text":"<ul> <li>Query objects by spatial relationships (left/right/above/below/close to)</li> <li>Find nearest objects to specified coordinates</li> <li>Filter by size, label, or custom criteria</li> </ul>"},{"location":"en/#llm-integration","title":"LLM Integration","text":"<ul> <li>Generate natural language descriptions of objects and scenes</li> <li>Structured output formats for AI agent integration</li> <li>Easy-to-parse object properties</li> </ul>"},{"location":"en/#quality-assurance","title":"Quality Assurance","text":"<ul> <li> <p>90% test coverage with comprehensive unit and integration tests</p> </li> <li>Full type annotations for better IDE support</li> <li>Extensive documentation and examples</li> </ul>"},{"location":"en/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"en/#core-components","title":"Core Components","text":"<pre><code>robot_workspace/\n\u251c\u2500\u2500 objects/                   # Object detection and representation\n\u2502   \u251c\u2500\u2500 object.py              # Single object with properties and methods\n\u2502   \u251c\u2500\u2500 objects.py             # Collection of objects with spatial queries\n\u2502   \u251c\u2500\u2500 object_api.py          # API interface for objects\n\u2502   \u2514\u2500\u2500 pose_object.py         # 6-DOF pose representation (x, y, z, roll, pitch, yaw)\n\u251c\u2500\u2500 workspaces/                # Workspace definitions and management\n\u2502   \u251c\u2500\u2500 workspace.py           # Abstract workspace base class\n\u2502   \u251c\u2500\u2500 workspaces.py          # Collection of workspaces\n\u2502   \u251c\u2500\u2500 niryo_workspace.py     # Niryo Ned2 workspace implementation\n\u2502   \u251c\u2500\u2500 niryo_workspaces.py    # Niryo workspace collection\n\u2502   \u251c\u2500\u2500 widowx_workspace.py    # WidowX 250 6DOF implementation\n\u2502   \u2514\u2500\u2500 widowx_workspaces.py   # WidowX workspace collection\n\u2514\u2500\u2500 common/                    # Utilities\n    \u2514\u2500\u2500 logger.py              # Logging decorators\n</code></pre>"},{"location":"en/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"en/#coordinate-systems_1","title":"Coordinate Systems","text":"<p>The package handles three coordinate systems:</p> <ol> <li>Image Coordinates (Pixels): Raw camera pixel coordinates</li> <li>Relative Coordinates (0-1): Normalized workspace-independent coordinates</li> <li>World Coordinates (Meters): Robot base frame coordinates</li> </ol> <pre><code>Image (u, v) \u2192 Relative (u_rel, v_rel) \u2192 World (x, y, z) + Orientation\n</code></pre> <p>For detailed information, see Architecture Documentation.</p>"},{"location":"en/#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"en/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip package manager</li> </ul>"},{"location":"en/#basic-installation","title":"Basic Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/dgaida/robot_workspace.git\ncd robot_workspace\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"en/#with-robot-support","title":"With Robot Support","text":"<pre><code># Niryo Ned2 support\npip install -e \".[niryo]\"\n\n# All features\npip install -e \".[all]\"\n</code></pre>"},{"location":"en/#development-installation","title":"Development Installation","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs additional development tools: - pytest and pytest-cov for testing - black for code formatting - ruff for linting - mypy for type checking - pre-commit hooks</p>"},{"location":"en/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"en/#core-concepts","title":"Core Concepts","text":"<p>The repository provides three main capabilities:</p> <ol> <li>Pose Representation - 6-DOF poses (position + orientation) for objects and robot targets</li> <li>Object Management - Detected objects with physical properties, positions, and spatial queries</li> <li>Coordinate Transformation - Convert camera coordinates to robot world coordinates</li> </ol>"},{"location":"en/#essential-usage","title":"Essential Usage","text":"<pre><code>from robot_workspace import PoseObjectPNP, Object, Objects, NiryoWorkspaces\nfrom unittest.mock import Mock\n\n# 1. Working with Poses (position + orientation)\npose = PoseObjectPNP(x=0.2, y=0.1, z=0.05, roll=0.0, pitch=1.57, yaw=0.0)\nprint(f\"Position: [{pose.x}, {pose.y}, {pose.z}]\")\n\n# Add offsets to create new poses\npick_pose = pose.copy_with_offsets(z_offset=-0.02)  # Lower gripper 2cm\n\n# 2. Representing Detected Objects\n# Objects are created from vision system detections with bounding boxes\nobj = Object(\n    label=\"pencil\",\n    u_min=100, v_min=100, u_max=200, v_max=200,  # Bounding box in pixels\n    mask_8u=None,  # Optional segmentation mask\n    workspace=workspace  # Links to workspace for coordinate transforms\n)\n\n# Access object properties in physical units\nprint(f\"Object '{obj.label()}' at [{obj.x_com():.2f}, {obj.y_com():.2f}] meters\")\nprint(f\"Size: {obj.width_m():.3f}m \u00d7 {obj.height_m():.3f}m = {obj.size_m2()*10000:.1f} cm\u00b2\")\nprint(f\"Optimal grasp angle: {obj.gripper_rotation():.2f} radians\")\n\n# 3. Spatial Queries with Object Collections\nobjects = Objects([obj1, obj2, obj3])\n\n# Find objects by spatial relationships\nfrom robot_workspace import Location\nleft_objects = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.2, 0.0]\n)\n\n# Find nearest object to a coordinate\nnearest, distance = objects.get_nearest_detected_object([0.25, 0.05])\nprint(f\"Nearest: {nearest.label()} at {distance*100:.1f}cm away\")\n\n# Size-based queries\nlargest, size = objects.get_largest_detected_object()\nsorted_by_size = objects.get_detected_objects_sorted(ascending=True)\n\n# 4. Coordinate Transformation (requires robot environment)\n# Transform camera image coordinates to robot world coordinates\nworld_pose = workspace.transform_camera2world_coords(\n    workspace_id=\"niryo_ws\",\n    u_rel=0.5,  # Center of image (normalized 0-1)\n    v_rel=0.5,\n    yaw=0.0     # Object orientation\n)\nprint(f\"Image center \u2192 World: [{world_pose.x:.2f}, {world_pose.y:.2f}, {world_pose.z:.2f}]\")\n</code></pre>"},{"location":"en/#integration-with-vision-system","title":"Integration with Vision System","text":"<p>This framework integrates seamlessly with the vision detection pipeline: <pre><code># Typical workflow:\n# 1. vision_detect_segment detects objects in camera image\n# 2. robot_workspace converts detections to Object instances with world coordinates\n# 3. robot_environment uses Objects for pick-and-place planning\n# 4. robot_mcp enables natural language control: \"pick up the pencil\"\n\n# The Object class bridges vision and manipulation:\ndetected_objects = cortex.get_detected_objects()  # From vision system\nobjects = Objects([\n    Object(..., workspace=workspace) for detection in detected_objects\n])\nrobot.pick_object(objects[0].label(), objects[0].coordinate())\n</code></pre></p>"},{"location":"en/#examples","title":"\ud83d\udcda Examples","text":""},{"location":"en/#running-the-demo","title":"Running the Demo","text":"<p>The package includes a comprehensive demonstration script that uses mocked components:</p> <pre><code>python main.py\n</code></pre> <p>This demonstrates: - Pose object creation and manipulation - Workspace management (with mock environment) - Object creation and properties - Spatial queries and filtering - Serialization and deserialization - LLM-friendly formatting</p> <p>No robot hardware required for the demo!</p>"},{"location":"en/#more-examples","title":"More Examples","text":"<p>See examples.md for detailed usage examples including: - Object detection workflows - Multi-workspace management - Serialization patterns - Integration with robot controllers</p>"},{"location":"en/#documentation","title":"\ud83d\udcd6 Documentation","text":""},{"location":"en/#api-reference","title":"API Reference","text":"<ul> <li>API Documentation - Complete API reference</li> <li>Architecture Documentation - System design and patterns</li> <li>Examples - Usage examples and recipes</li> </ul>"},{"location":"en/#key-classes","title":"Key Classes","text":"<ul> <li>PoseObjectPNP: 6-DOF pose with position and orientation</li> <li>Object: Detected object with physical properties</li> <li>Objects: Collection with spatial query capabilities</li> <li>Workspace: Abstract workspace base class</li> <li>NiryoWorkspace: Niryo Ned2 implementation</li> <li>WidowXWorkspace: WidowX 250 6DOF implementation</li> </ul>"},{"location":"en/#testing","title":"\ud83e\uddea Testing","text":"<p>See docs/TESTING.md and tests/README.md for detailed testing documentation.</p>"},{"location":"en/#adding-robot-support","title":"\ud83d\udd27 Adding Robot Support","text":"<p>See docs/adding_robot_support.md for detailed guidelines.</p>"},{"location":"en/#development","title":"\ud83d\udcbb Development","text":""},{"location":"en/#code-quality-tools","title":"Code Quality Tools","text":"<p>This project uses: - Black for code formatting (line length: 127) - Ruff for fast Python linting - mypy for type checking - pre-commit hooks for automated checks</p>"},{"location":"en/#setup-pre-commit-hooks","title":"Setup Pre-commit Hooks","text":"<pre><code>pre-commit install\n</code></pre>"},{"location":"en/#manual-code-quality-checks","title":"Manual Code Quality Checks","text":"<pre><code># Format code\nblack .\n\n# Lint code\nruff check .\n\n# Type check\nmypy robot_workspace --ignore-missing-imports\n</code></pre>"},{"location":"en/#project-structure","title":"Project Structure","text":"<pre><code>robot_workspace/\n\u251c\u2500\u2500 .github/workflows/      # CI/CD workflows\n\u251c\u2500\u2500 docs/                   # Documentation\n\u251c\u2500\u2500 robot_workspace/        # Source code\n\u251c\u2500\u2500 tests/                  # Test suite\n\u251c\u2500\u2500 main.py                 # Demo script\n\u251c\u2500\u2500 pyproject.toml         # Project configuration\n\u251c\u2500\u2500 requirements.txt       # Dependencies\n\u2514\u2500\u2500 README.md              # This file\n</code></pre>"},{"location":"en/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>See CONTRIBUTING.md for detailed guidelines.</p>"},{"location":"en/#license","title":"\ud83d\udcc4 License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"en/#citation","title":"\ud83d\udcdd Citation","text":"<p>If you use this package in your research, please cite:</p> <pre><code>@software{robot_workspace,\n  author = {Gaida, Daniel},\n  title = {Robot Workspace: A Framework for Robotic Workspace Management},\n  year = {2025},\n  url = {https://github.com/dgaida/robot_workspace}\n}\n</code></pre>"},{"location":"en/#support","title":"\ud83d\udcde Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Documentation: See docs/ directory</li> <li>Examples: Run <code>python main.py</code> for demonstrations</li> <li>Email: daniel.gaida@th-koeln.de</li> </ul>"},{"location":"en/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>Built for the Niryo Ned2 and WidowX 250 6DOF robotic platforms</li> <li>Designed for integration with computer vision systems</li> <li>Supports both real robots and Gazebo simulation</li> <li>Mock environment enables hardware-free development and testing</li> </ul>"},{"location":"en/#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<ul> <li>[ ] Additional robot platform support</li> <li>[ ] Enhanced multi-workspace coordination</li> <li>Automatic object handoff between workspaces</li> <li>Synchronized multi-workspace scanning</li> <li>Cross-workspace object tracking and state management</li> <li>Collision-free multi-arm coordination</li> <li>Shared memory pools for collaborative tasks</li> <li>Priority-based workspace arbitration</li> <li>[ ] Integration with popular ML frameworks</li> <li>[ ] ROS2 compatibility layer</li> <li>[ ] Web-based visualization tools</li> </ul>"},{"location":"en/#related-projects","title":"\ud83d\udd17 Related Projects","text":"<p>This package is part of a larger ecosystem for robotic manipulation and AI-driven control:</p> <ul> <li>robot_environment - Complete robot control framework for pick-and-place operations with Niryo Ned2 and WidowX 250 6DOF robots</li> <li>vision_detect_segment - Real-time object detection and segmentation system with YOLO integration</li> <li>robot_mcp - Model Context Protocol (MCP) server enabling LLM-based natural language control of robotic systems</li> </ul> <p>Author: Daniel Gaida Email: daniel.gaida@th-koeln.de Repository: https://github.com/dgaida/robot_workspace Version: 0.1.0</p>"},{"location":"en/getting-started/","title":"Quick Reference Guide","text":"<p>Essential commands and patterns for the Robot Workspace package.</p>"},{"location":"en/getting-started/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Core Imports</li> <li>Pose Objects</li> <li>Workspaces</li> <li>Objects</li> <li>Spatial Queries</li> <li>Serialization</li> <li>Common Patterns</li> <li>Testing</li> <li>Troubleshooting</li> </ul>"},{"location":"en/getting-started/#installation","title":"Installation","text":"<pre><code># Basic installation\npip install -e .\n\n# With Niryo support\npip install -e \".[niryo]\"\n\n# Development installation\npip install -e \".[dev]\"\n\n# All features\npip install -e \".[all]\"\n</code></pre>"},{"location":"en/getting-started/#core-imports","title":"Core Imports","text":"<pre><code># Essential classes\nfrom robot_workspace import (\n    PoseObjectPNP,           # 6-DOF pose\n    Object,                  # Detected object\n    Objects,                 # Object collection\n    Location,                # Spatial relationships\n    Workspace,               # Abstract workspace\n    Workspaces,              # Workspace collection\n    NiryoWorkspace,          # Niryo implementation\n    NiryoWorkspaces,         # Niryo collection\n    WidowXWorkspace,         # WidowX implementation\n    WidowXWorkspaces,        # WidowX collection\n    ConfigManager,           # Configuration management\n)\n\n# For mocking (testing/demos)\nfrom unittest.mock import Mock\n</code></pre>"},{"location":"en/getting-started/#pose-objects","title":"Pose Objects","text":""},{"location":"en/getting-started/#create-a-pose","title":"Create a Pose","text":"<pre><code># Create 6-DOF pose\npose = PoseObjectPNP(\n    x=0.2,      # meters\n    y=0.1,      # meters\n    z=0.3,      # meters\n    roll=0.0,   # radians\n    pitch=1.57, # radians (~90\u00b0)\n    yaw=0.0     # radians\n)\n</code></pre>"},{"location":"en/getting-started/#pose-operations","title":"Pose Operations","text":"<pre><code># Arithmetic\nnew_pose = pose1 + pose2\ndifference = pose1 - pose2\n\n# Copy with offsets\noffset_pose = pose.copy_with_offsets(\n    x_offset=0.05,\n    yaw_offset=0.5\n)\n\n# Convert to list\npose_list = pose.to_list()  # [x, y, z, roll, pitch, yaw]\n\n# Get transformation matrix\nmatrix = pose.to_transformation_matrix()  # 4x4 numpy array\n\n# Get quaternion\nquat = pose.quaternion  # [qx, qy, qz, qw]\n\n# Check equality\nis_equal = pose1 == pose2\nis_close = pose1.approx_eq(pose2, eps_position=0.1, eps_orientation=0.1)\n</code></pre>"},{"location":"en/getting-started/#workspaces","title":"Workspaces","text":""},{"location":"en/getting-started/#initialize-with-mock-environment","title":"Initialize with Mock Environment","text":"<pre><code>from unittest.mock import Mock\n\n# Create mock environment\ndef create_mock_env():\n    env = Mock()\n    env.use_simulation.return_value = True\n\n    def mock_transform(ws_id, u_rel, v_rel, yaw):\n        x = 0.4 - u_rel * 0.3\n        y = 0.15 - v_rel * 0.3\n        return PoseObjectPNP(x, y, 0.05, 0.0, 1.57, yaw)\n\n    env.get_robot_target_pose_from_rel = mock_transform\n    return env\n\n# Create workspaces\nenv = create_mock_env()\nworkspaces = NiryoWorkspaces(env, verbose=False)\n</code></pre>"},{"location":"en/getting-started/#initialize-with-configuration","title":"Initialize with Configuration","text":"<pre><code>from robot_workspace import NiryoWorkspaces, ConfigManager\n\n# Using configuration file\nworkspaces = NiryoWorkspaces(\n    environment,\n    config_path='config/niryo_config.yaml'\n)\n\n# Or manually with ConfigManager\nconfig_mgr = ConfigManager()\nconfig_mgr.load_from_yaml('config/niryo_config.yaml')\nws_config = config_mgr.get_workspace_config('niryo_ws')\nworkspace = NiryoWorkspace.from_config(ws_config, environment)\n</code></pre>"},{"location":"en/getting-started/#access-workspaces","title":"Access Workspaces","text":"<pre><code># Get home workspace\nhome = workspaces.get_home_workspace()\n\n# Get by ID\nws = workspaces.get_workspace_by_id(\"niryo_ws\")\n\n# Get by index\nws = workspaces.get_workspace(0)\n\n# Get all IDs\nids = workspaces.get_workspace_ids()\n\n# Get workspace properties\nwidth = workspace.width_m()\nheight = workspace.height_m()\ncenter = workspace.xy_center_wc()\nobs_pose = workspace.observation_pose()\n</code></pre>"},{"location":"en/getting-started/#coordinate-transformation","title":"Coordinate Transformation","text":"<pre><code># Transform relative image coords to world coords\nworld_pose = workspace.transform_camera2world_coords(\n    workspace_id=\"niryo_ws\",\n    u_rel=0.5,  # [0-1] horizontal\n    v_rel=0.5,  # [0-1] vertical\n    yaw=0.0     # object orientation\n)\n</code></pre>"},{"location":"en/getting-started/#objects","title":"Objects","text":""},{"location":"en/getting-started/#create-an-object","title":"Create an Object","text":"<pre><code># Without mask\nobj = Object(\n    label=\"pencil\",\n    u_min=100, v_min=100,  # Bounding box\n    u_max=200, v_max=200,\n    mask_8u=None,\n    workspace=workspace\n)\n\n# With segmentation mask\nimport numpy as np\nmask = np.zeros((640, 480), dtype=np.uint8)\nmask[100:200, 100:200] = 255\n\nobj_with_mask = Object(\n    label=\"cube\",\n    u_min=100, v_min=100,\n    u_max=200, v_max=200,\n    mask_8u=mask,\n    workspace=workspace\n)\n</code></pre>"},{"location":"en/getting-started/#access-object-properties","title":"Access Object Properties","text":"<pre><code># Basic properties\nlabel = obj.label()\nposition = obj.coordinate()  # [x, y]\nx, y = obj.x_com(), obj.y_com()\nfull_pose = obj.pose_com()\n\n# Dimensions\nwidth = obj.width_m()\nheight = obj.height_m()\narea = obj.size_m2()\n\n# Orientation\nrotation = obj.gripper_rotation()  # radians\n</code></pre>"},{"location":"en/getting-started/#update-object-position","title":"Update Object Position","text":"<pre><code># Update XY position\nobj.set_position([0.3, 0.05])\n\n# Update full pose (including rotation)\nnew_pose = obj.pose_com().copy_with_offsets(\n    x_offset=0.1,\n    yaw_offset=0.5\n)\nobj.set_pose_com(new_pose)\n</code></pre>"},{"location":"en/getting-started/#spatial-queries","title":"Spatial Queries","text":""},{"location":"en/getting-started/#create-collection","title":"Create Collection","text":"<pre><code>objects = Objects([obj1, obj2, obj3])\n</code></pre>"},{"location":"en/getting-started/#location-based-queries","title":"Location-Based Queries","text":"<pre><code># Find objects by location\nleft_objects = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.25, 0.0]\n)\n\nright_objects = objects.get_detected_objects(\n    location=Location.RIGHT_NEXT_TO,\n    coordinate=[0.25, 0.0]\n)\n\nabove_objects = objects.get_detected_objects(\n    location=Location.ABOVE,\n    coordinate=[0.25, 0.0]\n)\n\nbelow_objects = objects.get_detected_objects(\n    location=Location.BELOW,\n    coordinate=[0.25, 0.0]\n)\n\n# Within 2cm radius\nnearby = objects.get_detected_objects(\n    location=Location.CLOSE_TO,\n    coordinate=[0.25, 0.0]\n)\n</code></pre>"},{"location":"en/getting-started/#label-based-queries","title":"Label-Based Queries","text":"<pre><code># Find by label\npens = objects.get_detected_objects(label=\"pen\")\n\n# Combine location and label\nleft_pens = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.25, 0.0],\n    label=\"pen\"\n)\n\n# Get specific object\nobj = objects.get_detected_object(\n    coordinate=[0.2, 0.1],\n    label=\"pencil\"\n)\n</code></pre>"},{"location":"en/getting-started/#distance-and-size-queries","title":"Distance and Size Queries","text":"<pre><code># Find nearest\nnearest, distance = objects.get_nearest_detected_object([0.2, 0.1])\n\n# Find largest/smallest\nlargest, size = objects.get_largest_detected_object()\nsmallest, size = objects.get_smallest_detected_object()\n\n# Sort by size\nsorted_asc = objects.get_detected_objects_sorted(ascending=True)\nsorted_desc = objects.get_detected_objects_sorted(ascending=False)\n</code></pre>"},{"location":"en/getting-started/#serialization","title":"Serialization","text":""},{"location":"en/getting-started/#object-serialization","title":"Object Serialization","text":"<pre><code># To dictionary\nobj_dict = obj.to_dict()\n\n# To JSON string\njson_str = obj.to_json()\n\n# From dictionary\nreconstructed = Object.from_dict(obj_dict, workspace)\n\n# From JSON string\nreconstructed = Object.from_json(json_str, workspace)\n</code></pre>"},{"location":"en/getting-started/#collection-serialization","title":"Collection Serialization","text":"<pre><code># To list of dicts\ndict_list = Objects.objects_to_dict_list(objects)\n\n# From list of dicts\nreconstructed = Objects.dict_list_to_objects(dict_list, workspace)\n\n# Get serializable results directly\nobj_dict = objects.get_detected_object(\n    [0.2, 0.1],\n    serializable=True\n)\n\nsorted_dicts = objects.get_detected_objects_sorted(\n    serializable=True\n)\n</code></pre>"},{"location":"en/getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"en/getting-started/#pick-and-place-workflow","title":"Pick and Place Workflow","text":"<pre><code># 1. Initialize\nworkspaces = NiryoWorkspaces(environment)\nworkspace = workspaces.get_home_workspace()\n\n# 2. Detect objects\ndetected = Objects([obj1, obj2, obj3])\n\n# 3. Find target\ntarget = detected.get_detected_object(\n    coordinate=[0.2, 0.1],\n    label=\"cube\"\n)\n\n# 4. Get pickup info\npickup_pose = target.pose_com()\nrotation = target.gripper_rotation()\n\n# 5. Execute pick (using robot API)\n# robot.pick(pickup_pose, rotation)\n\n# 6. Define place location\nplace_pose = PoseObjectPNP(0.3, -0.1, 0.05, 0.0, 1.57, 0.0)\n\n# 7. Execute place\n# robot.place(place_pose)\n\n# 8. Update object in memory\ntarget.set_pose_com(place_pose)\n</code></pre>"},{"location":"en/getting-started/#sorting-by-size","title":"Sorting by Size","text":"<pre><code># Get sorted objects\nsorted_objs = objects.get_detected_objects_sorted(ascending=True)\n\n# Place in order\nfor i, obj in enumerate(sorted_objs):\n    place_pose = PoseObjectPNP(\n        x=0.3,\n        y=-0.1 + i * 0.05,  # 5cm spacing\n        z=0.05,\n        roll=0.0,\n        pitch=1.57,\n        yaw=obj.gripper_rotation()\n    )\n    # robot.pick_and_place(obj.pose_com(), place_pose)\n    obj.set_pose_com(place_pose)\n</code></pre>"},{"location":"en/getting-started/#multi-workspace-transfer","title":"Multi-Workspace Transfer","text":"<pre><code># Get workspaces\nleft_ws = workspaces.get_workspace_left()\nright_ws = workspaces.get_workspace_right()\n\n# Get objects in each\nleft_objects = Objects([...])\nright_objects = Objects([...])\n\n# Transfer object\ntransfer_obj = left_objects.get_detected_object([0.2, 0.1])\nif transfer_obj:\n    left_objects.remove(transfer_obj)\n\n    # New position in right workspace\n    new_pose = right_ws.transform_camera2world_coords(\n        right_ws.id(), 0.5, 0.5, 0.0\n    )\n    transfer_obj.set_pose_com(new_pose)\n    right_objects.append(transfer_obj)\n</code></pre>"},{"location":"en/getting-started/#llm-integration","title":"LLM Integration","text":"<pre><code># Format for LLM\ndescription = obj.as_string_for_llm()\n# Output: \"- 'pencil' at world coordinates [0.20, 0.10] with...\"\n\n# Format for chat\nchat_msg = obj.as_string_for_chat_window()\n# Output: \"Detected a new object: pencil at...\"\n\n# Get all objects as text\nfor obj in objects:\n    print(obj.as_string_for_llm())\n</code></pre>"},{"location":"en/getting-started/#testing","title":"Testing","text":""},{"location":"en/getting-started/#run-tests","title":"Run Tests","text":"<pre><code># All tests\npytest\n\n# With coverage\npytest --cov=robot_workspace --cov-report=term\n\n# Specific test file\npytest tests/objects/test_object.py\n\n# Specific test\npytest tests/objects/test_object.py::test_object_initialization\n\n# Verbose output\npytest -v\n\n# Stop on first failure\npytest -x\n</code></pre>"},{"location":"en/getting-started/#run-demo","title":"Run Demo","text":"<pre><code># Run main demo (no hardware needed)\npython main.py\n</code></pre>"},{"location":"en/getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"en/getting-started/#common-issues","title":"Common Issues","text":"<pre><code># Import error\n# Solution: Install in editable mode\npip install -e .\n\n# Missing workspace image shape\ntry:\n    obj = Object(...)\nexcept ValueError as e:\n    print(\"Set workspace image shape first\")\n    workspace.set_img_shape((640, 480, 3))\n\n# Object not found\nobj = objects.get_detected_object([0.2, 0.1])\nif obj is None:\n    print(\"No object at that location\")\n\n# Workspace not visible\nvisible_ws = workspaces.get_visible_workspace(camera_pose)\nif visible_ws is None:\n    print(\"No workspace visible from camera\")\n</code></pre>"},{"location":"en/getting-started/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable verbose output\nworkspaces = NiryoWorkspaces(environment, verbose=True)\nworkspace = NiryoWorkspace(\"niryo_ws\", environment, verbose=True)\nobj = Object(..., verbose=True)\nobjects = Objects([...], verbose=True)\n</code></pre>"},{"location":"en/getting-started/#coordinate-systems-reference","title":"Coordinate Systems Reference","text":"<pre><code>Image Coordinates (pixels)\n    \u2193 normalize by image dimensions\nRelative Coordinates (0-1)\n    \u2193 transform_camera2world_coords()\nWorld Coordinates (meters)\n</code></pre>"},{"location":"en/getting-started/#niryo-coordinate-system","title":"Niryo Coordinate System","text":"<pre><code>Y (left/right)\n\u2191\n\u2502    Workspace\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   \u2502         \u2502\n\u2502   \u2502    \u2022    \u2502  \u2190 Center\n\u2502   \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 X (forward/back)\n\nZ (up/down) \u2299 (out of page)\n</code></pre>"},{"location":"en/getting-started/#location-filters","title":"Location Filters","text":"<ul> <li><code>LEFT_NEXT_TO</code>: y &gt; coordinate[1]</li> <li><code>RIGHT_NEXT_TO</code>: y &lt; coordinate[1]</li> <li><code>ABOVE</code>: x &gt; coordinate[0]</li> <li><code>BELOW</code>: x &lt; coordinate[0]</li> <li><code>CLOSE_TO</code>: within 2cm radius</li> </ul>"},{"location":"en/getting-started/#quick-command-reference","title":"Quick Command Reference","text":"Task Command Install package <code>pip install -e .</code> Run tests <code>pytest</code> Run demo <code>python main.py</code> Format code <code>black .</code> Lint code <code>ruff check .</code> Type check <code>mypy robot_workspace</code> Generate coverage <code>pytest --cov --cov-report=html</code>"},{"location":"en/getting-started/#file-structure","title":"File Structure","text":"<pre><code>robot_workspace/\n\u251c\u2500\u2500 objects/              # Object representation\n\u2502   \u251c\u2500\u2500 pose_object.py   # 6-DOF pose\n\u2502   \u251c\u2500\u2500 object.py        # Single object\n\u2502   \u251c\u2500\u2500 objects.py       # Collection\n\u2502   \u2514\u2500\u2500 object_api.py    # API interface\n\u251c\u2500\u2500 workspaces/          # Workspace management\n\u2502   \u251c\u2500\u2500 workspace.py     # Abstract base\n\u2502   \u251c\u2500\u2500 workspaces.py    # Collection\n\u2502   \u251c\u2500\u2500 niryo_*.py       # Niryo implementation\n\u2502   \u2514\u2500\u2500 widowx_*.py      # WidowX implementation\n\u251c\u2500\u2500 config.py            # Configuration\n\u2514\u2500\u2500 common/              # Utilities\n    \u2514\u2500\u2500 logger.py        # Logging\n</code></pre>"},{"location":"en/getting-started/#links","title":"Links","text":"<ul> <li>Documentation: docs/README.md</li> <li>API Reference: docs/api.md</li> <li>Examples: docs/examples.md</li> <li>Installation: docs/INSTALL.md</li> <li>Testing: docs/TESTING.md</li> <li>Contributing: CONTRIBUTING.md</li> </ul>"},{"location":"en/getting-started/#support","title":"Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Email: daniel.gaida@th-koeln.de</li> <li>Repository: https://github.com/dgaida/robot_workspace</li> </ul> <p>Version: 0.1.0 Author: Daniel Gaida Last Updated: December 2024</p>"},{"location":"en/installation/","title":"Installation Guide","text":"<p>Complete installation instructions for the Robot Workspace package.</p>"},{"location":"en/installation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Installation Methods</li> <li>Standard Installation</li> <li>Development Installation</li> <li>Robot-Specific Installation</li> <li>Verification</li> <li>Platform-Specific Notes</li> <li>Troubleshooting</li> </ul>"},{"location":"en/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"en/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.9 or higher (3.11 recommended)</li> <li>Operating System: Linux, Windows, or macOS</li> <li>Memory: Minimum 2GB RAM</li> <li>Disk Space: ~500MB for package and dependencies</li> </ul>"},{"location":"en/installation/#required-system-packages","title":"Required System Packages","text":""},{"location":"en/installation/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y python3-pip python3-dev build-essential\nsudo apt-get install -y libopencv-dev python3-opencv\n</code></pre>"},{"location":"en/installation/#macos","title":"macOS","text":"<pre><code>brew install python@3.11\nbrew install opencv\n</code></pre>"},{"location":"en/installation/#windows","title":"Windows","text":"<ul> <li>Install Python from python.org</li> <li>Visual Studio Build Tools may be required for some dependencies</li> </ul>"},{"location":"en/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"en/installation/#standard-installation","title":"Standard Installation","text":"<p>For general use without robot hardware:</p> <pre><code># Clone the repository\ngit clone https://github.com/dgaida/robot_workspace.git\ncd robot_workspace\n\n# Install in standard mode\npip install -e .\n</code></pre> <p>This installs: - Core workspace management - Object detection and representation - Coordinate transformations - Basic utilities</p>"},{"location":"en/installation/#development-installation","title":"Development Installation","text":"<p>For contributors and developers:</p> <pre><code># Clone the repository\ngit clone https://github.com/dgaida/robot_workspace.git\ncd robot_workspace\n\n# Install with development dependencies\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n</code></pre> <p>This includes: - All standard features - Testing tools (pytest, pytest-cov) - Code formatting (black, ruff) - Type checking (mypy) - Pre-commit hooks</p>"},{"location":"en/installation/#robot-specific-installation","title":"Robot-Specific Installation","text":""},{"location":"en/installation/#for-niryo-ned2-robot","title":"For Niryo Ned2 Robot","text":"<pre><code># Install with Niryo support\npip install -e \".[niryo]\"\n</code></pre> <p>Additional requirements: - Access to Niryo Ned2 robot (real or simulated) - Network connectivity to robot - PyNiryo library (installed automatically)</p>"},{"location":"en/installation/#for-widowx-robot","title":"For WidowX Robot","text":"<pre><code># Install standard version (WidowX support is built-in)\npip install -e .\n</code></pre> <p>Additional requirements: - Access to WidowX 250 6DOF robot - ROS installation (if using ROS interface) - Third-person camera setup (e.g., Intel RealSense)</p>"},{"location":"en/installation/#complete-installation-all-features","title":"Complete Installation (All Features)","text":"<pre><code># Install everything\npip install -e \".[all]\"\n</code></pre>"},{"location":"en/installation/#verification","title":"Verification","text":""},{"location":"en/installation/#quick-test","title":"Quick Test","text":"<pre><code># Run the demo script\npython main.py\n</code></pre> <p>Expected output: - Pose object demonstrations - Workspace management examples - Object creation and manipulation - Serialization examples - LLM formatting demonstrations</p>"},{"location":"en/installation/#run-test-suite","title":"Run Test Suite","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage report\npytest --cov=robot_workspace --cov-report=term-missing\n\n# Run specific test categories\npytest tests/objects/      # Object tests only\npytest tests/workspaces/   # Workspace tests only\n</code></pre>"},{"location":"en/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># In Python interpreter\nimport robot_workspace\nprint(robot_workspace.__version__)  # Should print version number\n\n# Test basic functionality\nfrom robot_workspace import PoseObjectPNP, Location\npose = PoseObjectPNP(0.2, 0.1, 0.3, 0.0, 1.57, 0.0)\nprint(pose)  # Should display pose information\n</code></pre>"},{"location":"en/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"en/installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<p>Recommended for production use. Best compatibility with robot hardware.</p> <p>Additional setup for camera access: <pre><code># Add user to video group\nsudo usermod -a -G video $USER\n# Logout and login for changes to take effect\n</code></pre></p>"},{"location":"en/installation/#macos_1","title":"macOS","text":"<p>Fully supported for development and testing.</p> <p>Known issues: - Some USB camera drivers may require additional setup</p>"},{"location":"en/installation/#windows_1","title":"Windows","text":"<p>Supported with some limitations.</p> <p>Important notes: - Use Command Prompt or PowerShell (not Git Bash for installation) - May need Visual Studio Build Tools for OpenCV compilation</p> <p>Visual Studio Build Tools: Download from Microsoft and install \"Desktop development with C++\"</p>"},{"location":"en/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":""},{"location":"en/installation/#using-venv","title":"Using venv","text":"<pre><code># Create virtual environment\npython -m venv robot_env\n\n# Activate (Linux/macOS)\nsource robot_env/bin/activate\n\n# Activate (Windows)\nrobot_env\\Scripts\\activate\n\n# Install package\npip install -e .\n</code></pre>"},{"location":"en/installation/#using-conda","title":"Using conda","text":"<pre><code># Create conda environment\nconda create -n robot_workspace python=3.11\n\n# Activate environment\nconda activate robot_workspace\n\n# Install package\npip install -e .\n</code></pre>"},{"location":"en/installation/#using-the-provided-environmentyaml","title":"Using the provided environment.yaml","text":"<pre><code># Create environment from file\nconda env create -f environment.yaml\n\n# Activate environment\nconda activate llm_niryo_cuda12\n\n# Install package\npip install -e .\n</code></pre>"},{"location":"en/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"en/installation/#common-issues","title":"Common Issues","text":""},{"location":"en/installation/#importerror-no-module-named-cv2","title":"ImportError: No module named 'cv2'","text":"<p>Solution: <pre><code>pip install opencv-python\n# or\nsudo apt-get install python3-opencv\n</code></pre></p>"},{"location":"en/installation/#importerror-no-module-named-pyniryo","title":"ImportError: No module named 'pyniryo'","text":"<p>Solution: <pre><code>pip install pyniryo\n# or install with Niryo support\npip install -e \".[niryo]\"\n</code></pre></p>"},{"location":"en/installation/#permission-denied-when-accessing-robot","title":"Permission denied when accessing robot","text":"<p>Solution (Linux): <pre><code>sudo usermod -a -G dialout $USER\n# Logout and login\n</code></pre></p>"},{"location":"en/installation/#tests-fail-with-no-module-named-robot_workspace","title":"Tests fail with \"No module named robot_workspace\"","text":"<p>Solution: <pre><code># Ensure you installed in editable mode\npip install -e .\n\n# Or add to PYTHONPATH\nexport PYTHONPATH=\"${PYTHONPATH}:$(pwd)\"\n</code></pre></p>"},{"location":"en/installation/#opencv-errors-on-import","title":"OpenCV errors on import","text":"<p>Solution: <pre><code># Uninstall all OpenCV versions\npip uninstall opencv-python opencv-python-headless opencv-contrib-python\n\n# Reinstall the correct one\npip install opencv-python\n</code></pre></p>"},{"location":"en/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check existing issues: GitHub Issues</li> <li>Create new issue: Provide:</li> <li>Python version: <code>python --version</code></li> <li>Operating system</li> <li>Installation method used</li> <li>Full error message</li> <li>Steps to reproduce</li> </ol>"},{"location":"en/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Read the documentation: See docs/README.md</li> <li>Run examples: Execute <code>python main.py</code></li> <li>Explore the API: Review docs/api.md</li> <li>Check examples: See docs/examples.md</li> <li>Run tests: Execute <code>pytest</code></li> </ol>"},{"location":"en/installation/#development-setup","title":"Development Setup","text":"<p>For contributors:</p> <pre><code># Fork and clone\ngit clone https://github.com/YOUR_USERNAME/robot_workspace.git\ncd robot_workspace\n\n# Install development version\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n\n# Create feature branch\ngit checkout -b feature/my-feature\n\n# Make changes and test\npytest\n\n# Format code\nblack .\nruff check . --fix\n\n# Commit and push\ngit commit -am \"Add feature\"\ngit push origin feature/my-feature\n</code></pre>"},{"location":"en/installation/#uninstallation","title":"Uninstallation","text":"<p>To remove the package:</p> <pre><code># Uninstall package\npip uninstall robot-workspace\n\n# Remove virtual environment (if used)\nrm -rf robot_env  # or your venv name\n\n# Remove conda environment (if used)\nconda env remove -n robot_workspace\n</code></pre>"},{"location":"en/installation/#updates","title":"Updates","text":"<p>To update to the latest version:</p> <pre><code># If installed from source\ncd robot_workspace\ngit pull origin master\npip install -e . --upgrade\n\n# If installed from PyPI (when available)\npip install --upgrade robot-workspace\n</code></pre> <p>Author: Daniel Gaida Email: daniel.gaida@th-koeln.de Repository: https://github.com/dgaida/robot_workspace</p>"},{"location":"en/quality_metrics/","title":"Quality Metrics","text":"<p>This page provides an overview of the project's quality metrics, including documentation coverage and test results.</p>"},{"location":"en/quality_metrics/#api-documentation-coverage","title":"\ud83d\udcca API Documentation Coverage","text":"<p>We use <code>interrogate</code> to measure our API documentation coverage. Our target is &gt;95%.</p> Metric Status Public API Coverage Docstring Style Google"},{"location":"en/quality_metrics/#test-coverage","title":"\ud83e\uddea Test Coverage","text":"Category Coverage Overall Unit Tests &gt;90% Integration Tests Verified"},{"location":"en/quality_metrics/#code-quality","title":"\ud83d\udee0\ufe0f Code Quality","text":"<ul> <li>Linter: Ruff</li> <li>Formatter: Black</li> <li>Type Checker: Mypy (Strict)</li> </ul>"},{"location":"en/api/","title":"API Reference","text":"<p>This section contains the automatically generated documentation for the <code>robot_workspace</code> package.</p>"},{"location":"en/api/#core-modules","title":"Core Modules","text":""},{"location":"en/architecture/","title":"Architecture Documentation","text":"<p>Comprehensive documentation for the architecture, design patterns, and implementation details of the <code>robot_workspace</code> package.</p>"},{"location":"en/architecture/#system-architecture","title":"System Architecture","text":"<p>The <code>robot_workspace</code> package separates concerns between:</p> <ul> <li>Object Representation: Detected objects with physical properties.</li> <li>Workspace Management: Robot workspace definitions and coordinate transformations.</li> <li>Spatial Reasoning: Queries and relationships between objects.</li> </ul>"},{"location":"en/architecture/#component-interaction","title":"Component Interaction","text":"graph TD     User[User Application] --&gt; Package[Robot Workspace Package]     subgraph Package         Obj[Object Layer] &lt;--&gt; WS[Workspace Layer]     end     Package --&gt; Robot[Robot Control Layer]     Robot --&gt; Hardware[Robot Hardware / Simulation]"},{"location":"en/architecture/#coordinate-systems","title":"Coordinate Systems","text":"<p>The package handles three distinct coordinate systems:</p> <ol> <li>Image Coordinates (Pixels): (u, v) in pixels. Origin at top-left.</li> <li>Relative Coordinates (Normalized): (u_rel, v_rel) in range [0, 1].</li> <li>World Coordinates (Meters): (x, y, z) in meters relative to robot base.</li> </ol>"},{"location":"en/architecture/#transformation-pipeline","title":"Transformation Pipeline","text":"<pre><code>Image Coordinates (u, v)\n         \u2193\n    [Normalization]\n         \u2193\nRelative Coordinates (u_rel, v_rel)\n         \u2193\n    [Workspace.transform_camera2world_coords()]\n         \u2193\nWorld Coordinates (x, y, z, roll, pitch, yaw)\n</code></pre>"},{"location":"en/architecture/#design-patterns","title":"Design Patterns","text":"<ul> <li>Abstract Factory: For creating different workspace types (Niryo, WidowX).</li> <li>Collection Pattern: <code>Objects</code> as an enhanced list for spatial queries.</li> <li>Serialization Pattern: Consistent JSON serialization across all objects.</li> </ul>"},{"location":"en/development/docstrings/","title":"Docstring Style Guide","text":"<p>We use the Google Python Style Guide for all docstrings in this project.</p>"},{"location":"en/development/docstrings/#format","title":"Format","text":"<pre><code>def function_name(param1: int, param2: str) -&gt; bool:\n    \"\"\"\n    Short description of the function.\n\n    Longer description explaining the logic, side effects, or\n    anything else that might be important for the user.\n\n    Args:\n        param1 (int): Description of the first parameter.\n        param2 (str): Description of the second parameter.\n\n    Returns:\n        bool: Description of the return value.\n\n    Raises:\n        ValueError: Description of when this error is raised.\n    \"\"\"\n</code></pre>"},{"location":"en/development/docstrings/#classes","title":"Classes","text":"<pre><code>class MyClass:\n    \"\"\"\n    Summary of the class.\n\n    Attributes:\n        attr1 (int): Description of attr1.\n        attr2 (str): Description of attr2.\n    \"\"\"\n</code></pre>"},{"location":"en/development/testing/","title":"\ud83e\uddea Testing","text":"<p>The package includes comprehensive tests with &gt;90% coverage.</p>"},{"location":"en/development/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"en/development/testing/#run-with-coverage-report","title":"Run with Coverage Report","text":"<pre><code>pytest --cov=robot_workspace --cov-report=html --cov-report=term\n</code></pre>"},{"location":"en/development/testing/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code># Unit tests only\npytest tests/objects/\npytest tests/workspaces/\n\n# Integration tests\npytest -m integration\n\n# Skip slow tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"en/development/testing/#test-coverage","title":"Test Coverage","text":"<p>The test suite covers: - PoseObjectPNP: Initialization, arithmetic, transformations, quaternions - Object: Creation, serialization, properties, mask operations - Objects: Collection operations, spatial queries, filtering - Workspace: Initialization, transformations, visibility checks - Integration: End-to-end workflows and multi-component interactions</p> <p>See tests/README.md for detailed testing documentation.</p>"},{"location":"en/usage/examples/","title":"Robot Workspace - Examples","text":"<p>Comprehensive examples demonstrating common use cases and workflows.</p>"},{"location":"en/usage/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Examples</li> <li>Workspace Management</li> <li>Object Detection</li> <li>Spatial Queries</li> <li>Serialization</li> <li>Advanced Examples</li> <li>Integration Examples</li> </ol>"},{"location":"en/usage/examples/#basic-examples","title":"Basic Examples","text":""},{"location":"en/usage/examples/#creating-a-pose","title":"Creating a Pose","text":"<pre><code>from robot_workspace import PoseObjectPNP\n\n# Create a 6-DOF pose\npose = PoseObjectPNP(\n    x=0.2,      # 20 cm forward\n    y=0.1,      # 10 cm to the right\n    z=0.3,      # 30 cm up\n    roll=0.0,   # No roll\n    pitch=1.57, # 90 degrees (gripper pointing down)\n    yaw=0.0     # No yaw rotation\n)\n\nprint(pose)\n# Output:\n# x = 0.2000, y = 0.1000, z = 0.3000\n# roll = 0.000, pitch = 1.570, yaw = 0.000\n</code></pre>"},{"location":"en/usage/examples/#pose-arithmetic","title":"Pose Arithmetic","text":"<pre><code>from robot_workspace import PoseObjectPNP\n\n# Initial pose\npose1 = PoseObjectPNP(x=0.2, y=0.1, z=0.3, roll=0.0, pitch=1.57, yaw=0.0)\n\n# Offset\noffset = PoseObjectPNP(x=0.05, y=0.02, z=0.0, roll=0.0, pitch=0.0, yaw=0.5)\n\n# Add offset\nnew_pose = pose1 + offset\nprint(f\"New position: ({new_pose.x}, {new_pose.y}, {new_pose.z})\")\n# Output: New position: (0.25, 0.12, 0.3)\n\n# Subtract poses\ndifference = new_pose - pose1\nprint(f\"Difference: ({difference.x}, {difference.y}, {difference.z})\")\n# Output: Difference: (0.05, 0.02, 0.0)\n</code></pre>"},{"location":"en/usage/examples/#pose-transformations","title":"Pose Transformations","text":"<pre><code># Convert to list\npose_list = pose.to_list()\nprint(pose_list)\n# [0.2, 0.1, 0.3, 0.0, 1.57, 0.0]\n\n# Get XY coordinates only\nxy = pose.xy_coordinate()\nprint(xy)\n# [0.2, 0.1]\n\n# Convert to transformation matrix\nmatrix = pose.to_transformation_matrix()\nprint(matrix.shape)\n# (4, 4)\n\n# Get quaternion representation\nquaternion = pose.quaternion\nprint(quaternion)\n# [qx, qy, qz, qw]\n</code></pre>"},{"location":"en/usage/examples/#workspace-management","title":"Workspace Management","text":""},{"location":"en/usage/examples/#creating-a-workspace-with-mock-environment","title":"Creating a Workspace with Mock Environment","text":"<pre><code>from unittest.mock import Mock\nfrom robot_workspace import NiryoWorkspaces, PoseObjectPNP\n\n# Create a mock environment (for demo/testing without real robot)\ndef create_mock_environment(use_simulation=True):\n    env = Mock()\n    env.use_simulation.return_value = use_simulation\n    env.verbose.return_value = False\n\n    # Mock coordinate transformation\n    def mock_get_target_pose(ws_id, u_rel, v_rel, yaw):\n        x = 0.4 - u_rel * 0.3    # x: 0.4 to 0.1\n        y = 0.15 - v_rel * 0.3   # y: 0.15 to -0.15\n        return PoseObjectPNP(x, y, 0.05, 0.0, 1.57, yaw)\n\n    env.get_robot_target_pose_from_rel = mock_get_target_pose\n    return env\n\n# Create workspace collection\nmock_env = create_mock_environment(use_simulation=True)\nworkspaces = NiryoWorkspaces(mock_env, verbose=False)\n\n# Get home workspace\nworkspace = workspaces.get_home_workspace()\nprint(f\"Workspace ID: {workspace.id()}\")\nprint(f\"Dimensions: {workspace.width_m():.3f}m \u00d7 {workspace.height_m():.3f}m\")\n</code></pre>"},{"location":"en/usage/examples/#coordinate-transformation","title":"Coordinate Transformation","text":"<pre><code># Transform relative image coordinates to world coordinates\nworld_pose = workspace.transform_camera2world_coords(\n    workspace_id=workspace.id(),\n    u_rel=0.5,  # Center of image (horizontal)\n    v_rel=0.5,  # Center of image (vertical)\n    yaw=0.0     # No rotation\n)\n\nprint(f\"World coordinates: ({world_pose.x:.3f}, {world_pose.y:.3f}, {world_pose.z:.3f})\")\n# Output: World coordinates: (0.250, 0.000, 0.050)\n</code></pre>"},{"location":"en/usage/examples/#multiple-workspaces","title":"Multiple Workspaces","text":"<pre><code>from robot_workspace import NiryoWorkspaces\n\n# Create workspaces (automatically loads configured workspaces)\nworkspaces = NiryoWorkspaces(environment, verbose=False)\n\nprint(f\"Number of workspaces: {len(workspaces)}\")\nprint(f\"Workspace IDs: {workspaces.get_workspace_ids()}\")\n\n# Access specific workspaces\nleft_ws = workspaces.get_workspace_left()\nright_ws = workspaces.get_workspace_right()\n\n# Get observation pose for each\nleft_obs = left_ws.observation_pose()\nright_obs = right_ws.observation_pose()\n\nprint(f\"Left workspace observation: {left_obs}\")\nprint(f\"Right workspace observation: {right_obs}\")\n</code></pre>"},{"location":"en/usage/examples/#checking-workspace-visibility","title":"Checking Workspace Visibility","text":"<pre><code># Get current camera pose (from robot)\ncurrent_pose = workspace.observation_pose()\n\n# Check if workspace is visible\nis_visible = workspace.is_visible(current_pose)\nprint(f\"Workspace visible: {is_visible}\")\n\n# Find visible workspace from current pose\nvisible_ws = workspaces.get_visible_workspace(current_pose)\nif visible_ws:\n    print(f\"Visible workspace: {visible_ws.id()}\")\nelse:\n    print(\"No workspace visible\")\n</code></pre>"},{"location":"en/usage/examples/#object-detection","title":"Object Detection","text":""},{"location":"en/usage/examples/#creating-objects","title":"Creating Objects","text":"<pre><code>from robot_workspace import Object\nimport numpy as np\n\n# Create object without segmentation mask\nobj = Object(\n    label=\"pencil\",\n    u_min=100, v_min=100,    # Bounding box top-left\n    u_max=200, v_max=200,    # Bounding box bottom-right\n    mask_8u=None,             # No segmentation mask\n    workspace=workspace\n)\n\n# Create object with segmentation mask\nmask = np.zeros((640, 480), dtype=np.uint8)\nmask[100:200, 100:200] = 255  # White square\n\nobj_with_mask = Object(\n    label=\"cube\",\n    u_min=100, v_min=100,\n    u_max=200, v_max=200,\n    mask_8u=mask,\n    workspace=workspace\n)\n</code></pre>"},{"location":"en/usage/examples/#accessing-object-properties","title":"Accessing Object Properties","text":"<pre><code># Label\nprint(f\"Label: {obj.label()}\")\n\n# Position\nprint(f\"Position (x, y): {obj.coordinate()}\")\nprint(f\"Center of mass: ({obj.x_com():.3f}, {obj.y_com():.3f})\")\nprint(f\"Center: ({obj.x_center():.3f}, {obj.y_center():.3f})\")\n\n# Dimensions\nprint(f\"Width: {obj.width_m():.3f}m\")\nprint(f\"Height: {obj.height_m():.3f}m\")\nprint(f\"Size: {obj.size_m2() * 10000:.2f} cm\u00b2\")\n\n# Orientation\nprint(f\"Gripper rotation: {obj.gripper_rotation():.3f} rad\")\nprint(f\"Gripper rotation: {np.degrees(obj.gripper_rotation()):.1f}\u00b0\")\n\n# Full poses\nprint(f\"Pose (center): {obj.pose_center()}\")\nprint(f\"Pose (COM): {obj.pose_com()}\")\n</code></pre>"},{"location":"en/usage/examples/#updating-object-position","title":"Updating Object Position","text":"<pre><code># Update position after robot moves object\nnew_position = [0.3, 0.05]\nobj.set_position(new_position)\n\n# Or update full pose (includes rotation)\nnew_pose = obj.pose_com().copy_with_offsets(\n    x_offset=0.1,\n    y_offset=0.05,\n    yaw_offset=0.5  # Rotate 0.5 radians\n)\nobj.set_pose_com(new_pose)\n\nprint(f\"New position: {obj.coordinate()}\")\n</code></pre>"},{"location":"en/usage/examples/#spatial-queries","title":"Spatial Queries","text":""},{"location":"en/usage/examples/#creating-an-object-collection","title":"Creating an Object Collection","text":"<pre><code>from robot_workspace import Objects\n\n# Create multiple objects\nobj1 = Object(\"pencil\", 100, 100, 180, 140, None, workspace)\nobj2 = Object(\"pen\", 280, 200, 360, 260, None, workspace)\nobj3 = Object(\"eraser\", 450, 350, 510, 410, None, workspace)\n\n# Create collection\nobjects = Objects([obj1, obj2, obj3])\n\nprint(f\"Total objects: {len(objects)}\")\nprint(f\"Objects: {objects.get_detected_objects_as_comma_separated_string()}\")\n</code></pre>"},{"location":"en/usage/examples/#finding-objects-by-location","title":"Finding Objects by Location","text":"<pre><code>from robot_workspace import Location\n\n# Reference point\nreference = [0.25, 0.0]\n\n# Find objects to the left\nleft_objects = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=reference\n)\nprint(f\"Objects to the left: {len(left_objects)}\")\n\n# Find objects to the right\nright_objects = objects.get_detected_objects(\n    location=Location.RIGHT_NEXT_TO,\n    coordinate=reference\n)\nprint(f\"Objects to the right: {len(right_objects)}\")\n\n# Find objects above\nabove_objects = objects.get_detected_objects(\n    location=Location.ABOVE,\n    coordinate=reference\n)\nprint(f\"Objects above: {len(above_objects)}\")\n\n# Find objects below\nbelow_objects = objects.get_detected_objects(\n    location=Location.BELOW,\n    coordinate=reference\n)\nprint(f\"Objects below: {len(below_objects)}\")\n\n# Find objects close to point (within 2cm)\nclose_objects = objects.get_detected_objects(\n    location=Location.CLOSE_TO,\n    coordinate=reference\n)\nprint(f\"Objects nearby: {len(close_objects)}\")\n</code></pre>"},{"location":"en/usage/examples/#finding-objects-by-label","title":"Finding Objects by Label","text":"<pre><code># Find all pens\npens = objects.get_detected_objects(label=\"pen\")\nprint(f\"Found {len(pens)} pen(s)\")\n\n# Note: Label matching is substring-based\n# \"pen\" will match both \"pen\" and \"pencil\"\npencils = objects.get_detected_objects(label=\"pencil\")\nprint(f\"Found {len(pencils)} pencil(s)\")\n\n# Combine filters\nleft_pens = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.25, 0.0],\n    label=\"pen\"\n)\n</code></pre>"},{"location":"en/usage/examples/#finding-nearest-object","title":"Finding Nearest Object","text":"<pre><code># Find nearest object to a point\npoint = [0.25, 0.05]\nnearest, distance = objects.get_nearest_detected_object(point)\n\nprint(f\"Nearest object: {nearest.label()}\")\nprint(f\"Distance: {distance:.3f}m\")\n\n# Find nearest with specific label\nnearest_pen, distance = objects.get_nearest_detected_object(\n    point,\n    label=\"pen\"\n)\n</code></pre>"},{"location":"en/usage/examples/#size-based-queries","title":"Size-Based Queries","text":"<pre><code># Find largest object\nlargest, size = objects.get_largest_detected_object()\nprint(f\"Largest: {largest.label()} ({size * 10000:.2f} cm\u00b2)\")\n\n# Find smallest object\nsmallest, size = objects.get_smallest_detected_object()\nprint(f\"Smallest: {smallest.label()} ({size * 10000:.2f} cm\u00b2)\")\n\n# Get all objects sorted by size\nsorted_ascending = objects.get_detected_objects_sorted(ascending=True)\nsorted_descending = objects.get_detected_objects_sorted(ascending=False)\n\nprint(\"\\nObjects by size (ascending):\")\nfor obj in sorted_ascending:\n    print(f\"  {obj.label()}: {obj.size_m2() * 10000:.2f} cm\u00b2\")\n</code></pre>"},{"location":"en/usage/examples/#specific-object-lookup","title":"Specific Object Lookup","text":"<pre><code># Get object at specific coordinate with label\nobj = objects.get_detected_object(\n    coordinate=[0.2, 0.1],\n    label=\"pencil\"\n)\n\nif obj:\n    print(f\"Found: {obj.label()} at {obj.coordinate()}\")\nelse:\n    print(\"Object not found\")\n</code></pre>"},{"location":"en/usage/examples/#serialization","title":"Serialization","text":""},{"location":"en/usage/examples/#object-serialization","title":"Object Serialization","text":"<pre><code># Serialize single object to dictionary\nobj_dict = obj.to_dict()\nprint(f\"Serialized keys: {list(obj_dict.keys())}\")\n\n# Serialize to JSON string\njson_str = obj.to_json()\nprint(f\"JSON length: {len(json_str)} characters\")\n\n# Deserialize from dictionary\nreconstructed = Object.from_dict(obj_dict, workspace)\nprint(f\"Reconstructed: {reconstructed.label()}\")\n\n# Deserialize from JSON string\nreconstructed = Object.from_json(json_str, workspace)\nprint(f\"Reconstructed: {reconstructed.label()}\")\n</code></pre>"},{"location":"en/usage/examples/#collection-serialization","title":"Collection Serialization","text":"<pre><code># Serialize collection to list of dictionaries\ndict_list = Objects.objects_to_dict_list(objects)\nprint(f\"Serialized {len(dict_list)} objects\")\n\n# Deserialize back to Objects collection\nreconstructed_objects = Objects.dict_list_to_objects(dict_list, workspace)\nprint(f\"Reconstructed {len(reconstructed_objects)} objects\")\n\n# Verify reconstruction\nfor orig, recon in zip(objects, reconstructed_objects):\n    print(f\"{orig.label()} \u2192 {recon.label()}: Match!\")\n</code></pre>"},{"location":"en/usage/examples/#serializable-query-results","title":"Serializable Query Results","text":"<pre><code># Get serializable results directly\nobj_dict = objects.get_detected_object(\n    [0.2, 0.1],\n    label=\"pencil\",\n    serializable=True\n)\n\nlargest_dict, size = objects.get_largest_detected_object(serializable=True)\n\nsorted_dicts = objects.get_detected_objects_sorted(serializable=True)\n\n# Useful for sending over network or storing in database\nimport json\njson_data = json.dumps(sorted_dicts)\n</code></pre>"},{"location":"en/usage/examples/#advanced-examples","title":"Advanced Examples","text":""},{"location":"en/usage/examples/#llm-friendly-formatting","title":"LLM-Friendly Formatting","text":"<pre><code># Format for LLM consumption (compact)\nllm_str = obj.as_string_for_llm()\nprint(llm_str)\n# Output: \"- 'pencil' at world coordinates [0.20, 0.10] with a width of\n#          0.05 meters, a height of 0.08 meters and a size of 40.00\n#          square centimeters.\"\n\n# Format for chat window\nchat_str = obj.as_string_for_chat_window()\nprint(chat_str)\n# Output: \"Detected a new object: pencil at world coordinate (0.20, 0.10)\n#          with orientation 0.5 rad and size 0.05 m x 0.08 m.\"\n\n# Format entire collection\nfor obj in objects:\n    print(obj.as_string_for_llm())\n</code></pre>"},{"location":"en/usage/examples/#working-with-segmentation-masks","title":"Working with Segmentation Masks","text":"<pre><code>import cv2\nimport numpy as np\n\n# Create object with mask\nmask = np.zeros((640, 480), dtype=np.uint8)\ncv2.circle(mask, (150, 150), 50, 255, -1)  # Filled circle\n\nobj = Object(\n    label=\"coin\",\n    u_min=100, v_min=100,\n    u_max=200, v_max=200,\n    mask_8u=mask,\n    workspace=workspace\n)\n\n# Access contour information\ncontour = obj.largest_contour()\nif contour is not None:\n    area = cv2.contourArea(contour)\n    print(f\"Contour area: {area} pixels\")\n</code></pre>"},{"location":"en/usage/examples/#pose-comparison-and-approximation","title":"Pose Comparison and Approximation","text":"<pre><code>pose1 = PoseObjectPNP(1.0, 2.0, 3.0, 0.1, 0.2, 0.3)\npose2 = PoseObjectPNP(1.05, 2.05, 3.05, 0.12, 0.22, 0.32)\n\n# Exact equality\nis_equal = pose1 == pose2\nprint(f\"Exactly equal: {is_equal}\")  # False\n\n# Approximate equality (full pose)\nis_approx = pose1.approx_eq(\n    pose2,\n    eps_position=0.1,      # 10cm tolerance\n    eps_orientation=0.1    # ~5.7\u00b0 tolerance\n)\nprint(f\"Approximately equal: {is_approx}\")  # True\n\n# Approximate equality (position only)\nis_approx_xyz = pose1.approx_eq_xyz(pose2, eps=0.1)\nprint(f\"Position approximately equal: {is_approx_xyz}\")  # True\n</code></pre>"},{"location":"en/usage/examples/#custom-object-filtering","title":"Custom Object Filtering","text":"<pre><code># Filter objects by custom criteria\ndef filter_by_size_range(objects, min_size, max_size):\n    \"\"\"Get objects within size range (in cm\u00b2)\"\"\"\n    return Objects([\n        obj for obj in objects\n        if min_size &lt;= obj.size_m2() * 10000 &lt;= max_size\n    ])\n\nmedium_objects = filter_by_size_range(objects, 20.0, 50.0)\nprint(f\"Medium-sized objects: {len(medium_objects)}\")\n\n# Filter by distance from point\ndef filter_by_distance(objects, point, max_distance):\n    \"\"\"Get objects within distance from point\"\"\"\n    return Objects([\n        obj for obj in objects\n        if np.sqrt((obj.x_com() - point[0])**2 +\n                  (obj.y_com() - point[1])**2) &lt;= max_distance\n    ])\n\nnearby = filter_by_distance(objects, [0.25, 0.05], 0.1)\nprint(f\"Objects within 10cm: {len(nearby)}\")\n</code></pre>"},{"location":"en/usage/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"en/usage/examples/#pick-and-place-workflow","title":"Pick-and-Place Workflow","text":"<pre><code>from robot_workspace import NiryoWorkspaces, Objects, Location\n\n# 1. Initialize workspace\nworkspaces = NiryoWorkspaces(environment)\nworkspace = workspaces.get_home_workspace()\n\n# 2. Detect objects (from vision system)\ndetected_objects = Objects([obj1, obj2, obj3])\n\n# 3. Find target object\ntarget = detected_objects.get_detected_object(\n    coordinate=[0.2, 0.1],\n    label=\"cube\"\n)\n\nif target:\n    # 4. Get pickup pose\n    pickup_pose = target.pose_com()\n    pickup_rotation = target.gripper_rotation()\n\n    print(f\"Pickup at: ({pickup_pose.x:.3f}, {pickup_pose.y:.3f})\")\n    print(f\"Rotation: {np.degrees(pickup_rotation):.1f}\u00b0\")\n\n    # 5. Execute pick (using robot API)\n    # robot.pick(pickup_pose, pickup_rotation)\n\n    # 6. Define placement location\n    place_pose = PoseObjectPNP(0.3, -0.1, 0.05, 0.0, 1.57, 0.0)\n\n    # 7. Execute place\n    # robot.place(place_pose)\n\n    # 8. Update object position in memory\n    target.set_pose_com(place_pose)\n</code></pre>"},{"location":"en/usage/examples/#multi-workspace-object-transfer","title":"Multi-Workspace Object Transfer","text":"<pre><code># Initialize multiple workspaces\nworkspaces = NiryoWorkspaces(environment)\nleft_ws = workspaces.get_workspace_left()\nright_ws = workspaces.get_workspace_right()\n\n# Objects in left workspace\nleft_objects = Objects([...])\n\n# Objects in right workspace\nright_objects = Objects([...])\n\n# Find object to transfer\ntransfer_obj = left_objects.get_detected_object(\n    coordinate=[0.2, 0.1],\n    label=\"cube\"\n)\n\nif transfer_obj:\n    # Remove from left workspace memory\n    left_objects.remove(transfer_obj)\n\n    # Calculate new position in right workspace\n    # (transform from left to right workspace coordinates)\n    new_pose = right_ws.transform_camera2world_coords(\n        right_ws.id(),\n        u_rel=0.5,\n        v_rel=0.5,\n        yaw=0.0\n    )\n\n    # Update object's pose\n    transfer_obj.set_pose_com(new_pose)\n\n    # Add to right workspace memory\n    right_objects.append(transfer_obj)\n\n    print(f\"Transferred {transfer_obj.label()} to right workspace\")\n</code></pre>"},{"location":"en/usage/examples/#sorting-objects-by-size","title":"Sorting Objects by Size","text":"<pre><code># Detect all objects\nall_objects = Objects([...])\n\n# Sort by size\nsorted_objects = all_objects.get_detected_objects_sorted(ascending=True)\n\n# Pick and place in order (smallest to largest)\nfor i, obj in enumerate(sorted_objects):\n    pickup_pose = obj.pose_com()\n\n    # Calculate placement in a row\n    place_pose = PoseObjectPNP(\n        x=0.3,\n        y=-0.1 + i * 0.05,  # Space objects 5cm apart\n        z=0.05,\n        roll=0.0,\n        pitch=1.57,\n        yaw=obj.gripper_rotation()\n    )\n\n    print(f\"Moving {obj.label()} from {obj.coordinate()} to row position {i}\")\n\n    # Execute pick and place\n    # robot.pick(pickup_pose, obj.gripper_rotation())\n    # robot.place(place_pose)\n\n    # Update object position\n    obj.set_pose_com(place_pose)\n</code></pre>"},{"location":"en/usage/examples/#finding-and-grouping-similar-objects","title":"Finding and Grouping Similar Objects","text":"<pre><code># Find all objects with \"pen\" in the label\npen_like = Objects([\n    obj for obj in all_objects\n    if \"pen\" in obj.label().lower()\n])\n\nprint(f\"Found {len(pen_like)} pen-like objects:\")\nfor obj in pen_like:\n    print(f\"  - {obj.label()} at {obj.coordinate()}\")\n\n# Group by size category\ndef categorize_by_size(obj):\n    size_cm2 = obj.size_m2() * 10000\n    if size_cm2 &lt; 30:\n        return \"small\"\n    elif size_cm2 &lt; 60:\n        return \"medium\"\n    else:\n        return \"large\"\n\nfrom collections import defaultdict\ngrouped = defaultdict(list)\n\nfor obj in all_objects:\n    category = categorize_by_size(obj)\n    grouped[category].append(obj)\n\nfor category, objs in grouped.items():\n    print(f\"{category.capitalize()}: {len(objs)} objects\")\n</code></pre>"},{"location":"en/usage/examples/#real-time-object-tracking","title":"Real-time Object Tracking","text":"<pre><code>import time\n\n# Initial detection\nprevious_objects = detect_objects_from_camera(workspace)\n\n# Track changes\nwhile True:\n    time.sleep(0.5)  # Check every 500ms\n\n    # Detect current objects\n    current_objects = detect_objects_from_camera(workspace)\n\n    # Find new objects\n    new_labels = set(obj.label() for obj in current_objects)\n    old_labels = set(obj.label() for obj in previous_objects)\n\n    added = new_labels - old_labels\n    removed = old_labels - new_labels\n\n    if added:\n        print(f\"New objects detected: {added}\")\n    if removed:\n        print(f\"Objects removed: {removed}\")\n\n    # Track position changes\n    for curr_obj in current_objects:\n        for prev_obj in previous_objects:\n            if curr_obj.label() == prev_obj.label():\n                distance = np.sqrt(\n                    (curr_obj.x_com() - prev_obj.x_com())**2 +\n                    (curr_obj.y_com() - prev_obj.y_com())**2\n                )\n                if distance &gt; 0.01:  # Moved more than 1cm\n                    print(f\"{curr_obj.label()} moved {distance*100:.1f}cm\")\n\n    previous_objects = current_objects\n</code></pre>"},{"location":"en/usage/examples/#see-also","title":"See Also","text":"<ul> <li>Installation Guide</li> <li>Architecture Documentation</li> <li>API Reference</li> <li>Contributing Guide</li> </ul> <p>Author: Daniel Gaida Email: daniel.gaida@th-koeln.de Repository: https://github.com/dgaida/robot_workspace</p>"},{"location":"en/development/adding_robot_support/","title":"\ud83d\udd27 Adding Robot Support","text":"<p>To add support for a new robot platform:</p>"},{"location":"en/development/adding_robot_support/#1-create-a-workspace-class","title":"1. Create a Workspace Class","text":"<pre><code>from robot_workspace.workspaces.workspace import Workspace\nfrom robot_workspace.objects.pose_object import PoseObjectPNP\n\nclass MyRobotWorkspace(Workspace):\n    def __init__(self, workspace_id: str, environment, verbose: bool = False):\n        self._environment = environment\n        super().__init__(workspace_id, verbose)\n\n    def transform_camera2world_coords(self, workspace_id, u_rel, v_rel, yaw=0.0):\n        # Implement coordinate transformation using your robot's API\n        return self._environment.get_robot_target_pose_from_rel(\n            workspace_id, u_rel, v_rel, yaw\n        )\n\n    def _set_observation_pose(self):\n        # Define observation pose for your workspace\n        self._observation_pose = PoseObjectPNP(\n            x=0.3, y=0.0, z=0.25,\n            roll=0.0, pitch=1.57, yaw=0.0\n        )\n\n    def _set_4corners_of_workspace(self):\n        # Define workspace corners\n        self._xy_ul_wc = self.transform_camera2world_coords(self._id, 0.0, 0.0)\n        self._xy_lr_wc = self.transform_camera2world_coords(self._id, 1.0, 1.0)\n        # ... set other corners\n</code></pre>"},{"location":"en/development/adding_robot_support/#2-create-a-workspaces-collection","title":"2. Create a Workspaces Collection","text":"<pre><code>from robot_workspace.workspaces.workspaces import Workspaces\n\nclass MyRobotWorkspaces(Workspaces):\n    def __init__(self, environment, verbose: bool = False):\n        super().__init__(verbose)\n        workspace = MyRobotWorkspace(\"my_workspace_id\", environment, verbose)\n        self.append_workspace(workspace)\n</code></pre>"},{"location":"en/development/adding_robot_support/#3-integrate-with-environment","title":"3. Integrate with Environment","text":"<p>Your <code>Environment</code> class should provide: - <code>use_simulation()</code> - Returns True if in simulation mode - <code>get_robot_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)</code> - Coordinate transformation</p> <p>See WidowXWorkspace for a complete example.</p>"}]}